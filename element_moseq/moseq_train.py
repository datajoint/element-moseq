"""
Code adapted from the Datta Lab
DataJoint Schema for Keypoint-MoSeq training pipeline
"""

import importlib
import inspect
import os
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

import cv2
import datajoint as dj
import numpy as np
from element_interface.utils import find_full_path

from .readers import kpms_reader

schema = dj.schema()
_linking_module = None
logger = dj.logger


def activate(
    train_schema_name: str,
    *,
    create_schema: bool = True,
    create_tables: bool = True,
    linking_module: str = None,
):
    """Activate this schema.

    Args:
        train_schema_name (str): A string containing the name of the `moseq_train` schema.
        create_schema (bool): If True (default), schema will be created in the database.
        create_tables (bool): If True (default), tables related to the schema will be created in the database.
        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.
    Functions:
        get_kpms_root_data_dir(): Returns absolute path for root data directory/ies
                                  with all behavioral recordings, as (list of) string(s).
        get_kpms_processed_data_dir(): Optional. Returns absolute path for processed
                                       data.
    """

    if isinstance(linking_module, str):
        linking_module = importlib.import_module(linking_module)
    assert inspect.ismodule(
        linking_module
    ), "The argument 'dependency' must be a module's name or a module object"

    assert hasattr(
        linking_module, "get_kpms_root_data_dir"
    ), "The linking module must specify a lookup function for a root data directory"

    global _linking_module
    _linking_module = linking_module

    # activate
    schema.activate(
        train_schema_name,
        create_schema=create_schema,
        create_tables=create_tables,
        add_objects=_linking_module.__dict__,
    )


# -------------- Functions required by element-moseq ---------------


def get_kpms_root_data_dir() -> list:
    """Fetches absolute data path to kpms data directories.

    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.

    Returns:
        A list of the absolute path(s) to kpms data directories.
    """

    root_directories = _linking_module.get_kpms_root_data_dir()
    if isinstance(root_directories, (str, Path)):
        root_directories = [root_directories]

    if (
        hasattr(_linking_module, "get_kpms_processed_data_dir")
        and get_kpms_processed_data_dir() not in root_directories
    ):
        root_directories.append(_linking_module.get_kpms_processed_data_dir())

    return root_directories


def get_kpms_processed_data_dir() -> Optional[str]:
    """Retrieve the root directory for all processed data.

    Returns:
        A string for the full path to the root directory for processed data.
    """
    if hasattr(_linking_module, "get_kpms_processed_data_dir"):
        return _linking_module.get_kpms_processed_data_dir()
    else:
        return None


# ----------------------------- Table declarations ----------------------


@schema
class PoseEstimationMethod(dj.Lookup):
    """Name of the pose estimation method supported by the keypoint loader of `keypoint-moseq` package.

    Attributes:
        pose_estimation_method  (str): Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)
        pose_estimation_desc    (str): Optional. Pose estimation method description with the supported formats.
    """

    definition = """
    # Pose estimation methods supported by the keypoint loader of `keypoint-moseq` package.
    pose_estimation_method  : char(15)         # Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)
    ---
    pose_estimation_desc    : varchar(1000)    # Optional. Pose estimation method description with the supported formats.
    """

    contents = [
        ["deeplabcut", "`.csv` and `.h5/.hdf5` files generated by DeepLabcut analysis"],
        ["sleap", "`.slp` and `.h5/.hdf5` files generated by SLEAP analysis"],
        ["anipose", "`.csv` files generated by anipose analysis"],
        ["sleap-anipose", "`.h5/.hdf5` files generated by sleap-anipose analysis"],
        ["nwb", "`.nwb` files with Neurodata Without Borders (NWB) format"],
        ["facemap", "`.h5` files generated by Facemap analysis"],
    ]


@schema
class KeypointSet(dj.Manual):
    """Store the keypoint data and the video set directory for model training.

    Attributes:
        kpset_id (int)                          : Unique ID for each keypoint set.
        PoseEstimationMethod (foreign key)      : Unique format method used to obtain the keypoints data.
        kpset_dir (str)                         : Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory.
        kpset_desc (str)                        : Optional. User-entered description.
    """

    definition = """
    kpset_id                        : int           # Unique ID for each keypoint set
    ---
    -> PoseEstimationMethod                         # Unique format method used to obtain the keypoints data
    kpset_dir                       : varchar(255)  # Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory
    kpset_desc=''                   : varchar(1000) # Optional. User-entered description
    """

    class VideoFile(dj.Part):
        """Store the IDs and file paths of each video file that will be used for model training.

        Attributes:
            KeypointSet (foreign key) : Unique ID for each keypoint set.
            video_id (int)            : Unique ID for each video corresponding to each keypoint data file, relative to root data directory.
            video_path (str)          : Filepath of each video from which the keypoints are derived, relative to root data directory.
        """

        definition = """
        -> master
        video_id                      : int           # Unique ID for each video corresponding to each keypoint data file, relative to root data directory
        ---
        video_path                    : varchar(1000) # Filepath of each video from which the keypoints are derived, relative to root data directory
        """


@schema
class Bodyparts(dj.Manual):
    """Store the body parts to use in the analysis.

    Attributes:
        KeypointSet (foreign key)       : Unique ID for each `KeypointSet` key.
        bodyparts_id (int)              : Unique ID for a set of bodyparts for a particular keypoint set.
        anterior_bodyparts (blob)       : List of strings of anterior bodyparts
        posterior_bodyparts (blob)      : List of strings of posterior bodyparts
        use_bodyparts (blob)            : List of strings of bodyparts to be used
        bodyparts_desc (varchar)        : Optional. User-entered description.
    """

    definition = """
    -> KeypointSet                              # Unique ID for each `KeypointSet` key
    bodyparts_id                : int           # Unique ID for a set of bodyparts for a particular keypoint set
    ---
    anterior_bodyparts          : blob          # List of strings of anterior bodyparts
    posterior_bodyparts         : blob          # List of strings of posterior bodyparts
    use_bodyparts               : blob          # List of strings of bodyparts to be used
    bodyparts_desc=''           : varchar(1000) # Optional. User-entered description
    """


@schema
class PCATask(dj.Manual):
    """
    Define the Principal Component Analysis (PCA) task for dimensionality reduction of keypoint data.

    Attributes:
        Bodyparts (foreign key)         : Unique ID for each `Bodyparts` key
        outlier_scale_factor (int)      : Scale factor for outlier detection in keypoint data (default: 6)
        kpms_project_output_dir (str)   : Optional. Keypoint-MoSeq project output directory, relative to root data directory
        task_mode (enum)                : 'load' to load existing results, 'trigger' to compute new PCA
    """

    definition = """
    -> Bodyparts                                            # Unique ID for each `Bodyparts` key
    ---
    outlier_scale_factor=6          : int                   # Scale factor for outlier detection in keypoint data (default: 6)
    kpms_project_output_dir=''      : varchar(255)          # Optional. Keypoint-MoSeq project output directory, relative to root data directory
    task_mode='load'                :enum('load','trigger') # 'load' to load existing results, 'trigger' to compute new PCA

    """


@schema
class PreProcessing(dj.Computed):
    """
    Preprocess keypoint data by cleaning outliers and setting up the Keypoint-MoSeq project configuration.

    Attributes:
        PCATask (foreign key)           : Unique ID for each `PCATask` key.
        coordinates (longblob)          : Dictionary mapping filenames to cleaned keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3]).
        confidences (longblob)          : Dictionary mapping filenames to updated likelihood scores as ndarrays of shape (n_frames, n_bodyparts).
        formatted_bodyparts (longblob)  : List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.
        average_frame_rate (float)      : Average frame rate of the videos for model training (used for kappa calculation).
    """

    definition = """
    -> PCATask                          # Unique ID for each `PCATask` key
    ---
    coordinates             : longblob  # Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3])
    confidences             : longblob  # Dictionary mapping filenames to `likelihood` scores as ndarrays of shape (n_frames, n_bodyparts)
    formatted_bodyparts     : longblob  # List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.
    average_frame_rate      : float     # Average frame rate of the videos for model training (used for kappa calculation).
    """

    class Video(dj.Part):
        definition = """
        -> master
        video_name: varchar(255)
        ---
        video_duration              : int           # Duration of each video in minutes
        frame_rate                  : float         # Frame rate of the video in frames per second (Hz)
        """

    def make_fetch(self, key):
        """
        Preprocess keypoint data by cleaning outliers and setting up project configuration.

        Args:
            key (dict): Primary key from the `PCATask` table.

        Raises:
            NotImplementedError: Only `deeplabcut` pose estimation method is supported.
            FileNotFoundError: No DLC config file found in `kpset_dir`.

        High-Level Logic:
        1. Fetch the bodyparts, format method, and the directories.
        2. Set variables for each of the full path of the mentioned directories.
        3. Find the first existing pose estimation config file in the `kpset_dir` directory, if not found, raise an error.
        4. Check that the pose_estimation_method is `deeplabcut` and set up the project output directory with the default `config.yml`.
        5. Create the `kpms_project_output_dir` (if it does not exist), and generates the kpms default `config.yml` with the default values from the pose estimation config.
        6. Create a copy of the kpms `config.yml` named `kpms_dj_config.yml` that will be updated with both the `video_dir` and bodyparts
        7. Load keypoint data from the keypoint files found in the `kpset_dir` that will serve as the training set.
        8. Detect and remove outlier keypoints using medoid distance analysis, then interpolate missing values.
        9. Calculate the average frame rate and the frame rate list of the videoset from which the keypoint set is derived. These two attributes can be used to calculate the kappa value.
        10. Insert the results of this `make` function into the table.
        """

        anterior_bodyparts, posterior_bodyparts, use_bodyparts = (
            Bodyparts & key
        ).fetch1(
            "anterior_bodyparts",
            "posterior_bodyparts",
            "use_bodyparts",
        )

        pose_estimation_method, kpset_dir = (KeypointSet & key).fetch1(
            "pose_estimation_method", "kpset_dir"
        )
        video_paths, video_ids = (KeypointSet.VideoFile & key).fetch(
            "video_path", "video_id"
        )

        kpms_project_output_dir, task_mode = (PCATask & key).fetch1(
            "kpms_project_output_dir", "task_mode"
        )

        outlier_scale_factor = (PCATask & key).fetch1("outlier_scale_factor")

        return (
            anterior_bodyparts,
            posterior_bodyparts,
            use_bodyparts,
            pose_estimation_method,
            kpset_dir,
            video_paths,
            video_ids,
            kpms_project_output_dir,
            task_mode,
            outlier_scale_factor,
        )

    def make_compute(
        self,
        key,
        anterior_bodyparts,
        posterior_bodyparts,
        use_bodyparts,
        pose_estimation_method,
        kpset_dir,
        video_paths,
        video_ids,
        kpms_project_output_dir,
        task_mode,
        outlier_scale_factor,
    ):
        from keypoint_moseq import (
            find_medoid_distance_outliers,
            interpolate_keypoints,
            load_keypoints,
            plot_medoid_distance_outliers,
        )

        if task_mode == "trigger":
            from keypoint_moseq import setup_project

            try:
                kpms_project_output_dir = find_full_path(
                    get_kpms_processed_data_dir(), kpms_project_output_dir
                )

            except FileNotFoundError:
                kpms_project_output_dir = (
                    Path(get_kpms_processed_data_dir()) / kpms_project_output_dir
                )

            kpset_dir = find_full_path(get_kpms_root_data_dir(), kpset_dir)
            videos_dir = find_full_path(
                get_kpms_root_data_dir(), Path(video_paths[0]).parent
            )

            if pose_estimation_method == "deeplabcut":
                from .readers.kpms_reader import _base_config_path

                cfg_path = _base_config_path(kpset_dir)
                if not os.path.exists(cfg_path):
                    raise FileNotFoundError(
                        f"No DLC config.(yml|yaml) found in {kpset_dir}"
                    )
                cfg = Path(cfg_path)
                setup_project(
                    project_dir=kpms_project_output_dir.as_posix(),
                    deeplabcut_config=cfg.as_posix(),
                )

            else:
                raise NotImplementedError(
                    "Currently, `deeplabcut` is the only pose estimation method supported by this Element. Please reach out at `support@datajoint.com` if you use another method."
                )

        else:
            kpms_project_output_dir = find_full_path(
                get_kpms_processed_data_dir(), kpms_project_output_dir
            )
            kpset_dir = find_full_path(get_kpms_root_data_dir(), kpset_dir)
            videos_dir = find_full_path(
                get_kpms_root_data_dir(), Path(video_paths[0]).parent
            )

        raw_coordinates, raw_confidences, formatted_bodyparts = load_keypoints(
            filepath_pattern=kpset_dir, format=pose_estimation_method
        )

        video_metadata_list = []
        frame_rates = []
        for fp, video_id in zip(video_paths, video_ids):
            video_path = (find_full_path(get_kpms_root_data_dir(), fp)).as_posix()
            cap = cv2.VideoCapture(video_path)
            fps = float(cap.get(cv2.CAP_PROP_FPS))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()
            duration_minutes = (frame_count / fps) / 60.0
            frame_rates.append(fps)

            # Get video name for the Video part table
            video_key = {"kpset_id": key["kpset_id"], "video_id": video_id}
            if KeypointSet.VideoFile & video_key:
                video_record = (KeypointSet.VideoFile & video_key).fetch1()
                video_name = Path(
                    video_record["video_path"]
                ).stem  # Get filename without extension
                video_metadata_list.append(
                    {
                        "video_id": video_id,
                        "video_name": video_name,
                        "video_duration": int(duration_minutes),
                        "frame_rate": fps,
                    }
                )
            else:
                logger.warning(f"Video record not found for video_id {video_id}")

        average_frame_rate = float(np.mean(frame_rates))

        # Generate a copy of config.yml with the generated/updated info after it is known
        kpms_reader.dj_generate_config(
            project_dir=kpms_project_output_dir,
            video_dir=str(videos_dir),
            use_bodyparts=list(use_bodyparts),
            anterior_bodyparts=list(anterior_bodyparts),
            posterior_bodyparts=list(posterior_bodyparts),
            outlier_scale_factor=float(outlier_scale_factor),
        )
        kpms_reader.update_kpms_dj_config(
            kpms_project_output_dir,
            fps=average_frame_rate,
        )

        # Remove outlier keypoints
        kpms_config = kpms_reader.load_kpms_dj_config(kpms_project_output_dir)
        cleaned_coordinates = {}
        cleaned_confidences = {}

        for recording_name in raw_coordinates:
            raw_coords = raw_coordinates[recording_name].copy()
            raw_conf = raw_confidences[recording_name].copy()

            # Find outliers using medoid distance analysis
            outliers = find_medoid_distance_outliers(
                raw_coords, outlier_scale_factor=outlier_scale_factor
            )

            # Interpolate keypoints to fix outliers
            cleaned_coords = interpolate_keypoints(raw_coords, outliers["mask"])

            # Update confidences for outlier points
            cleaned_conf = np.where(outliers["mask"], 0, raw_conf)

            cleaned_coordinates[recording_name] = cleaned_coords
            cleaned_confidences[recording_name] = cleaned_conf

            # Plot outliers
            if formatted_bodyparts is not None:
                try:
                    plot_medoid_distance_outliers(
                        project_dir=kpms_project_output_dir.as_posix(),
                        recording_name=recording_name,
                        original_coordinates=raw_coords,
                        interpolated_coordinates=cleaned_coords,
                        outlier_mask=outliers["mask"],
                        outlier_thresholds=outliers["thresholds"],
                        **kpms_config,
                    )

                except Exception as e:
                    logger.warning(
                        f"Could not create outlier plot for {recording_name}: {e}"
                    )

        return (
            cleaned_coordinates,
            cleaned_confidences,
            formatted_bodyparts,
            average_frame_rate,
            video_metadata_list,
        )

    def make_insert(
        self,
        key,
        cleaned_coordinates,
        cleaned_confidences,
        formatted_bodyparts,
        average_frame_rate,
        video_metadata_list,
    ):
        self.insert1(
            dict(
                **key,
                coordinates=cleaned_coordinates,
                confidences=cleaned_confidences,
                formatted_bodyparts=formatted_bodyparts,
                average_frame_rate=average_frame_rate,
            )
        )

        for video_metadata in video_metadata_list:
            self.Video.insert1(
                dict(
                    **key,
                    video_name=video_metadata["video_name"],
                    video_duration=video_metadata["video_duration"],
                    frame_rate=video_metadata["frame_rate"],
                )
            )


@schema
class PCAFit(dj.Computed):
    """Fit Principal Component Analysis (PCA) model for dimensionality reduction of keypoint data.

    Attributes:
        PreProcessing (foreign key)     : `PreProcessing` Key.
        pca_fit_time (datetime)         : datetime of the PCA fitting analysis.
    """

    definition = """
    -> PreProcessing                      # `PreProcessing` Key
    ---
    pca_fit_time=NULL        : datetime  # datetime of the PCA fitting analysis
    """

    def make(self, key):
        """
        Format keypoint data and fit PCA model for dimensionality reduction.

        Args:
            key (dict): `PreProcessing` Key

        High-Level Logic:
        1. Fetch project output directory and load configuration.
        2. Format keypoint data with coordinates and confidences.
        3. Fit PCA model and save as `pca.p` file.
        4. Insert creation datetime into table.
        """
        from keypoint_moseq import fit_pca, format_data, save_pca

        kpms_project_output_dir, task_mode = (PCATask & key).fetch1(
            "kpms_project_output_dir", "task_mode"
        )
        kpms_project_output_dir = (
            Path(get_kpms_processed_data_dir()) / kpms_project_output_dir
        )

        kpms_default_config = kpms_reader.load_kpms_dj_config(kpms_project_output_dir)
        coordinates, confidences = (PreProcessing & key).fetch1(
            "coordinates", "confidences"
        )
        data, _ = format_data(
            **kpms_default_config, coordinates=coordinates, confidences=confidences
        )

        if task_mode == "trigger":
            pca = fit_pca(**data, **kpms_default_config)
            save_pca(pca, kpms_project_output_dir.as_posix())
            creation_datetime = datetime.now(timezone.utc)
        else:
            creation_datetime = None

        self.insert1(dict(**key, pca_fit_time=creation_datetime))


@schema
class LatentDimension(dj.Imported):
    """
    Determine the optimal latent dimension for model fitting based on variance explained by PCA components.

    Attributes:
        PCAFit (foreign key)               : `PCAFit` Key.
        variance_percentage (float)        : Variance threshold. Fixed value to 90%.
        latent_dimension (int)             : Number of principal components required to explain the specified variance.
        latent_dim_desc (varchar)          : Automated description of the computation result.
    """

    definition = """
    -> PCAFit                                   # `PCAFit` Key
    ---
    variance_percentage      : float            # Variance threshold. Fixed value to 90 percent.
    latent_dimension         : int              # Number of principal components required to explain the specified variance.
    latent_dim_desc          : varchar(1000)    # Automated description of the computation result.
    """

    def make(self, key):
        """
        Compute and store the optimal latent dimension based on 90% variance threshold.

        Args:
            key (dict): `PCAFit` Key.

        Raises:
            FileNotFoundError: No PCA model found in project directory.

        High-Level Logic:
        1. Fetch project output directory and load PCA model.
        2. Calculate cumulative explained variance ratio.
        3. Determine number of components needed for 90% variance.
        4. Insert results into table.
        """

        VARIANCE_THRESHOLD = 0.90

        from keypoint_moseq import load_pca

        kpms_project_output_dir = (PCATask & key).fetch1("kpms_project_output_dir")
        kpms_project_output_dir = (
            Path(get_kpms_processed_data_dir()) / kpms_project_output_dir
        )

        pca_path = kpms_project_output_dir / "pca.p"
        if pca_path.exists():
            pca = load_pca(kpms_project_output_dir.as_posix())
        else:
            raise FileNotFoundError(
                f"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}"
            )

        cs = np.cumsum(
            pca.explained_variance_ratio_
        )  # explained_variance_ratio_ndarray of shape (n_components,)

        if cs[-1] < VARIANCE_THRESHOLD:
            latent_dimension = len(cs)
            variance_percentage = cs[-1] * 100
            latent_dim_desc = (
                f"All components together only explain {cs[-1]*100}% of variance."
            )
        else:
            latent_dimension = (cs > VARIANCE_THRESHOLD).nonzero()[0].min() + 1
            variance_percentage = VARIANCE_THRESHOLD * 100
            latent_dim_desc = f">={VARIANCE_THRESHOLD*100}% of variance explained by {(cs>VARIANCE_THRESHOLD).nonzero()[0].min()+1} components."

        self.insert1(
            dict(
                **key,
                variance_percentage=variance_percentage,
                latent_dimension=latent_dimension,
                latent_dim_desc=latent_dim_desc,
            )
        )


@schema
class PreFitTask(dj.Manual):
    """Define parameters for Stage 1: Auto-Regressive Hidden Markov Model (AR-HMM) pre-fitting.

    Attributes:
        PCAFit (foreign key)                : `PCAFit` task.
        pre_latent_dim (int)                : Latent dimension to use for the model pre-fitting.
        pre_kappa (int)                     : Kappa value to use for the model pre-fitting (controls syllable duration).
        pre_num_iterations (int)            : Number of Gibbs sampling iterations to run in the model pre-fitting (typically 10-50).
        model_name (varchar)                : Name of the model to be loaded if `task_mode='load'`
        task_mode (enum)                    : 'load': load computed analysis results, 'trigger': trigger computation
        pre_fit_desc(varchar)               : User-defined description of the pre-fitting task.
    """

    definition = """
    -> PCAFit                                            # `PCAFit` Key
    pre_latent_dim               : int                   # Latent dimension to use for the model pre-fitting.
    pre_kappa                    : int                   # Kappa value to use for the model pre-fitting (controls syllable duration).
    pre_num_iterations           : int                   # Number of Gibbs sampling iterations to run in the model pre-fitting (typically 10-50).
    ---
    model_name=''                : varchar(1000)         # Name of the model to be loaded if `task_mode='load'`
    task_mode='load'             :enum('load','trigger') # 'load': load computed analysis results, 'trigger': trigger computation
    pre_fit_desc=''              : varchar(1000)         # User-defined description of the pre-fitting task
    """


@schema
class PreFit(dj.Computed):
    """Fit Auto-Regressive Hidden Markov Model (AR-HMM) for initial behavioral syllable discovery.

    Attributes:
        PreFitTask (foreign key)                : `PreFitTask` Key.
        model_name (varchar)                    : Name of the model as "model_name".
        pre_fit_duration (float)                : Time duration (seconds) of the model fitting computation.
    """

    definition = """
    -> PreFitTask                                # `PreFitTask` Key
    ---
    model_name=''                : varchar(1000) # Name of the model as "kpms_project_output_dir/model_name"
    pre_fit_duration=NULL        : float         # Time duration (seconds) of the model fitting computation
    """

    def make(self, key):
        """
        Fit AR-HMM model for initial behavioral syllable discovery.

        Args:
            key (dict): Dictionary with the `PreFitTask` Key.

        Raises:
            FileNotFoundError: No PCA model found in project directory.

        High-Level Logic:
        1. Fetch project output directory and model parameters.
        2. Update configuration with latent dimension and kappa values.
        3. Load PCA model and format keypoint data.
        4. Initialize and fit AR-HMM model.
        5. Calculate fitting duration and insert results.
        """
        from keypoint_moseq import (
            fit_model,
            format_data,
            init_model,
            load_pca,
            update_hypparams,
        )

        kpms_project_output_dir = find_full_path(
            get_kpms_processed_data_dir(),
            (PCATask & key).fetch1("kpms_project_output_dir"),
        )

        pre_latent_dim, pre_kappa, pre_num_iterations, task_mode, model_name = (
            PreFitTask & key
        ).fetch1(
            "pre_latent_dim",
            "pre_kappa",
            "pre_num_iterations",
            "task_mode",
            "model_name",
        )
        if task_mode == "trigger":
            from keypoint_moseq import estimate_sigmasq_loc

            # Update the existing kpms_dj_config.yml with new latent_dim and kappa values
            kpms_reader.update_kpms_dj_config(
                kpms_project_output_dir,
                latent_dim=int(pre_latent_dim),
                kappa=float(pre_kappa),
            )

            # Load the updated config for use in model fitting
            kpms_dj_config = kpms_reader.load_kpms_dj_config(kpms_project_output_dir)

            pca_path = kpms_project_output_dir / "pca.p"
            if pca_path.exists():
                pca = load_pca(kpms_project_output_dir.as_posix())
            else:
                raise FileNotFoundError(
                    f"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}"
                )

            coordinates, confidences = (PreProcessing & key).fetch1(
                "coordinates", "confidences"
            )
            data, metadata = format_data(
                coordinates=coordinates, confidences=confidences, **kpms_dj_config
            )

            kpms_reader.update_kpms_dj_config(
                kpms_project_output_dir,
                sigmasq_loc=estimate_sigmasq_loc(
                    data["Y"], data["mask"], filter_size=int(kpms_dj_config["fps"])
                ),
            )

            kpms_dj_config = kpms_reader.load_kpms_dj_config(
                project_dir=kpms_project_output_dir
            )

            model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)

            model = update_hypparams(
                model, kappa=float(pre_kappa), latent_dim=int(pre_latent_dim)
            )

            model_name_str = f"latent_dim_{int(pre_latent_dim)}_kappa_{float(pre_kappa)}_iters_{int(pre_num_iterations)}"

            start_time = datetime.now(timezone.utc)
            model, model_name = fit_model(
                model=model,
                model_name=model_name_str,
                data=data,
                metadata=metadata,
                project_dir=kpms_project_output_dir.as_posix(),
                ar_only=True,
                num_iters=pre_num_iterations,
                generate_progress_plots=True,  # saved to {project_dir}/{model_name}/plots/
                save_every_n_iters=25,
            )
            end_time = datetime.now(timezone.utc)

            duration_seconds = (end_time - start_time).total_seconds()
        else:
            duration_seconds = None

        self.insert1(
            {
                **key,
                "model_name": (
                    kpms_project_output_dir.relative_to(get_kpms_processed_data_dir())
                    / model_name
                ).as_posix(),
                "pre_fit_duration": duration_seconds,
            }
        )


@schema
class FullFitTask(dj.Manual):
    """Define parameters for FullFit step of model fitting.

    Attributes:
        PCAFit (foreign key)                 : `PCAFit` Key.
        full_latent_dim (int)                : Latent dimension to use for the model full fitting.
        full_kappa (int)                     : Kappa value to use for the model full fitting (typically lower than pre-fit kappa).
        full_num_iterations (int)            : Number of Gibbs sampling iterations to run in the model full fitting (typically 200-500).
        model_name (varchar)                 : Name of the model to be loaded if `task_mode='load'`
        task_mode (enum)                     : 'load': load computed analysis results, 'trigger': trigger computation
        full_fit_desc(varchar)               : User-defined description of the model full fitting task.
    """

    definition = """
    -> PCAFit                                           # `PCAFit` Key
    full_latent_dim              : int                  # Latent dimension to use for the model full fitting
    full_kappa                   : int                  # Kappa value to use for the model full fitting (typically lower than pre-fit kappa).
    full_num_iterations          : int                  # Number of Gibbs sampling iterations to run in the model full fitting (typically 200-500).
    ---
    model_name=''                : varchar(1000)        # Name of the model to be loaded if `task_mode='load'`
    task_mode='load'             :enum('load','trigger')# Trigger or load the task
    full_fit_desc=''             : varchar(1000)        # User-defined description of the model full fitting task
    """


@schema
class FullFit(dj.Computed):
    """Fit the complete Keypoint Switching Linear Dynamical System (Keypoint-SLDS) model.

    Attributes:
        FullFitTask (foreign key)            : `FullFitTask` Key.
        model_name                           : varchar(100) # Name of the model as "kpms_project_output_dir/model_name"
        full_fit_duration (float)            : Time duration (seconds) of the full fitting computation
    """

    definition = """
    -> FullFitTask                                # `FullFitTask` Key
    ---
    model_name=''                 : varchar(1000) # Name of the model as "kpms_project_output_dir/model_name"
    full_fit_duration=NULL        : float         # Time duration (seconds) of the full fitting computation
    """

    def make(self, key):
        """
        Fit the complete Keypoint-SLDS model with spatial and temporal dynamics.

        Args:
            key (dict): Dictionary with the `FullFitTask` Key.

        Raises:
            FileNotFoundError: No PCA model found in project directory.

        High-Level Logic:
        1. Fetch project output directory and model parameters.
        2. Update configuration with latent dimension and kappa values.
        3. Load PCA model and format keypoint data.
        4. Initialize and fit Keypoint-SLDS model.
        5. Reindex syllable labels by frequency.
        6. Calculate fitting duration and insert results.
        """
        from keypoint_moseq import (
            estimate_sigmasq_loc,
            fit_model,
            format_data,
            init_model,
            load_pca,
            reindex_syllables_in_checkpoint,
            update_hypparams,
        )

        kpms_project_output_dir = find_full_path(
            get_kpms_processed_data_dir(),
            (PCATask & key).fetch1("kpms_project_output_dir"),
        )

        full_latent_dim, full_kappa, full_num_iterations, task_mode, model_name = (
            FullFitTask & key
        ).fetch1(
            "full_latent_dim",
            "full_kappa",
            "full_num_iterations",
            "task_mode",
            "model_name",
        )
        if task_mode == "trigger":
            kpms_reader.update_kpms_dj_config(
                project_dir=kpms_project_output_dir,
                latent_dim=int(full_latent_dim),
                kappa=float(full_kappa),
            )

            kpms_dj_config = kpms_reader.load_kpms_dj_config(
                project_dir=kpms_project_output_dir
            )

            pca_path = kpms_project_output_dir / "pca.p"
            if pca_path.exists():
                pca = load_pca(kpms_project_output_dir.as_posix())
            else:
                raise FileNotFoundError(
                    f"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}"
                )

            coordinates, confidences = (PreProcessing & key).fetch1(
                "coordinates", "confidences"
            )
            data, metadata = format_data(
                coordinates=coordinates, confidences=confidences, **kpms_dj_config
            )
            kpms_reader.update_kpms_dj_config(
                project_dir=kpms_project_output_dir,
                sigmasq_loc=estimate_sigmasq_loc(
                    data["Y"], data["mask"], filter_size=int(kpms_dj_config["fps"])
                ),
            )

            kpms_dj_config = kpms_reader.load_kpms_dj_config(
                project_dir=kpms_project_output_dir
            )

            model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)
            model = update_hypparams(
                model, kappa=float(full_kappa), latent_dim=int(full_latent_dim)
            )

            model_name_str = f"latent_dim_{int(full_latent_dim)}_kappa_{float(full_kappa)}_iters_{int(full_num_iterations)}"

            start_time = datetime.now(timezone.utc)
            model, model_name = fit_model(
                model=model,
                model_name=model_name_str,
                data=data,
                metadata=metadata,
                project_dir=kpms_project_output_dir.as_posix(),
                ar_only=False,
                num_iters=full_num_iterations,
            )
            end_time = datetime.now(timezone.utc)
            duration_seconds = (end_time - start_time).total_seconds()

            reindex_syllables_in_checkpoint(
                project_dir=kpms_project_output_dir.as_posix(),
                model_name=Path(model_name).parts[-1],
            )

        else:
            duration_seconds = None

        self.insert1(
            {
                **key,
                "model_name": (
                    kpms_project_output_dir.relative_to(get_kpms_processed_data_dir())
                    / model_name
                ).as_posix(),
                "full_fit_duration": duration_seconds,
            }
        )


@schema
class SelectedFullFit(dj.Manual):
    """Register selected FullFit models for use in the inference pipeline.

    Attributes:
        FullFit (foreign key)          : `FullFit` Key.
        registered_model_name (varchar): User-friendly model name
        registered_model_desc (varchar): Optional user-defined description
    """

    definition = """
    -> FullFit
    ---
    registered_model_name         : varchar(1000)   # User-friendly model name
    registered_model_desc=''      : varchar(1000) # Optional user-defined description
    """
