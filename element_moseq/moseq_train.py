"""
Code adapted from the Datta Lab
DataJoint Schema for Keypoint-MoSeq training pipeline
"""

import importlib
import inspect
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

import cv2
import datajoint as dj
import matplotlib.pyplot as plt
import numpy as np
import yaml
from element_interface.utils import find_full_path

from .plotting import viz_utils
from .readers import kpms_reader

schema = dj.schema()
_linking_module = None
logger = dj.logger


def activate(
    train_schema_name: str,
    *,
    create_schema: bool = True,
    create_tables: bool = True,
    linking_module: str = None,
):
    """Activate this schema.

    Args:
        train_schema_name (str): A string containing the name of the `moseq_train` schema.
        create_schema (bool): If True (default), schema will be created in the database.
        create_tables (bool): If True (default), tables related to the schema will be created in the database.
        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.
    Functions:
        get_kpms_root_data_dir(): Returns absolute path for root data directory/ies
                                  with all behavioral recordings, as (list of) string(s).
        get_kpms_processed_data_dir(): Optional. Returns absolute path for processed
                                       data.
    """

    if isinstance(linking_module, str):
        linking_module = importlib.import_module(linking_module)
    assert inspect.ismodule(
        linking_module
    ), "The argument 'dependency' must be a module's name or a module object"

    assert hasattr(
        linking_module, "get_kpms_root_data_dir"
    ), "The linking module must specify a lookup function for a root data directory"

    global _linking_module
    _linking_module = linking_module

    # activate
    schema.activate(
        train_schema_name,
        create_schema=create_schema,
        create_tables=create_tables,
        add_objects=_linking_module.__dict__,
    )


# -------------- Functions required by element-moseq ---------------


def get_kpms_root_data_dir() -> list:
    """Fetches absolute data path to kpms data directories.

    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.

    Returns:
        A list of the absolute path(s) to kpms data directories.
    """

    root_directories = _linking_module.get_kpms_root_data_dir()
    if isinstance(root_directories, (str, Path)):
        root_directories = [root_directories]

    if (
        hasattr(_linking_module, "get_kpms_processed_data_dir")
        and get_kpms_processed_data_dir() not in root_directories
    ):
        root_directories.append(_linking_module.get_kpms_processed_data_dir())

    return root_directories


def get_kpms_processed_data_dir() -> Optional[str]:
    """Retrieve the root directory for all processed data.

    Returns:
        A string for the full path to the root directory for processed data.
    """
    if hasattr(_linking_module, "get_kpms_processed_data_dir"):
        return _linking_module.get_kpms_processed_data_dir()
    else:
        return None


# ----------------------------- Table declarations ----------------------


@schema
class PoseEstimationMethod(dj.Lookup):
    """Name of the pose estimation method supported by the keypoint loader of `keypoint-moseq` package.

    Attributes:
        pose_estimation_method  (str): Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)
        pose_estimation_desc    (str): Optional. Pose estimation method description with the supported formats.
    """

    definition = """
    # Pose estimation methods supported by the keypoint loader of `keypoint-moseq` package.
    pose_estimation_method  : char(15)         # Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)
    ---
    pose_estimation_desc    : varchar(1000)    # Optional. Pose estimation method description with the supported formats.
    """

    contents = [
        ["deeplabcut", "`.csv` and `.h5/.hdf5` files generated by DeepLabcut analysis"],
        ["sleap", "`.slp` and `.h5/.hdf5` files generated by SLEAP analysis"],
        ["anipose", "`.csv` files generated by anipose analysis"],
        ["sleap-anipose", "`.h5/.hdf5` files generated by sleap-anipose analysis"],
        ["nwb", "`.nwb` files with Neurodata Without Borders (NWB) format"],
        ["facemap", "`.h5` files generated by Facemap analysis"],
    ]


@schema
class KeypointSet(dj.Manual):
    """Store the keypoint data and the video set directory for model training.

    Attributes:
        kpset_id (int)                          : Unique ID for each keypoint set.
        PoseEstimationMethod (foreign key)      : Unique format method used to obtain the keypoints data.
        kpset_dir (str)                         : Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory.
        kpset_desc (str)                        : Optional. User-entered description.
    """

    definition = """
    kpset_id                        : int           # Unique ID for each keypoint set
    ---
    -> PoseEstimationMethod                         # Unique format method used to obtain the keypoints data
    kpset_dir                       : varchar(255)  # Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory
    kpset_desc=''                   : varchar(1000) # Optional. User-entered description
    """

    class VideoFile(dj.Part):
        """Store the IDs and file paths of each video file that will be used for model training.

        Attributes:
            KeypointSet (foreign key) : Unique ID for each keypoint set.
            video_id (int)            : Unique ID for each video corresponding to each keypoint data file, relative to root data directory.
            video_path (str)          : Filepath of each video from which the keypoints are derived, relative to root data directory.
        """

        definition = """
        -> master
        video_id                            : int           # Unique ID for each video corresponding to each keypoint data file, relative to root data directory
        ---
        video_path                          : varchar(1000) # Filepath of each video (e.g., `.mp4`) from which the keypoints are derived, relative to root data directory
        pose_estimation_path=''             : varchar(1000) # Filepath of each pose estimation file (e.g., `.h5`) that contains the keypoints, relative to root data directory
        """


@schema
class BodyParts(dj.Manual):
    """Store the body parts to use in the analysis.

    Attributes:
        KeypointSet (foreign key)       : Unique ID for each `KeypointSet` key.
        bodyparts_id (int)              : Unique ID for a set of bodyparts for a particular keypoint set.
        anterior_bodyparts (blob)       : List of strings of anterior bodyparts
        posterior_bodyparts (blob)      : List of strings of posterior bodyparts
        use_bodyparts (blob)            : List of strings of bodyparts to be used
        bodyparts_desc (varchar)        : Optional. User-entered description.
    """

    definition = """
    -> KeypointSet                              # Unique ID for each `KeypointSet` key
    bodyparts_id                : int           # Unique ID for a set of bodyparts for a particular keypoint set
    ---
    anterior_bodyparts          : blob          # List of strings of anterior bodyparts
    posterior_bodyparts         : blob          # List of strings of posterior bodyparts
    use_bodyparts               : blob          # List of strings of bodyparts to be used
    bodyparts_desc=''           : varchar(1000) # Optional. User-entered description
    """


@schema
class PCATask(dj.Manual):
    """
    Define the Principal Component Analysis (PCA) task for dimensionality reduction of keypoint data.

    Attributes:
        BodyParts (foreign key)         : Unique ID for each `BodyParts` key
        outlier_scale_factor (float)    : Scale factor for outlier detection in keypoint data (default: 6)
        kpms_project_output_dir (str)   : Optional. Keypoint-MoSeq project output directory, relative to root data directory
        task_mode (enum)                : 'load' to load existing results, 'trigger' to compute new PCA
    """

    definition = """
    -> BodyParts                                            # Unique ID for each `BodyParts` key
    ---
    outlier_scale_factor=6          : float                 # Scale factor for outlier detection in keypoint data (default: 6)
    kpms_project_output_dir=''      : varchar(255)          # Optional. Keypoint-MoSeq project output directory, relative to root data directory
    task_mode='load'                :enum('load','trigger') # 'load' to load existing results, 'trigger' to compute new PCA
    """


@schema
class PreProcessing(dj.Computed):
    """
    Preprocess keypoint data by cleaning outliers and setting up the Keypoint-MoSeq project configuration.

    Attributes:
        PCATask (foreign key)           : Unique ID for each `PCATask` key.
        formatted_bodyparts (longblob)  : List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.
        coordinates (longblob)          : Cleaned coordinates dictionary {recording_name: array} after outlier removal.
        confidences (longblob)          : Cleaned confidences dictionary {recording_name: array} after outlier removal.
        average_frame_rate (float)      : Average frame rate of the videos for model training (used for kappa calculation).
        pre_processing_time (datetime)  : datetime of the preprocessing execution.
        pre_processing_duration (int)   : Execution time of the preprocessing in seconds.
    """

    definition = """
    -> PCATask                          # Unique ID for each `PCATask` key
    ---
    formatted_bodyparts      : longblob  # List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.
    coordinates              : longblob  # Cleaned coordinates dictionary (recording_name: array) after outlier removal
    confidences              : longblob  # Cleaned confidences dictionary (recording_name: array) after outlier removal
    average_frame_rate       : float     # Average frame rate of the videos for model training (used for kappa calculation).
    pre_processing_time      : datetime  # datetime of the preprocessing execution.
    pre_processing_duration  : int       # Execution time of the preprocessing in seconds.
    """

    class Video(dj.Part):
        definition = """
        -> master
        video_id: varchar(255)
        ---
        video_duration              : int           # Duration of each video in minutes
        frame_rate                  : float         # Frame rate of the video in frames per second (Hz)
        file_size                   : float         # File size of the video in megabytes (MB)
        """

    class OutlierRemoval(dj.Part):
        """Store outlier detection QA plots per video."""

        definition = """
        -> master
        video_id: varchar(255)
        ---
        outlier_plot: attach  # QA visualization showing detected outliers and interpolation.
        """

    class ConfigFile(dj.Part):
        """
        Store the configuration files (first creation of the config file and the updates after processing).
        """

        definition = """
        -> master
        ---
        base_config_file: attach  # KPMS config attachment. Stored as binary in database.
        config_file: attach       # Updated KPMS DJ config attachment. Stored as binary in database.
        """

    def make_fetch(self, key):
        """
        Fetch required data for preprocessing from database tables.
        """
        anterior_bodyparts, posterior_bodyparts, use_bodyparts = (
            BodyParts & key
        ).fetch1(
            "anterior_bodyparts",
            "posterior_bodyparts",
            "use_bodyparts",
        )
        pose_estimation_method, kpset_dir = (KeypointSet & key).fetch1(
            "pose_estimation_method", "kpset_dir"
        )
        keypoint_videofile_metadata = (KeypointSet.VideoFile & key).fetch(as_dict=True)
        kpms_project_output_dir, task_mode, outlier_scale_factor = (
            PCATask & key
        ).fetch1("kpms_project_output_dir", "task_mode", "outlier_scale_factor")

        return (
            anterior_bodyparts,
            posterior_bodyparts,
            use_bodyparts,
            pose_estimation_method,
            kpset_dir,
            keypoint_videofile_metadata,
            kpms_project_output_dir,
            task_mode,
            outlier_scale_factor,
        )

    def make_compute(
        self,
        key,
        anterior_bodyparts,
        posterior_bodyparts,
        use_bodyparts,
        pose_estimation_method,
        kpset_dir,
        keypoint_videofile_metadata,
        kpms_project_output_dir,
        task_mode,
        outlier_scale_factor,
    ):
        """
        Compute preprocessing steps including outlier removal and video metadata extraction.

        Args:
            key (dict): Primary key from the `PCATask` table.
            anterior_bodyparts (list): List of anterior bodyparts.
            posterior_bodyparts (list): List of posterior bodyparts.
            use_bodyparts (list): List of bodyparts to use.
            pose_estimation_method (str): Pose estimation method (e.g., 'deeplabcut').
            kpset_dir (str): Keypoint set directory path.
            video_paths (list): List of video file paths.
            video_ids (list): List of video IDs.
            kpms_project_output_dir (str): Project output directory path.
            task_mode (str): Task mode ('load' or 'trigger').
            outlier_scale_factor (int): Scale factor for outlier detection.

        Returns:
            tuple: Processed data including cleaned coordinates, confidences, and video metadata.

        Raises:
            NotImplementedError: Only `deeplabcut` pose estimation method is supported.
            FileNotFoundError: No DLC config file found in `kpset_dir`.

        High-Level Logic:
        1. Find the first existing pose estimation config file in the `kpset_dir` directory, if not found, raise an error.
        2. Check that the pose_estimation_method is `deeplabcut` and set up the project output directory with the default `config.yml`.
        3. Create the `kpms_project_output_dir` (if it does not exist), and generates the kpms default `config.yml` with the default values from the pose estimation config.
        4. Create a copy of the kpms `config.yml` named `kpms_dj_config.yml` that will be updated with both the `video_dir` and bodyparts
        5. Load keypoint data from the keypoint files found in the `kpset_dir` that will serve as the training set.
        6. Detect and remove outlier keypoints using medoid distance analysis, then interpolate missing values.
        7. Calculate the average frame rate and the frame rate list of the videoset from which the keypoint set is derived. These two attributes can be used to calculate the kappa value.
        """
        from keypoint_moseq import (
            find_medoid_distance_outliers,
            interpolate_keypoints,
            load_keypoints,
        )

        from .plotting.viz_utils import plot_medoid_distance_outliers

        execution_time = datetime.now(timezone.utc)
        if task_mode == "trigger":
            from keypoint_moseq import setup_project

            # check if the project output directory exists
            try:
                kpms_project_output_dir = find_full_path(
                    get_kpms_processed_data_dir(), kpms_project_output_dir
                )
            # if the project output directory does not exist, create it
            except FileNotFoundError:
                kpms_project_output_dir = (
                    Path(get_kpms_processed_data_dir()) / kpms_project_output_dir
                )
            kpset_dir = find_full_path(get_kpms_root_data_dir(), kpset_dir)
            # Setup of the project creates KPMS base `config.yml` file copying the pose estimation config file
            from .readers.kpms_reader import _pose_estimation_config_path

            pose_estimation_config_file = Path(_pose_estimation_config_path(kpset_dir))
            if not pose_estimation_config_file.exists():
                raise FileNotFoundError(
                    f"No `config.yml` or `config.yaml` file found in {kpset_dir}"
                )
            if pose_estimation_method == "deeplabcut":
                setup_project(
                    project_dir=kpms_project_output_dir.as_posix(),
                    deeplabcut_config=pose_estimation_config_file.as_posix(),
                )
            else:
                raise NotImplementedError(
                    "Currently, `deeplabcut` is the only pose estimation method supported by this Element. Please reach out at `support@datajoint.com` if you use another method."
                )
        # task mode is load
        else:
            kpms_project_output_dir = find_full_path(
                get_kpms_processed_data_dir(), kpms_project_output_dir
            )
            kpset_dir = find_full_path(get_kpms_root_data_dir(), kpset_dir)

        # Format keypoint data
        raw_coordinates, raw_confidences, formatted_bodyparts = load_keypoints(
            filepath_pattern=kpset_dir, format=pose_estimation_method
        )

        # Confirm that `use_bodyparts` are a subset of `formatted_bodyparts`
        if not set(use_bodyparts).issubset(set(formatted_bodyparts)):
            raise ValueError(
                f"use_bodyparts ({use_bodyparts}) is not a subset of formatted bodyparts ({formatted_bodyparts})"
            )

        # Extract frame rate and file size from keypoint video files
        video_metadata_dict = dict()
        frame_rates = []
        for row in keypoint_videofile_metadata:
            video_id = int(row["video_id"])
            video_path = find_full_path(get_kpms_root_data_dir(), row["video_path"])

            # Get file size in MB (rounded to 2 decimal places)
            file_size_mb = round(video_path.stat().st_size / (1024 * 1024), 2)

            # Get video properties
            cap = cv2.VideoCapture(video_path.as_posix())
            fps = float(cap.get(cv2.CAP_PROP_FPS))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()
            if fps <= 0:
                raise ValueError(
                    f"Invalid FPS ({fps}) for video_id {video_id} at {video_path}"
                )
            duration_minutes = int((frame_count / fps) / 60.0)
            frame_rates.append(fps)
            video_metadata_dict[video_id] = {
                "video_duration": duration_minutes,
                "frame_rate": fps,
                "file_size": file_size_mb,
                "outlier_plot": None,
            }
        average_frame_rate = float(np.mean(frame_rates))

        # Get all unique parent directories for all video files
        parent_dirs = {
            Path(video["video_path"]).parent for video in keypoint_videofile_metadata
        }
        # Check if there is only one unique parent
        if len(parent_dirs) > 1:
            raise ValueError(
                f"Videos are located in multiple directories: {parent_dirs}. All videos must be in the same directory."
            )
        videos_dir = find_full_path(
            get_kpms_root_data_dir(),
            Path(keypoint_videofile_metadata[0]["video_path"]).parent,
        )

        # Generate a new KPMS DJ config file copying the KPMS base config file in the same kpms project output directory
        (
            kpms_dj_config_path,
            kpms_dj_config_dict,
            kpms_base_config_path,
            kpms_base_config_dict,
        ) = kpms_reader.dj_generate_config(
            kpms_project_dir=kpms_project_output_dir,
            video_dir=str(videos_dir),
            use_bodyparts=list(use_bodyparts),
            anterior_bodyparts=list(anterior_bodyparts),
            posterior_bodyparts=list(posterior_bodyparts),
            outlier_scale_factor=float(outlier_scale_factor),
        )

        # Get absolute paths for attach fields
        kpms_dj_config_path = find_full_path(
            get_kpms_processed_data_dir(), kpms_dj_config_path
        )
        kpms_base_config_path = find_full_path(
            get_kpms_processed_data_dir(), kpms_base_config_path
        )

        # Update the KPMS DJ config file on disk with the average frame rate
        kpms_dj_config_dict = kpms_reader.update_kpms_dj_config(
            kpms_project_dir=kpms_project_output_dir, fps=average_frame_rate
        )

        # Clean outlier keypoints and generate plots
        cleaned_coordinates = {}
        cleaned_confidences = {}

        for row in keypoint_videofile_metadata:
            video_id = int(row["video_id"])
            pose_estimation_path = row["pose_estimation_path"]
            pose_estimation_name = Path(pose_estimation_path).stem
            raw_coords = raw_coordinates[pose_estimation_name].copy()
            raw_conf = raw_confidences[pose_estimation_name].copy()
            outliers = find_medoid_distance_outliers(
                raw_coords, outlier_scale_factor=outlier_scale_factor
            )
            cleaned_coords = interpolate_keypoints(raw_coords, outliers["mask"])
            cleaned_conf = np.where(outliers["mask"], 0, raw_conf)
            cleaned_coordinates[pose_estimation_name] = cleaned_coords
            cleaned_confidences[pose_estimation_name] = cleaned_conf
            _, outlier_plot_path = plot_medoid_distance_outliers(
                project_dir=kpms_project_output_dir.as_posix(),
                recording_name=pose_estimation_name,
                original_coordinates=raw_coords,
                interpolated_coordinates=cleaned_coords,
                outlier_mask=outliers["mask"],
                outlier_thresholds=outliers["thresholds"],
                **kpms_dj_config_dict,
            )  # outlier plot stored at kpms_project_output_dir/QA/plots/keypoint_distance_outliers/f"{pose_estimation_name}.png
            video_metadata_dict[video_id] = {
                **video_metadata_dict[video_id],
                "outlier_plot_path": outlier_plot_path,
            }

        completion_time = datetime.now(timezone.utc)
        if task_mode == "trigger":
            duration_seconds = (completion_time - execution_time).total_seconds()
        else:
            duration_seconds = None

        return (
            cleaned_coordinates,
            cleaned_confidences,
            formatted_bodyparts,
            average_frame_rate,
            video_metadata_dict,
            kpms_dj_config_path,
            kpms_base_config_path,
            execution_time,
            duration_seconds,
        )

    def make_insert(
        self,
        key,
        cleaned_coordinates,
        cleaned_confidences,
        formatted_bodyparts,
        average_frame_rate,
        video_metadata_dict,
        kpms_dj_config_path,
        kpms_base_config_path,
        execution_time,
        duration_seconds,
    ):
        """
        Insert processed data into the PreProcessing table and part tables.
        """

        # Insert in the main table
        self.insert1(
            {
                **key,
                "formatted_bodyparts": formatted_bodyparts,
                "coordinates": cleaned_coordinates,
                "confidences": cleaned_confidences,
                "average_frame_rate": average_frame_rate,
                "pre_processing_time": execution_time,
                "pre_processing_duration": duration_seconds,
            }
        )

        # Insert video metadata in Video table
        if video_metadata_dict:
            self.Video.insert(
                [
                    {
                        **key,
                        "video_id": vid,
                        "video_duration": meta["video_duration"],
                        "frame_rate": meta["frame_rate"],
                        "file_size": meta["file_size"],
                    }
                    for vid, meta in video_metadata_dict.items()
                ]
            )

        # Insert outlier removal QA plots
        if video_metadata_dict:
            self.OutlierRemoval.insert(
                [
                    {
                        **key,
                        "video_id": vid,
                        "outlier_plot": meta["outlier_plot_path"],
                    }
                    for vid, meta in video_metadata_dict.items()
                ]
            )
        # Insert configuration files
        self.ConfigFile.insert1(
            {
                **key,
                "config_file": kpms_dj_config_path,
                "base_config_file": kpms_base_config_path,
            }
        )


@schema
class PCAFit(dj.Computed):
    """Fit Principal Component Analysis (PCA) model for dimensionality reduction of keypoint data.

    Attributes:
        PreProcessing (foreign key)     : `PreProcessing` Key.
        pca_fit_time (datetime)         : datetime of the PCA fitting analysis.
        pca_fit_duration (int)          : Execution time of the PCA fitting analysis in seconds.
    """

    definition = """
    -> PreProcessing                      # `PreProcessing` Key
    ---
    pca_fit_time=NULL        : datetime  # datetime of the PCA fitting analysis
    pca_fit_duration=NULL    : int       # Execution time of the PCA fitting analysis in seconds.
    """

    class File(dj.Part):
        """
        Store the PCA files (pca.p file).
        """

        definition = """
        -> master
        ---
        file_name    : varchar(1000)    # name of the pca file (e.g. 'pca.p').
        file_path    : filepath@moseq-train-processed   # path to the pca file (relative to the project output directory).
        """

    def make(self, key):
        """
        Format keypoint data and fit PCA model for dimensionality reduction.

        Args:
            key (dict): `PreProcessing` Key

        High-Level Logic:
        1. Fetch project output directory and load configuration.
        2. Format keypoint data with coordinates and confidences.
        3. Fit PCA model and save as `pca.p` file.
        4. Insert creation datetime into table.
        """
        import tempfile

        from keypoint_moseq import fit_pca, format_data, save_pca

        kpms_project_output_dir, task_mode = (PCATask & key).fetch1(
            "kpms_project_output_dir", "task_mode"
        )
        kpms_project_output_dir = (
            Path(get_kpms_processed_data_dir()) / kpms_project_output_dir
        )
        coordinates, confidences = (PreProcessing & key).fetch1(
            "coordinates", "confidences"
        )

        # Load the configuration from database
        kpms_dj_config_path = (PreProcessing.ConfigFile & key).fetch1("config_file")
        kpms_dj_config_dict = kpms_reader.load_kpms_dj_config(
            config_path=kpms_dj_config_path
        )

        execution_time = datetime.now(timezone.utc)

        # Format keypoint data
        data, _ = format_data(
            **kpms_dj_config_dict, coordinates=coordinates, confidences=confidences
        )

        # Fit PCA model and save as `pca.p` file
        if task_mode == "trigger":
            pca = fit_pca(**data, **kpms_dj_config_dict)
            save_pca(pca, kpms_project_output_dir.as_posix())

        # Check for pca.p file
        pca_path = kpms_project_output_dir / "pca.p"
        if not pca_path.exists():
            raise FileNotFoundError(
                f"No pca file (`pca.p`) found in the project directory {kpms_project_output_dir}"
            )

        completion_time = datetime.now(timezone.utc)

        if task_mode == "trigger":
            duration_seconds = (completion_time - execution_time).total_seconds()
        else:
            duration_seconds = None

        # Insert in the main table
        self.insert1(
            {
                **key,
                "pca_fit_time": execution_time,
                "pca_fit_duration": duration_seconds,
            }
        )

        # Insert in File table
        self.File.insert(
            [
                {
                    **key,
                    "file_name": pca_path.name,
                    "file_path": pca_path,
                }
            ]
        )


@schema
class LatentDimension(dj.Computed):
    """
    Determine the optimal latent dimension for model fitting based on variance explained by PCA components.

    Attributes:
        PCAFit (foreign key)               : `PCAFit` Key.
        variance_percentage (float)        : Variance threshold. Fixed value to 90%.
        latent_dimension (int)             : Number of principal components required to explain the specified variance.
        latent_dim_desc (varchar)          : Automated description of the computation result.
    """

    definition = """
    -> PCAFit                                   # `PCAFit` Key
    ---
    variance_percentage      : float            # Variance threshold. Fixed value to 90 percent.
    latent_dimension         : int              # Number of principal components required to explain the specified variance.
    latent_dim_desc          : varchar(1000)    # Automated description of the computation result.
    """

    class Plots(dj.Part):
        """
        Store the PCA visualization plots.
        """

        definition = """
        -> master
        ---
        scree_plot: attach # A cumulative scree plot showing explained variance
        pcs_plot: attach   # A visualization of each Principal Component (PC)
        pcs_xy_plot: attach # A visualization of the Principal Components (PCs) in the XY plane
        """

    def make(self, key):
        """
        Compute and store the optimal latent dimension based on 90% variance threshold.

        Args:
            key (dict): `PCAFit` Key.

        Raises:
            FileNotFoundError: No PCA model found in project directory.

        High-Level Logic:
        1. Fetch project output directory and load PCA model.
        2. Calculate cumulative explained variance ratio.
        3. Determine number of components needed for 90% variance.
        4. Insert results into table.
        """
        import tempfile

        from keypoint_moseq import load_pca

        VARIANCE_THRESHOLD = 0.90

        kpms_project_output_dir = (PCATask & key).fetch1("kpms_project_output_dir")
        kpms_project_output_dir = (
            Path(get_kpms_processed_data_dir()) / kpms_project_output_dir
        )

        # Fetch PCA file path from upstream PCAFit.File table
        pca_path = (PCAFit.File & key & 'file_name="pca.p"').fetch1("file_path")
        pca = load_pca(Path(pca_path).parent.as_posix())
        cs = np.cumsum(
            pca.explained_variance_ratio_
        )  # explained_variance_ratio_ndarray of shape (n_components,)

        if cs[-1] < VARIANCE_THRESHOLD:
            latent_dimension = len(cs)
            variance_percentage = cs[-1] * 100
            latent_dim_desc = (
                f"All components together only explain {cs[-1]*100}% of variance."
            )
        else:
            latent_dimension = (cs > VARIANCE_THRESHOLD).nonzero()[0].min() + 1
            variance_percentage = VARIANCE_THRESHOLD * 100
            latent_dim_desc = f">={VARIANCE_THRESHOLD*100}% of variance explained by {(cs>VARIANCE_THRESHOLD).nonzero()[0].min()+1} components."

        # Load the configuration from database
        kpms_dj_config_path = (PreProcessing.ConfigFile & key).fetch1("config_file")
        kpms_dj_config_dict = kpms_reader.load_kpms_dj_config(
            config_path=kpms_dj_config_path
        )

        # Generate scree plot
        scree_fig = plt.figure()
        num_pcs = len(pca.components_)
        plt.plot(np.arange(num_pcs) + 1, np.cumsum(pca.explained_variance_ratio_))
        plt.xlabel("PCs")
        plt.ylabel("Explained variance")
        plt.gcf().set_size_inches((2.5, 2))
        plt.grid()
        plt.tight_layout()

        # Generate PCs plot
        pcs_fig = viz_utils.plot_pcs(
            pca,
            interactive=False,
            project_dir=kpms_project_output_dir,
            **kpms_dj_config_dict,
        )

        # Load the pcs-xy.pdf file
        pcs_xy_file = kpms_project_output_dir / "pcs-xy.pdf"

        if not pcs_xy_file.exists():
            raise FileNotFoundError(
                f"No pcs xy file (`pcs-xy.pdf`) found in the project directory {kpms_project_output_dir}"
            )

        # Save plots to temporary directory
        tmpdir = tempfile.TemporaryDirectory()
        fname = f"{key['kpset_id']}_{key['bodyparts_id']}"

        scree_path = Path(tmpdir.name) / f"{fname}_scree_plot.png"
        scree_fig.savefig(scree_path)

        pcs_path = Path(tmpdir.name) / f"{fname}_pcs_plot.png"
        pcs_fig.savefig(pcs_path)

        # Insert main results
        self.insert1(
            dict(
                **key,
                variance_percentage=variance_percentage,
                latent_dimension=latent_dimension,
                latent_dim_desc=latent_dim_desc,
            )
        )

        # Insert plots
        self.Plots.insert1(
            {
                **key,
                "scree_plot": scree_path,
                "pcs_plot": pcs_path,
                "pcs_xy_plot": pcs_xy_file,
            }
        )

        tmpdir.cleanup()


@schema
class PreFitTask(dj.Manual):
    """Define parameters for Stage 1: Auto-Regressive Hidden Markov Model (AR-HMM) pre-fitting.

    Attributes:
        PCAFit (foreign key)                : `PCAFit` task.
        pre_latent_dim (int)                : Latent dimension to use for the model pre-fitting.
        pre_kappa (int)                     : Kappa value to use for the model pre-fitting (controls syllable duration).
        pre_num_iterations (int)            : Number of Gibbs sampling iterations to run in the model pre-fitting (typically 10-50).
        model_name (varchar)                : Name of the model to be loaded if `task_mode='load'`
        task_mode (enum)                    : 'load': load computed analysis results, 'trigger': trigger computation
        pre_fit_desc(varchar)               : User-defined description of the pre-fitting task.
    """

    definition = """
    -> PCAFit                                            # `PCAFit` Key
    pre_latent_dim               : int                   # Latent dimension to use for the model pre-fitting.
    pre_kappa                    : int                   # Kappa value to use for the model pre-fitting (controls syllable duration).
    pre_num_iterations           : int                   # Number of Gibbs sampling iterations to run in the model pre-fitting (typically 10-50).
    ---
    model_name=''                : varchar(1000)         # Name of the model to be loaded if `task_mode='load'`
    task_mode='load'             :enum('load','trigger') # 'load': load computed analysis results, 'trigger': trigger computation
    pre_fit_desc=''              : varchar(1000)         # User-defined description of the pre-fitting task
    """


@schema
class PreFit(dj.Computed):
    """Fit Auto-Regressive Hidden Markov Model (AR-HMM) for initial behavioral syllable discovery.

    Attributes:
        PreFitTask (foreign key)                : `PreFitTask` Key.
        model_name (varchar)                    : Name of the model as "model_name".
        pre_fit_duration (float)                : Time duration (seconds) of the model fitting computation.
    """

    definition = """
    -> PreFitTask                                # `PreFitTask` Key
    ---
    model_name=''                : varchar(1000) # Name of the model as "kpms_project_output_dir/model_name"
    pre_fit_time=NULL            : datetime      # datetime of the model fitting computation.
    pre_fit_duration=NULL        : float         # Time duration (seconds) of the model fitting computation
    """

    class ConfigFile(dj.Part):
        """
        Store the updated configuration file after PreFit computation.
        """

        definition = """
        -> master
        ---
        config_file: attach  # Updated config file after PreFit computation
        """

    class CheckpointFile(dj.Part):
        """
        Store the checkpoint file used for resuming the fitting process.
        """

        definition = """
        -> master
        ---
        checkpoint_file_name: varchar(1000) # Name of the checkpoint file (e.g. 'checkpoint.p').
        checkpoint_file: filepath@moseq-train-processed # path to the checkpoint file.
        """

    class Plots(dj.Part):
        """
        Store the fitting progress of the PreFit computation:
        - Plots in PDF and PNG formats used for visualization.
        - Checkpoint file used for resuming the fitting process (~500MB).
        """

        definition = """
        -> master
        ---
        fitting_progress_plot_png: attach
        fitting_progress_plot_pdf: attach
        """

    def make(self, key):
        """
        Fit AR-HMM model for initial behavioral syllable discovery.

        Args:
            key (dict): Dictionary with the `PreFitTask` Key.

        Raises:
            FileNotFoundError: No PCA model found in project directory.

        High-Level Logic:
        1. Fetch project output directory and model parameters.
        2. Update configuration with latent dimension and kappa values.
        3. Load PCA model and format keypoint data.
        4. Initialize and fit AR-HMM model.
        5. Calculate fitting duration and insert results.
        """
        from keypoint_moseq import (
            fit_model,
            format_data,
            init_model,
            load_pca,
            update_hypparams,
        )

        kpms_project_output_dir = find_full_path(
            get_kpms_processed_data_dir(),
            (PCATask & key).fetch1("kpms_project_output_dir"),
        )

        pre_latent_dim, pre_kappa, pre_num_iterations, task_mode, model_name = (
            PreFitTask & key
        ).fetch1(
            "pre_latent_dim",
            "pre_kappa",
            "pre_num_iterations",
            "task_mode",
            "model_name",
        )
        if task_mode == "trigger":
            from keypoint_moseq import estimate_sigmasq_loc

            # Update the existing kpms_dj_config.yml with new latent_dim and kappa values
            kpms_reader.update_kpms_dj_config(
                kpms_project_output_dir,
                latent_dim=int(pre_latent_dim),
                kappa=float(pre_kappa),
            )

            # Load the updated config for use in model fitting
            kpms_dj_config = kpms_reader.load_kpms_dj_config(kpms_project_output_dir)

            # Load the PCA model from the project directory
            pca_path = kpms_project_output_dir / "pca.p"
            if pca_path.exists():
                pca = load_pca(kpms_project_output_dir.as_posix())
            else:
                raise FileNotFoundError(
                    f"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}"
                )

            # Format the data for model fitting
            coordinates, confidences = (PreProcessing & key).fetch1(
                "coordinates", "confidences"
            )
            data, metadata = format_data(
                coordinates=coordinates, confidences=confidences, **kpms_dj_config
            )

            # Update the kpms_dj_config.yml with the new sigmasq_loc
            kpms_reader.update_kpms_dj_config(
                kpms_project_output_dir,
                sigmasq_loc=estimate_sigmasq_loc(
                    data["Y"], data["mask"], filter_size=int(kpms_dj_config["fps"])
                ),
            )

            # Load the updated config for use in model fitting
            kpms_dj_config = kpms_reader.load_kpms_dj_config(
                project_dir=kpms_project_output_dir
            )

            # Initialize the model
            model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)

            # Update the model hyperparameters
            model = update_hypparams(
                model, kappa=float(pre_kappa), latent_dim=int(pre_latent_dim)
            )

            # Determine model directory name for outputs
            if model_name is None or not str(model_name).strip():
                model_dir_name = f"latent_dim_{float(pre_latent_dim)}_kappa_{float(pre_kappa)}_iters_{float(pre_num_iterations)}"
            else:
                model_dir_name = str(model_name)

            execution_time = datetime.now(timezone.utc)
            # Fit the model
            model, model_name = fit_model(
                model=model,
                model_name=model_dir_name,
                data=data,
                metadata=metadata,
                project_dir=kpms_project_output_dir.as_posix(),
                ar_only=True,
                num_iters=pre_num_iterations,
                generate_progress_plots=True,  # saved to {project_dir}/{model_name}/plots/
                save_every_n_iters=25,
            )

            # Normalize to folder name returned by fit_model
            model_dir_name = Path(model_name).name

            # Copy the PDF progress plot to PNG
            viz_utils.copy_pdf_to_png(kpms_project_output_dir, model_dir_name)

        else:
            # Load mode must specify a model_name
            if model_name is None or not str(model_name).strip():
                raise ValueError("model_name is required when task_mode='load'")
            model_dir_name = Path(model_name).name

        # Get the path to the updated config file
        updated_cfg_path = (kpms_project_output_dir / "kpms_dj_config.yml").as_posix()

        # Check for fitting progress files
        prefit_model_dir = kpms_project_output_dir / model_dir_name
        pdf_path = prefit_model_dir / "fitting_progress.pdf"
        png_path = prefit_model_dir / "fitting_progress.png"
        if not pdf_path.exists():
            raise FileNotFoundError(f"PreFit PDF progress plot not found at {pdf_path}")
        if not png_path.exists():
            raise FileNotFoundError(f"PreFit PNG progress plot not found at {png_path}")

        # Find checkpoint file
        checkpoint_files = []
        for pattern in ("checkpoint*", "*.h5"):
            checkpoint_files.extend(prefit_model_dir.glob(pattern))
        if checkpoint_files:
            checkpoint_file = max(checkpoint_files, key=lambda f: f.stat().st_size)
        else:
            raise FileNotFoundError(f"No checkpoint files found in {prefit_model_dir}")

        completion_time = datetime.now(timezone.utc)

        if task_mode == "trigger":
            duration_seconds = (completion_time - execution_time).total_seconds()
        else:
            duration_seconds = None

        self.insert1(
            {
                **key,
                "model_name": (
                    kpms_project_output_dir.relative_to(get_kpms_processed_data_dir())
                    / model_dir_name
                ).as_posix(),
                "pre_fit_duration": duration_seconds,
            }
        )

        self.ConfigFile.insert1(
            dict(
                **key,
                config_file=updated_cfg_path,
            )
        )

        self.Plots.insert1(
            {
                **key,
                "fitting_progress_plot_png": png_path,
                "fitting_progress_plot_pdf": pdf_path,
            }
        )

        self.CheckpointFile.insert1(
            {
                **key,
                "checkpoint_file_name": checkpoint_file.name,
                "checkpoint_file": checkpoint_file,
            }
        )


@schema
class FullFitTask(dj.Manual):
    """Define parameters for FullFit step of model fitting.

    Attributes:
        PCAFit (foreign key)                 : `PCAFit` Key.
        full_latent_dim (int)                : Latent dimension to use for the model full fitting.
        full_kappa (int)                     : Kappa value to use for the model full fitting (typically lower than pre-fit kappa).
        full_num_iterations (int)            : Number of Gibbs sampling iterations to run in the model full fitting (typically 200-500).
        model_name (varchar)                 : Name of the model to be loaded if `task_mode='load'`
        task_mode (enum)                     : 'load': load computed analysis results, 'trigger': trigger computation
        full_fit_desc(varchar)               : User-defined description of the model full fitting task.
    """

    definition = """
    -> PCAFit                                           # `PCAFit` Key
    full_latent_dim              : int                  # Latent dimension to use for the model full fitting
    full_kappa                   : int                  # Kappa value to use for the model full fitting (typically lower than pre-fit kappa).
    full_num_iterations          : int                  # Number of Gibbs sampling iterations to run in the model full fitting (typically 200-500).
    ---
    model_name=''                : varchar(1000)        # Name of the model to be loaded if `task_mode='load'`
    task_mode='load'             :enum('load','trigger')# Trigger or load the task
    full_fit_desc=''             : varchar(1000)        # User-defined description of the model full fitting task
    """


@schema
class FullFit(dj.Computed):
    """Fit the complete Keypoint Switching Linear Dynamical System (Keypoint-SLDS) model.

    Attributes:
        FullFitTask (foreign key)            : `FullFitTask` Key.
        model_name                           : varchar(100) # Name of the model as "kpms_project_output_dir/model_name"
        full_fit_duration (float)            : Time duration (seconds) of the full fitting computation
    """

    definition = """
    -> FullFitTask                                # `FullFitTask` Key
    ---
    model_name=''                 : varchar(1000) # Name of the model as "kpms_project_output_dir/model_name"
    full_fit_time=NULL            : datetime      # datetime of the full fitting computation.
    full_fit_duration=NULL        : float         # Time duration (seconds) of the full fitting computation
    """

    class ConfigFile(dj.Part):
        """
        Store the updated configuration file after FullFit computation.
        """

        definition = """
        -> master
        ---
        config_file: attach  # the updated config file after FullFit computation
        """

    class CheckpointFile(dj.Part):
        """
        Store the checkpoint file used for resuming the fitting process.
        """

        definition = """
        -> master
        ---
        checkpoint_file_name: varchar(1000) # Name of the checkpoint file (e.g. 'checkpoint.p').
        checkpoint_file: filepath@moseq-train-processed # path to the checkpoint file.
        """

    class Plots(dj.Part):
        """
        Store the fitting progress of the FullFit computation:
        - Plots in PDF and PNG formats used for visualization.
        """

        definition = """
        -> master
        ---
        fitting_progress_plot_png: attach
        fitting_progress_plot_pdf: attach
        """

    def make(self, key):
        """
        Fit the complete Keypoint-SLDS model with spatial and temporal dynamics.

        Args:
            key (dict): Dictionary with the `FullFitTask` Key.

        Raises:
            FileNotFoundError: No PCA model found in project directory.

        High-Level Logic:
        1. Fetch project output directory and model parameters.
        2. Update configuration with latent dimension and kappa values.
        3. Load PCA model and format keypoint data.
        4. Initialize and fit Keypoint-SLDS model.
        5. Reindex syllable labels by frequency.
        6. Calculate fitting duration and insert results.
        """
        from keypoint_moseq import (
            estimate_sigmasq_loc,
            fit_model,
            format_data,
            init_model,
            load_pca,
            reindex_syllables_in_checkpoint,
            update_hypparams,
        )

        kpms_project_output_dir = find_full_path(
            get_kpms_processed_data_dir(),
            (PCATask & key).fetch1("kpms_project_output_dir"),
        )
        full_latent_dim, full_kappa, full_num_iterations, task_mode, model_name = (
            FullFitTask & key
        ).fetch1(
            "full_latent_dim",
            "full_kappa",
            "full_num_iterations",
            "task_mode",
            "model_name",
        )

        if task_mode == "trigger":

            # Update the kpms_dj_config.yml with latent dimension and kappa values
            kpms_reader.update_kpms_dj_config(
                project_dir=kpms_project_output_dir,
                latent_dim=int(full_latent_dim),
                kappa=float(full_kappa),
            )

            # Load the updated config for data formatting
            kpms_dj_config = kpms_reader.load_kpms_dj_config(
                project_dir=kpms_project_output_dir
            )

            # Load the PCA model
            pca_path = kpms_project_output_dir / "pca.p"
            if pca_path.exists():
                pca = load_pca(kpms_project_output_dir.as_posix())
            else:
                raise FileNotFoundError(
                    f"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}"
                )

            # Format the data for model fitting
            coordinates, confidences = (PreProcessing & key).fetch1(
                "coordinates", "confidences"
            )
            data, metadata = format_data(
                coordinates=coordinates, confidences=confidences, **kpms_dj_config
            )

            # Update the kpms_dj_config.yml with the new sigmasq_loc
            kpms_reader.update_kpms_dj_config(
                project_dir=kpms_project_output_dir,
                sigmasq_loc=estimate_sigmasq_loc(
                    data["Y"], data["mask"], filter_size=int(kpms_dj_config["fps"])
                ),
            )

            # Load the updated config for use in model fitting
            kpms_dj_config = kpms_reader.load_kpms_dj_config(
                project_dir=kpms_project_output_dir
            )

            # Initialize the model
            model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)
            # Update the model hyperparameters
            model = update_hypparams(
                model, kappa=float(full_kappa), latent_dim=int(full_latent_dim)
            )
            # Generate the model directory name
            if model_name is None or not str(model_name).strip():
                model_dir_name = f"latent_dim_{float(full_latent_dim)}_kappa_{float(full_kappa)}_iters_{float(full_num_iterations)}"
            else:
                model_dir_name = str(model_name)

            execution_time = datetime.now(timezone.utc)

            # Fit the model
            model, model_name = fit_model(
                model=model,
                model_name=model_dir_name,
                data=data,
                metadata=metadata,
                project_dir=kpms_project_output_dir.as_posix(),
                ar_only=False,
                num_iters=full_num_iterations,
                generate_progress_plots=True,  # saved to {project_dir}/{model_name}/plots/
                save_every_n_iters=25,
            )

            # Reindex the syllables in the checkpoint file
            reindex_syllables_in_checkpoint(
                project_dir=kpms_project_output_dir.as_posix(),
                model_name=Path(model_name).name,
            )

        # Copy the PDF progress plot to PNG
        viz_utils.copy_pdf_to_png(kpms_project_output_dir, Path(model_name).name)

        # Get the path to the updated config file
        updated_cfg_path = kpms_reader._dj_config_path(kpms_project_output_dir)

        # Get the path to the full fit model directory
        fullfit_model_dir = kpms_project_output_dir / Path(model_name).name

        # Check for progress plot files
        pdf_path = fullfit_model_dir / "fitting_progress.pdf"
        png_path = fullfit_model_dir / "fitting_progress.png"
        if not pdf_path.exists():
            raise FileNotFoundError(
                f"FullFit PDF progress plot not found at {pdf_path}"
            )
        if not png_path.exists():
            raise FileNotFoundError(
                f"FullFit PNG progress plot not found at {png_path}"
            )

        # Find checkpoint file
        checkpoint_files = []
        for pattern in ("checkpoint*", "*.h5"):
            checkpoint_files.extend(fullfit_model_dir.glob(pattern))
        if checkpoint_files:
            checkpoint_file = max(checkpoint_files, key=lambda f: f.stat().st_size)
        else:
            raise FileNotFoundError(f"No checkpoint files found in {fullfit_model_dir}")

        completion_time = datetime.now(timezone.utc)

        if task_mode == "trigger":
            duration_seconds = (completion_time - execution_time).total_seconds()
        else:
            duration_seconds = None

        self.insert1(
            {
                **key,
                "model_name": (
                    kpms_project_output_dir.relative_to(get_kpms_processed_data_dir())
                    / Path(model_name).name
                ).as_posix(),
                "full_fit_time": completion_time,
                "full_fit_duration": duration_seconds,
            }
        )

        # Insert config file
        self.ConfigFile.insert1(
            {
                **key,
                "config_file": updated_cfg_path,
            }
        )

        # Insert plots
        self.Plots.insert1(
            {
                **key,
                "fitting_progress_plot_png": png_path,
                "fitting_progress_plot_pdf": pdf_path,
            }
        )

        # Insert checkpoint file
        self.CheckpointFile.insert1(
            {
                **key,
                "checkpoint_file_name": checkpoint_file.name,
                "checkpoint_file": checkpoint_file,
            }
        )


@schema
class SelectedFullFit(dj.Manual):
    """Register selected FullFit models for use in the inference pipeline.

    Attributes:
        FullFit (foreign key)          : `FullFit` Key.
        registered_model_name (varchar): User-friendly model name
        registered_model_desc (varchar): Optional user-defined description
    """

    definition = """
    -> FullFit
    ---
    registered_model_name         : varchar(1000)   # User-friendly model name
    registered_model_desc=''      : varchar(1000) # Optional user-defined description
    """
