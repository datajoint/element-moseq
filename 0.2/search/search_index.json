{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element MoSeq", "text": "<p>DataJoint Element for Motion Sequencing with  Keypoint-MoSeq,  from keypoint data extracted with DeepLabCut.  DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p>"}, {"location": "#experiment-flowchart", "title": "Experiment Flowchart", "text": ""}, {"location": "#data-pipeline-diagram", "title": "Data Pipeline Diagram", "text": ""}, {"location": "#getting-started", "title": "Getting Started", "text": "<ul> <li>Please fork the repository</li> </ul> <ul> <li> <p>Clone the repository to your computer</p> <pre><code>git clone https://github.com/&lt;enter_github_username&gt;/element-moseq\n</code></pre> </li> </ul> <ul> <li> <p>Install with <code>pip</code></p> <pre><code>pip install .\n</code></pre> </li> </ul> <ul> <li>Data Pipeline - Pipeline and table descriptions</li> </ul> <ul> <li>Tutorials - Start building your data pipeline</li> </ul> <ul> <li>Code Repository</li> </ul>"}, {"location": "#support", "title": "Support", "text": "<ul> <li>If you need help getting started or run into any errors, please contact our team by  email at support@datajoint.com.</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the following resources, please cite the respective manuscript and/or Research Resource Identifier (RRID):</p> <ul> <li> <p>DataJoint Element MoSeq - Version 0.2.1</p> <ul> <li>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,   Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for   Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>RRID:SCR_021894</li> </ul> </li> </ul> <ul> <li>Keypoint-MoSeq<ul> <li>Weinreb C, Pearl J, Lin S, Osman MAM, Zhang L, Annapragada S, Conlin E, Hoffman R,  Makowska S, Gillis WF and Jay M. Keypoint-MoSeq: parsing behavior by linking point  tracking to pose dynamics. BioRxiv. 2023 Dec 23. doi: https://doi.org/10.1101/2023.03.16.532307</li> <li>Wiltschko AB, Johnson MJ, Iurilli G, Peterson RE, Katon JM, Pashkovski SL, Abraira VE, Adams RP, Datta SR. Mapping sub-second structure in mouse behavior. Neuron. 2015 Dec 16;88(6):1121-35.</li> </ul> </li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#keypoint-moseq-advanced-motion-sequencing-through-pose-dynamics", "title": "Keypoint-MoSeq: Advanced Motion Sequencing through Pose Dynamics", "text": "<p>Keypoint-MoSeq[^1] introduces a novel machine learning platform tailored for identifying behavioral modules or \"syllables\" from keypoint data extracted from conventional video recordings of animal behavior. This innovative approach addresses the challenge posed by continuous keypoint data, prone to high-frequency jitter, often mistaken for transitions between behavioral states by conventional clustering algorithms. To overcome this hurdle, Keypoint-MoSeq leverages a generative model adept at discerning between keypoint noise and genuine behavior, facilitating precise identification of syllables marked by natural sub-second discontinuities inherent in mouse behavior.</p> <p>While keypoint tracking methods have significantly advanced the quantification of animal movement kinematics, the task of clustering behavioral data into discrete modules remains complex. Such clustering is vital for creating ethograms that delineate the sequential expression of behavioral modules. Existing methods vary in logic and assumptions, yielding diverse descriptions of identical behavior. Motion Sequencing (MoSeq)[^2] stands out as a validated technique for identifying behavioral modules and their temporal sequences using unsupervised machine learning. However, conventional MoSeq is tailored for depth camera data and faces challenges with high-frequency keypoint jitter.</p> <p>To address the limitations of traditional MoSeq when applied to keypoint data, Keypoint-MoSeq emerges as a promising solution. This new model enables simultaneous inference of keypoint positions and associated behavioral syllables, facilitating the identification of behavioral structure across diverse experimental settings without necessitating specialized hardware. Keypoint-MoSeq excels over alternative clustering methods in accurately delineating behavioral transitions, capturing neural activity correlations, and identifying complex features of solitary and social behavior. Its flexibility and accessibility, with freely available code for academic use^3, promise widespread adoption and further innovation in behavioral analysis methods.</p> <p>[^1]: Weinreb, C., Pearl, J., Lin, S., Osman, M. A. M., Zhang, L., Annapragada, S., Conlin, E., Hoffman, R., Makowska, S., Gillis, W. F., Jay, M., Ye, S., Mathis, A., Mathis, M. W., Pereira, T., Linderman, S. W., &amp; Datta, S. R. (2023). Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics. bioRxiv : the preprint server for biology, 2023.03.16.532307. https://doi.org/10.1101/2023.03.16.532307</p> <p>[^2]: Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... &amp; Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.</p>"}, {"location": "concepts/#element-features", "title": "Element Features", "text": "<p>Through our interviews and direct collaborations, we identified the core motifs to construct Element MoSeq.</p> <p>Key features include: - Ingestion and storage of input video metadata  - Loading and formatting of 2D deeplabcut keypoint tracking data for model training - Queue management and initiation of Keypoint-MoSeq analysis across multiple sessions - Ingestion of analysis outcomes such as PCA, AR-HMM, and Keypoint-SLDS components - Ingestion of analysis outcomes from motion sequencing inference</p>"}, {"location": "partnerships/", "title": "Key partnerships", "text": "<p>Element MoSeq was developed in collaboration with the Keypoint-MoSeq developers, particularly with Kai Fox from Datta's Lab at Harvard Medical School, to foster integration and interoperability between Keypoint-MoSeq and the DataJoint Element-MoSeq.</p>"}, {"location": "pipeline/", "title": "Data Pipeline", "text": "<p>Each node in the following diagram represents the analysis code in the pipeline and the corresponding table in the database.  Within the pipeline, Element MoSeq connects to upstream Elements including Lab, Animal, Session, and Event. For more  detailed documentation on each table, see the API docs for the respective schemas.</p> <p>The Element is composed of two main schemas, <code>moseq_train</code> and <code>moseq_infer</code>. The <code>moseq_train</code> schema is designed to handle the analysis and ingestion of PCA model for formatted keypoint tracking and train the Kepoint-MoSeq model. The <code>moseq_infer</code> schema is designed to handle the analysis and ingestion of Keypoint-MoSeq's motion sequencing on video recordings by using one registered model.</p>"}, {"location": "pipeline/#diagrams", "title": "Diagrams", "text": ""}, {"location": "pipeline/#moseq_train-module", "title": "<code>moseq_train</code> module", "text": "<ul> <li>The <code>moseq_train</code> schema is designed to handle the analysis and ingestion of PCA model for formatted keypoint tracking and train the Kepoint-MoSeq model.  </li> </ul>"}, {"location": "pipeline/#moseq_infer-module", "title": "<code>moseq_infer</code> module", "text": "<ul> <li>The <code>moseq_infer</code> schema is designed to handle the analysis and ingestion of Keypoint-MoSeq's motion sequencing on video recordings by using one registered model. </li> </ul>"}, {"location": "pipeline/#table-descriptions", "title": "Table Descriptions", "text": ""}, {"location": "pipeline/#lab-schema", "title": "<code>lab</code> schema", "text": "<ul> <li>For further details see the lab schema API docs</li> </ul> Table Description Device Scanner metadata"}, {"location": "pipeline/#subject-schema", "title": "<code>subject</code> schema", "text": "<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</li> </ul> <ul> <li>For further details see the subject schema API docs</li> </ul> Table Description Subject Basic information of the research subject"}, {"location": "pipeline/#session-schema", "title": "<code>session</code> schema", "text": "<ul> <li>For further details see the session schema API docs</li> </ul> Table Description Session Unique experimental session identifier"}, {"location": "pipeline/#moseq_train-schema", "title": "<code>moseq_train</code> schema", "text": "<ul> <li>For further details see the <code>moseq_train</code> schema API docs</li> </ul> Table Description KeypointSet Store keypoint data and video set directory for model training. KeypointSet.VideoFile IDs and file paths of each video file that will be used for model training. Bodyparts Store the body parts to use in the analysis. PCATask Staging table to define the PCA task and its output directory. PCAPrep Setup the Keypoint-MoSeq project output directory (<code>kpms_project_output_dir</code>) creating the default <code>config.yml</code> and updating it in a new <code>dj_config.yml</code>. PCAFit Fit PCA model. LatentDimension Calculate the latent dimension as one of the autoregressive hyperparameters (<code>ar_hypparams</code>) necessary for the model fitting. PreFitTask Specify parameters for model (AR-HMM) pre-fitting. PreFit Fit AR-HMM model. FullFitTask Specify parameters for the model full-fitting. FullFit Fit the full (Keypoint-SLDS) model."}, {"location": "pipeline/#moseq_infer-schema", "title": "<code>moseq_infer</code> schema", "text": "<ul> <li>For further details see the <code>moseq_infer</code> schema API docs</li> </ul> Table Description Model Register a model. VideoRecording Set of video recordings for the Keypoint-MoSeq inference. VideoRecording.File File IDs and paths associated with a given <code>recording_id</code>. PoseEstimationMethod Pose estimation methods supported by the keypoint loader of <code>keypoint-moseq</code> package. InferenceTask Staging table to define the Inference task and its output directory. Inference Infer the model from the checkpoint file and save the results as <code>results.h5</code> file. Inference.MotionSequence Results of the model inference. Inference.GridMoviesSampledInstances Store the sampled instances of the grid movies."}, {"location": "roadmap/", "title": "Roadmap", "text": "<p>Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will continue adding features to this Element.</p>"}, {"location": "api/element_moseq/moseq_infer/", "title": "moseq_infer.py", "text": ""}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.activate", "title": "<code>activate(infer_schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>infer_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>moseq_infer</code> schema.</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it                 does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create schema tables in the database                  if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module (or name) containing the required dependencies.</p> <code>None</code> <p>Functions:</p> Name Description <code>get_kpms_root_data_dir</code> <p>Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s)</p> <code>get_kpms_processed_data_dir</code> <p>Optional. Returns absolute path for processed data.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>def activate(\n    infer_schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n    \"\"\"Activate this schema.\n\n    Args:\n        infer_schema_name (str): Schema name on the database server to activate the `moseq_infer` schema.\n        create_schema (bool): When True (default), create schema in the database if it\n                            does not yet exist.\n        create_tables (bool): When True (default), create schema tables in the database\n                             if they do not yet exist.\n        linking_module (str): A module (or name) containing the required dependencies.\n\n    Functions:\n        get_kpms_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s)\n        get_kpms_processed_data_dir(): Optional. Returns absolute path for processed data.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n    assert hasattr(\n        linking_module, \"get_kpms_root_data_dir\"\n    ), \"The linking module must specify a lookup function for a root data directory\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    schema.activate(\n        infer_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.get_kpms_root_data_dir", "title": "<code>get_kpms_root_data_dir()</code>", "text": "<p>Pulls relevant func from parent namespace to specify root data dir(s).</p> <p>It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strings for possible root data directories.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>def get_kpms_root_data_dir() -&gt; list:\n    \"\"\"Pulls relevant func from parent namespace to specify root data dir(s).\n\n    It is recommended that all paths in DataJoint Elements stored as relative\n    paths, with respect to some user-configured \"root\" director(y/ies). The\n    root(s) may vary between data modalities and user machines. Returns a full path\n    string or list of strings for possible root data directories.\n    \"\"\"\n    root_directories = _linking_module.get_kpms_root_data_dir()\n    if isinstance(root_directories, (str, Path)):\n        root_directories = [root_directories]\n\n    if (\n        hasattr(_linking_module, \"get_kpms_processed_data_dir\")\n        and get_kpms_processed_data_dir() not in root_directories\n    ):\n        root_directories.append(_linking_module.get_kpms_processed_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.get_kpms_processed_data_dir", "title": "<code>get_kpms_processed_data_dir()</code>", "text": "<p>Pulls relevant func from parent namespace. Defaults to KPMS's project /videos/.</p> <p>Method in parent namespace should provide a string to a directory where KPMS output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per Keypoint-MoSeq default.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>def get_kpms_processed_data_dir() -&gt; Optional[str]:\n    \"\"\"Pulls relevant func from parent namespace. Defaults to KPMS's project /videos/.\n\n    Method in parent namespace should provide a string to a directory where KPMS output\n    files will be stored. If unspecified, output files will be stored in the\n    session directory 'videos' folder, per Keypoint-MoSeq default.\n    \"\"\"\n    if hasattr(_linking_module, \"get_kpms_processed_data_dir\"):\n        return _linking_module.get_kpms_processed_data_dir()\n    else:\n        return None\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.Model", "title": "<code>Model</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Register a model.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>int)                      </code> <p>Unique ID for each model.</p> <code>model_name</code> <code>varchar)                </code> <p>User-friendly model name.</p> <code>model_dir</code> <code>varchar)                 </code> <p>Model directory relative to root data directory (e.g. <code>kpms_project_output_dir/2024_03_21-00_51_39</code>)</p> <code>latent_dim</code> <code>int)                    </code> <p>Latent dimension of the model.</p> <code>kappa</code> <code>float)                       </code> <p>Kappa value of the model.</p> <code>model_desc</code> <code>varchar)                </code> <p>Optional. User-defined description of the model</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>@schema\nclass Model(dj.Manual):\n    \"\"\"Register a model.\n\n    Attributes:\n        model_id (int)                      : Unique ID for each model.\n        model_name (varchar)                : User-friendly model name.\n        model_dir (varchar)                 : Model directory relative to root data directory (e.g. `kpms_project_output_dir/2024_03_21-00_51_39`)\n        latent_dim (int)                    : Latent dimension of the model.\n        kappa (float)                       : Kappa value of the model.\n        model_desc (varchar)                : Optional. User-defined description of the model\n\n    \"\"\"\n\n    definition = \"\"\"\n    model_id                : int          # Unique ID for each model\n    ---\n    model_name              : varchar(64)  # User-friendly model name\n    model_dir               : varchar(1000)# Model directory relative to root data directory\n    latent_dim              : int          # Latent dimension of the model\n    kappa                   : float        # Kappa value of the model\n    model_desc=''           : varchar(1000)# Optional. User-defined description of the model\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.VideoRecording", "title": "<code>VideoRecording</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Set of video recordings for the Keypoint-MoSeq inference.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key)               </code> <p><code>Session</code> key.</p> <code>recording_id</code> <code>int)                  </code> <p>Unique ID for each recording.</p> <code>Device</code> <code>foreign key)                </code> <p>Device primary key.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>@schema\nclass VideoRecording(dj.Manual):\n    \"\"\"Set of video recordings for the Keypoint-MoSeq inference.\n\n    Attributes:\n        Session (foreign key)               : `Session` key.\n        recording_id (int)                  : Unique ID for each recording.\n        Device (foreign key)                : Device primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Session                             # `Session` key\n    recording_id: int                      # Unique ID for each recording\n    ---\n    -&gt; Device                              # Device primary key\n    \"\"\"\n\n    class File(dj.Part):\n        \"\"\"File IDs and paths associated with a given `recording_id`.\n\n        Attributes:\n            VideoRecording (foreign key)   : `VideoRecording` key.\n            file_id(int)                   : Unique ID for each file.\n            file_path (varchar)            : Filepath of each video, relative to root data directory.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master               \n        file_id: int             # Unique ID for each file\n        ---\n        file_path: varchar(1000) # Filepath of each video, relative to root data directory.\n        \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.VideoRecording.File", "title": "<code>File</code>", "text": "<p>               Bases: <code>Part</code></p> <p>File IDs and paths associated with a given <code>recording_id</code>.</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key)   </code> <p><code>VideoRecording</code> key.</p> <code>file_id(int)</code> <code> </code> <p>Unique ID for each file.</p> <code>file_path</code> <code>varchar)            </code> <p>Filepath of each video, relative to root data directory.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>class File(dj.Part):\n    \"\"\"File IDs and paths associated with a given `recording_id`.\n\n    Attributes:\n        VideoRecording (foreign key)   : `VideoRecording` key.\n        file_id(int)                   : Unique ID for each file.\n        file_path (varchar)            : Filepath of each video, relative to root data directory.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master               \n    file_id: int             # Unique ID for each file\n    ---\n    file_path: varchar(1000) # Filepath of each video, relative to root data directory.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.PoseEstimationMethod", "title": "<code>PoseEstimationMethod</code>", "text": "<p>               Bases: <code>Lookup</code></p> <p>Pose estimation methods supported by the keypoint loader of <code>keypoint-moseq</code> package.</p> <p>Attributes:</p> Name Type Description <code>pose_estimation_method</code> <code> (str</code> <p>Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)</p> <code>pose_estimation_desc</code> <code>   (str</code> <p>Optional. Pose estimation method description with the supported formats.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>@schema\nclass PoseEstimationMethod(dj.Lookup):\n    \"\"\"Pose estimation methods supported by the keypoint loader of `keypoint-moseq` package.\n\n    Attributes:\n        pose_estimation_method  (str): Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)\n        pose_estimation_desc    (str): Optional. Pose estimation method description with the supported formats.\n    \"\"\"\n\n    definition = \"\"\" \n    # Pose estimation methods supported by the keypoint loader of `keypoint-moseq` package. \n    pose_estimation_method  : char(15)         # Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)\n    ---\n    pose_estimation_desc    : varchar(1000)    # Optional. Pose estimation method description with the supported formats.\n    \"\"\"\n\n    contents = [\n        [\"deeplabcut\", \"`.csv` and `.h5/.hdf5` files generated by DeepLabcut analysis\"],\n        [\"sleap\", \"`.slp` and `.h5/.hdf5` files generated by SLEAP analysis\"],\n        [\"anipose\", \"`.csv` files generated by anipose analysis\"],\n        [\"sleap-anipose\", \"`.h5/.hdf5` files generated by sleap-anipose analysis\"],\n        [\"nwb\", \"`.nwb` files with Neurodata Without Borders (NWB) format\"],\n        [\"facemap\", \"`.h5` files generated by Facemap analysis\"],\n    ]\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.InferenceTask", "title": "<code>InferenceTask</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Staging table to define the Inference task and its output directory.</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key)         </code> <p><code>VideoRecording</code> key</p> <code>Model</code> <code>foreign key)                  </code> <p><code>Model</code> key</p> <code>PoseEstimationMethod</code> <code>foreign key)   </code> <p>Pose estimation method used for the specified <code>recording_id</code>.</p> <code>inference_output_dir</code> <code>varchar)       </code> <p>Optional. Sub-directory where the results will be stored.</p> <code>inference_desc</code> <code>varchar)             </code> <p>Optional. User-defined description of the inference task.</p> <code>num_iterations</code> <code>int)                 </code> <p>Optional. Number of iterations to use for the model inference. If null, the default number internally is 50.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>@schema\nclass InferenceTask(dj.Manual):\n    \"\"\"Staging table to define the Inference task and its output directory.\n\n    Attributes:\n        VideoRecording (foreign key)         : `VideoRecording` key\n        Model (foreign key)                  : `Model` key\n        PoseEstimationMethod (foreign key)   : Pose estimation method used for the specified `recording_id`.\n        inference_output_dir (varchar)       : Optional. Sub-directory where the results will be stored.\n        inference_desc (varchar)             : Optional. User-defined description of the inference task.\n        num_iterations (int)                 : Optional. Number of iterations to use for the model inference. If null, the default number internally is 50.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; VideoRecording                                       # `VideoRecording` key\n    -&gt; Model                                                # `Model` key \n    ---\n    -&gt; PoseEstimationMethod                                 # Pose estimation method used for the specified `recording_id`\n    keypointset_dir               : varchar(1000)           # Keypointset directory for the specified VideoRecording\n    inference_output_dir=''       : varchar(1000)           # Optional. Sub-directory where the results will be stored\n    inference_desc=''             : varchar(1000)           # Optional. User-defined description of the inference task\n    num_iterations=NULL           : int                     # Optional. Number of iterations to use for the model inference. If null, the default number internally is 50.\n    task_mode='load'              : enum('load', 'trigger') # Task mode for the inference task\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.Inference", "title": "<code>Inference</code>", "text": "<p>               Bases: <code>Computed</code></p> <p>Infer the model from the checkpoint file and save the results as <code>results.h5</code> file.</p> <p>Attributes:</p> Name Type Description <code>InferenceTask</code> <code>foreign_key)         </code> <p><code>InferenceTask</code> key.</p> <code>inference_duration</code> <code>float)          </code> <p>Time duration (seconds) of the inference computation.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>@schema\nclass Inference(dj.Computed):\n    \"\"\"Infer the model from the checkpoint file and save the results as `results.h5` file.\n\n    Attributes:\n        InferenceTask (foreign_key)         : `InferenceTask` key.\n        inference_duration (float)          : Time duration (seconds) of the inference computation.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; InferenceTask                        # `InferenceTask` key\n    --- \n    inference_duration=NULL        : float  # Time duration (seconds) of the inference computation\n    \"\"\"\n\n    class MotionSequence(dj.Part):\n        \"\"\"Store the results of the model inference.\n\n        Attributes:\n            video_name (varchar)                : Name of the video.\n            syllable (longblob)                 : Syllable labels (z). The syllable label assigned to each frame (i.e. the state indexes assigned by the model).\n            latent_state (longblob)             : Inferred low-dim pose state (x). Low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, are modified to reflect the pose dynamics and noise estimates inferred by the model.\n            centroid (longblob)                 : Inferred centroid (v). The centroid of the animal in each frame, as estimated by the model.\n            heading (longblob)                  : Inferred heading (h). The heading of the animal in each frame, as estimated by the model.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        video_name      : varchar(150)    # Name of the video\n        ---\n        syllable        : longblob        # Syllable labels (z). The syllable label assigned to each frame (i.e. the state indexes assigned by the model)\n        latent_state    : longblob        # Inferred low-dim pose state (x). Low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, are modified to reflect the pose dynamics and noise estimates inferred by the model\n        centroid        : longblob        # Inferred centroid (v). The centroid of the animal in each frame, as estimated by the model\n        heading         : longblob        # Inferred heading (h). The heading of the animal in each frame, as estimated by the model\n        \"\"\"\n\n    class GridMoviesSampledInstances(dj.Part):\n        \"\"\"Store the sampled instances of the grid movies.\n\n        Attributes:\n            syllable (int)                  : Syllable label.\n            instances (longblob)            : List of instances shown in each in grid movie (in row-major order), where each instance is specified as a tuple with the video name, start frame and end frame.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        syllable: int           # Syllable label\n        ---\n        instances: longblob     # List of instances shown in each in grid movie (in row-major order), where each instance is specified as a tuple with the video name, start frame and end frame\n        \"\"\"\n\n    def make(self, key):\n        \"\"\"\n        This function is used to infer the model results from the checkpoint file and store the results in `MotionSequence` and `GridMoviesSampledInstances` tables.\n\n        Args:\n            key (dict): `InferenceTask` primary key.\n\n        Raises:\n            FileNotFoundError: If no pca model (`pca.p`) found in the parent model directory.\n            FileNotFoundError: If no model (`checkpoint.h5`) found in the model directory.\n            NotImplementedError: If the format method is not `deeplabcut`.\n            FileNotFoundError: If no valid `kpms_dj_config` found in the parent model directory.\n\n        High-level Logic:\n        1. Fetch the `inference_output_dir` where the results will be stored, and if it does not exist, create it.\n        2. Fetch the `model_name` and the `num_iterations` from the `InferenceTask` table.\n        3. Load the most recent model checkpoint and the pca model from files in the `kpms_project_output_dir`.\n        4. Load the keypoint data for inference as `filepath_patterns` and format it.\n        5. Initialize and apply the model with the new keypoint data.\n        6. If the `num_iterations` is set, fit the model with the new keypoint data for `num_iterations` iterations; otherwise, fit the model with the default number of iterations (50).\n        7. Save the results as a CSV file and store the histogram showing the frequency of each syllable.\n        8. Generate and save the plots showing the median trajectory of poses associated with each given syllable.\n        9. Generate and save video clips showing examples of each syllable.\n        10. Generate and save the dendrogram representing distances between each syllable's median trajectory.\n        11. Insert the inference duration in the `Inference` table.\n        12. Insert the results in the `MotionSequence` and `GridMoviesSampledInstances` tables.\n        \"\"\"\n        from keypoint_moseq import (\n            load_checkpoint,\n            load_pca,\n            load_keypoints,\n            format_data,\n            apply_model,\n            save_results_as_csv,\n            plot_syllable_frequencies,\n            generate_trajectory_plots,\n            generate_grid_movies,\n            plot_similarity_dendrogram,\n        )\n\n        (\n            keypointset_dir,\n            inference_output_dir,\n            num_iterations,\n            model_id,\n            pose_estimation_method,\n            task_mode,\n        ) = (InferenceTask &amp; key).fetch1(\n            \"keypointset_dir\",\n            \"inference_output_dir\",\n            \"num_iterations\",\n            \"model_id\",\n            \"pose_estimation_method\",\n            \"task_mode\",\n        )\n\n        kpms_root = get_kpms_root_data_dir()\n        kpms_processed = get_kpms_processed_data_dir()\n\n        model_dir = find_full_path(\n            kpms_processed,\n            (Model &amp; f\"model_id = {model_id}\").fetch1(\"model_dir\"),\n        )\n        keypointset_dir = find_full_path(kpms_root, keypointset_dir)\n\n        inference_output_dir = os.path.join(model_dir, inference_output_dir)\n\n        if not os.path.exists(inference_output_dir):\n            os.makedirs(model_dir / inference_output_dir)\n\n        pca_path = model_dir.parent / \"pca.p\"\n        if pca_path:\n            pca = load_pca(model_dir.parent.as_posix())\n        else:\n            raise FileNotFoundError(\n                f\"No pca model (`pca.p`) found in the parent model directory {model_dir.parent}\"\n            )\n\n        model_path = model_dir / \"checkpoint.h5\"\n        if model_path:\n            model = load_checkpoint(\n                project_dir=model_dir.parent, model_name=model_dir.parts[-1]\n            )[0]\n        else:\n            raise FileNotFoundError(\n                f\"No model (`checkpoint.h5`) found in the model directory {model_dir}\"\n            )\n\n        if pose_estimation_method == \"deeplabcut\":\n            coordinates, confidences, _ = load_keypoints(\n                filepath_pattern=keypointset_dir, format=pose_estimation_method\n            )\n        else:\n            raise NotImplementedError(\n                \"The currently supported format method is `deeplabcut`. If you require \\\n        support for another format method, please reach out to us at `support@datajoint.com`.\"\n            )\n\n        kpms_dj_config = load_kpms_dj_config(\n            model_dir.parent.as_posix(), check_if_valid=True, build_indexes=True\n        )\n\n        if kpms_dj_config:\n            data, metadata = format_data(coordinates, confidences, **kpms_dj_config)\n        else:\n            raise FileNotFoundError(\n                f\"No valid `kpms_dj_config` found in the parent model directory {model_dir.parent}\"\n            )\n\n        if task_mode == \"trigger\":\n            start_time = datetime.utcnow()\n            results = apply_model(\n                model=model,\n                data=data,\n                metadata=metadata,\n                pca=pca,\n                project_dir=model_dir.parent.as_posix(),\n                model_name=Path(model_dir).name,\n                results_path=(inference_output_dir / \"results.h5\").as_posix(),\n                return_model=False,\n                num_iters=num_iterations\n                or 50,  # default internal value in the keypoint-moseq function\n                **kpms_dj_config,\n            )\n            end_time = datetime.utcnow()\n\n            duration_seconds = (end_time - start_time).total_seconds()\n\n            save_results_as_csv(\n                results=results,\n                save_dir=(inference_output_dir / \"results_as_csv\").as_posix(),\n            )\n\n            fig, _ = plot_syllable_frequencies(\n                results=results, path=inference_output_dir.as_posix()\n            )\n            fig.savefig(inference_output_dir / \"syllable_frequencies.png\")\n            plt.close(fig)\n\n            generate_trajectory_plots(\n                coordinates=coordinates,\n                results=results,\n                output_dir=(inference_output_dir / \"trajectory_plots\").as_posix(),\n                **kpms_dj_config,\n            )\n\n            sampled_instances = generate_grid_movies(\n                coordinates=coordinates,\n                results=results,\n                output_dir=(inference_output_dir / \"grid_movies\").as_posix(),\n                **kpms_dj_config,\n            )\n\n            plot_similarity_dendrogram(\n                coordinates=coordinates,\n                results=results,\n                save_path=(inference_output_dir / \"similarity_dendogram\").as_posix(),\n                **kpms_dj_config,\n            )\n\n        else:\n            from keypoint_moseq import (\n                load_results,\n                filter_centroids_headings,\n                get_syllable_instances,\n                sample_instances,\n            )\n\n            # load results\n            results = load_results(\n                project_dir=Path(inference_output_dir).parent,\n                model_name=Path(inference_output_dir).parts[-1],\n            )\n\n            # extract sampled_instances\n            ## extract syllables from results\n            syllables = {k: v[\"syllable\"] for k, v in results.items()}\n\n            ## extract and smooth centroids and headings\n            centroids = {k: v[\"centroid\"] for k, v in results.items()}\n            headings = {k: v[\"heading\"] for k, v in results.items()}\n\n            filter_size = 9  # default value\n            centroids, headings = filter_centroids_headings(\n                centroids, headings, filter_size=filter_size\n            )\n\n            # sample instances for each syllable\n            syllable_instances = get_syllable_instances(\n                syllables, min_duration=3, min_frequency=0.005\n            )\n\n            sampled_instances = sample_instances(\n                syllable_instances=syllable_instances,\n                num_samples=4 * 6,  # minimum rows * cols\n                coordinates=coordinates,\n                centroids=centroids,\n                headings=headings,\n            )\n\n            duration_seconds = None\n\n        self.insert1({**key, \"inference_duration\": duration_seconds})\n\n        for result_idx, result in results.items():\n            self.MotionSequence.insert1(\n                {\n                    **key,\n                    \"video_name\": result_idx,\n                    \"syllable\": result[\"syllable\"],\n                    \"latent_state\": result[\"latent_state\"],\n                    \"centroid\": result[\"centroid\"],\n                    \"heading\": result[\"heading\"],\n                }\n            )\n\n        for syllable, sampled_instance in sampled_instances.items():\n            self.GridMoviesSampledInstances.insert1(\n                {**key, \"syllable\": syllable, \"instances\": sampled_instance}\n            )\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.Inference.MotionSequence", "title": "<code>MotionSequence</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Store the results of the model inference.</p> <p>Attributes:</p> Name Type Description <code>video_name</code> <code>varchar)                </code> <p>Name of the video.</p> <code>syllable</code> <code>longblob)                 </code> <p>Syllable labels (z). The syllable label assigned to each frame (i.e. the state indexes assigned by the model).</p> <code>latent_state</code> <code>longblob)             </code> <p>Inferred low-dim pose state (x). Low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, are modified to reflect the pose dynamics and noise estimates inferred by the model.</p> <code>centroid</code> <code>longblob)                 </code> <p>Inferred centroid (v). The centroid of the animal in each frame, as estimated by the model.</p> <code>heading</code> <code>longblob)                  </code> <p>Inferred heading (h). The heading of the animal in each frame, as estimated by the model.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>class MotionSequence(dj.Part):\n    \"\"\"Store the results of the model inference.\n\n    Attributes:\n        video_name (varchar)                : Name of the video.\n        syllable (longblob)                 : Syllable labels (z). The syllable label assigned to each frame (i.e. the state indexes assigned by the model).\n        latent_state (longblob)             : Inferred low-dim pose state (x). Low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, are modified to reflect the pose dynamics and noise estimates inferred by the model.\n        centroid (longblob)                 : Inferred centroid (v). The centroid of the animal in each frame, as estimated by the model.\n        heading (longblob)                  : Inferred heading (h). The heading of the animal in each frame, as estimated by the model.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    video_name      : varchar(150)    # Name of the video\n    ---\n    syllable        : longblob        # Syllable labels (z). The syllable label assigned to each frame (i.e. the state indexes assigned by the model)\n    latent_state    : longblob        # Inferred low-dim pose state (x). Low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, are modified to reflect the pose dynamics and noise estimates inferred by the model\n    centroid        : longblob        # Inferred centroid (v). The centroid of the animal in each frame, as estimated by the model\n    heading         : longblob        # Inferred heading (h). The heading of the animal in each frame, as estimated by the model\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.Inference.GridMoviesSampledInstances", "title": "<code>GridMoviesSampledInstances</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Store the sampled instances of the grid movies.</p> <p>Attributes:</p> Name Type Description <code>syllable</code> <code>int)                  </code> <p>Syllable label.</p> <code>instances</code> <code>longblob)            </code> <p>List of instances shown in each in grid movie (in row-major order), where each instance is specified as a tuple with the video name, start frame and end frame.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>class GridMoviesSampledInstances(dj.Part):\n    \"\"\"Store the sampled instances of the grid movies.\n\n    Attributes:\n        syllable (int)                  : Syllable label.\n        instances (longblob)            : List of instances shown in each in grid movie (in row-major order), where each instance is specified as a tuple with the video name, start frame and end frame.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    syllable: int           # Syllable label\n    ---\n    instances: longblob     # List of instances shown in each in grid movie (in row-major order), where each instance is specified as a tuple with the video name, start frame and end frame\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_infer/#element_moseq.moseq_infer.Inference.make", "title": "<code>make(key)</code>", "text": "<p>This function is used to infer the model results from the checkpoint file and store the results in <code>MotionSequence</code> and <code>GridMoviesSampledInstances</code> tables.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p><code>InferenceTask</code> primary key.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no pca model (<code>pca.p</code>) found in the parent model directory.</p> <code>FileNotFoundError</code> <p>If no model (<code>checkpoint.h5</code>) found in the model directory.</p> <code>NotImplementedError</code> <p>If the format method is not <code>deeplabcut</code>.</p> <code>FileNotFoundError</code> <p>If no valid <code>kpms_dj_config</code> found in the parent model directory.</p> <p>High-level Logic: 1. Fetch the <code>inference_output_dir</code> where the results will be stored, and if it does not exist, create it. 2. Fetch the <code>model_name</code> and the <code>num_iterations</code> from the <code>InferenceTask</code> table. 3. Load the most recent model checkpoint and the pca model from files in the <code>kpms_project_output_dir</code>. 4. Load the keypoint data for inference as <code>filepath_patterns</code> and format it. 5. Initialize and apply the model with the new keypoint data. 6. If the <code>num_iterations</code> is set, fit the model with the new keypoint data for <code>num_iterations</code> iterations; otherwise, fit the model with the default number of iterations (50). 7. Save the results as a CSV file and store the histogram showing the frequency of each syllable. 8. Generate and save the plots showing the median trajectory of poses associated with each given syllable. 9. Generate and save video clips showing examples of each syllable. 10. Generate and save the dendrogram representing distances between each syllable's median trajectory. 11. Insert the inference duration in the <code>Inference</code> table. 12. Insert the results in the <code>MotionSequence</code> and <code>GridMoviesSampledInstances</code> tables.</p> Source code in <code>element_moseq/moseq_infer.py</code> <pre><code>def make(self, key):\n    \"\"\"\n    This function is used to infer the model results from the checkpoint file and store the results in `MotionSequence` and `GridMoviesSampledInstances` tables.\n\n    Args:\n        key (dict): `InferenceTask` primary key.\n\n    Raises:\n        FileNotFoundError: If no pca model (`pca.p`) found in the parent model directory.\n        FileNotFoundError: If no model (`checkpoint.h5`) found in the model directory.\n        NotImplementedError: If the format method is not `deeplabcut`.\n        FileNotFoundError: If no valid `kpms_dj_config` found in the parent model directory.\n\n    High-level Logic:\n    1. Fetch the `inference_output_dir` where the results will be stored, and if it does not exist, create it.\n    2. Fetch the `model_name` and the `num_iterations` from the `InferenceTask` table.\n    3. Load the most recent model checkpoint and the pca model from files in the `kpms_project_output_dir`.\n    4. Load the keypoint data for inference as `filepath_patterns` and format it.\n    5. Initialize and apply the model with the new keypoint data.\n    6. If the `num_iterations` is set, fit the model with the new keypoint data for `num_iterations` iterations; otherwise, fit the model with the default number of iterations (50).\n    7. Save the results as a CSV file and store the histogram showing the frequency of each syllable.\n    8. Generate and save the plots showing the median trajectory of poses associated with each given syllable.\n    9. Generate and save video clips showing examples of each syllable.\n    10. Generate and save the dendrogram representing distances between each syllable's median trajectory.\n    11. Insert the inference duration in the `Inference` table.\n    12. Insert the results in the `MotionSequence` and `GridMoviesSampledInstances` tables.\n    \"\"\"\n    from keypoint_moseq import (\n        load_checkpoint,\n        load_pca,\n        load_keypoints,\n        format_data,\n        apply_model,\n        save_results_as_csv,\n        plot_syllable_frequencies,\n        generate_trajectory_plots,\n        generate_grid_movies,\n        plot_similarity_dendrogram,\n    )\n\n    (\n        keypointset_dir,\n        inference_output_dir,\n        num_iterations,\n        model_id,\n        pose_estimation_method,\n        task_mode,\n    ) = (InferenceTask &amp; key).fetch1(\n        \"keypointset_dir\",\n        \"inference_output_dir\",\n        \"num_iterations\",\n        \"model_id\",\n        \"pose_estimation_method\",\n        \"task_mode\",\n    )\n\n    kpms_root = get_kpms_root_data_dir()\n    kpms_processed = get_kpms_processed_data_dir()\n\n    model_dir = find_full_path(\n        kpms_processed,\n        (Model &amp; f\"model_id = {model_id}\").fetch1(\"model_dir\"),\n    )\n    keypointset_dir = find_full_path(kpms_root, keypointset_dir)\n\n    inference_output_dir = os.path.join(model_dir, inference_output_dir)\n\n    if not os.path.exists(inference_output_dir):\n        os.makedirs(model_dir / inference_output_dir)\n\n    pca_path = model_dir.parent / \"pca.p\"\n    if pca_path:\n        pca = load_pca(model_dir.parent.as_posix())\n    else:\n        raise FileNotFoundError(\n            f\"No pca model (`pca.p`) found in the parent model directory {model_dir.parent}\"\n        )\n\n    model_path = model_dir / \"checkpoint.h5\"\n    if model_path:\n        model = load_checkpoint(\n            project_dir=model_dir.parent, model_name=model_dir.parts[-1]\n        )[0]\n    else:\n        raise FileNotFoundError(\n            f\"No model (`checkpoint.h5`) found in the model directory {model_dir}\"\n        )\n\n    if pose_estimation_method == \"deeplabcut\":\n        coordinates, confidences, _ = load_keypoints(\n            filepath_pattern=keypointset_dir, format=pose_estimation_method\n        )\n    else:\n        raise NotImplementedError(\n            \"The currently supported format method is `deeplabcut`. If you require \\\n    support for another format method, please reach out to us at `support@datajoint.com`.\"\n        )\n\n    kpms_dj_config = load_kpms_dj_config(\n        model_dir.parent.as_posix(), check_if_valid=True, build_indexes=True\n    )\n\n    if kpms_dj_config:\n        data, metadata = format_data(coordinates, confidences, **kpms_dj_config)\n    else:\n        raise FileNotFoundError(\n            f\"No valid `kpms_dj_config` found in the parent model directory {model_dir.parent}\"\n        )\n\n    if task_mode == \"trigger\":\n        start_time = datetime.utcnow()\n        results = apply_model(\n            model=model,\n            data=data,\n            metadata=metadata,\n            pca=pca,\n            project_dir=model_dir.parent.as_posix(),\n            model_name=Path(model_dir).name,\n            results_path=(inference_output_dir / \"results.h5\").as_posix(),\n            return_model=False,\n            num_iters=num_iterations\n            or 50,  # default internal value in the keypoint-moseq function\n            **kpms_dj_config,\n        )\n        end_time = datetime.utcnow()\n\n        duration_seconds = (end_time - start_time).total_seconds()\n\n        save_results_as_csv(\n            results=results,\n            save_dir=(inference_output_dir / \"results_as_csv\").as_posix(),\n        )\n\n        fig, _ = plot_syllable_frequencies(\n            results=results, path=inference_output_dir.as_posix()\n        )\n        fig.savefig(inference_output_dir / \"syllable_frequencies.png\")\n        plt.close(fig)\n\n        generate_trajectory_plots(\n            coordinates=coordinates,\n            results=results,\n            output_dir=(inference_output_dir / \"trajectory_plots\").as_posix(),\n            **kpms_dj_config,\n        )\n\n        sampled_instances = generate_grid_movies(\n            coordinates=coordinates,\n            results=results,\n            output_dir=(inference_output_dir / \"grid_movies\").as_posix(),\n            **kpms_dj_config,\n        )\n\n        plot_similarity_dendrogram(\n            coordinates=coordinates,\n            results=results,\n            save_path=(inference_output_dir / \"similarity_dendogram\").as_posix(),\n            **kpms_dj_config,\n        )\n\n    else:\n        from keypoint_moseq import (\n            load_results,\n            filter_centroids_headings,\n            get_syllable_instances,\n            sample_instances,\n        )\n\n        # load results\n        results = load_results(\n            project_dir=Path(inference_output_dir).parent,\n            model_name=Path(inference_output_dir).parts[-1],\n        )\n\n        # extract sampled_instances\n        ## extract syllables from results\n        syllables = {k: v[\"syllable\"] for k, v in results.items()}\n\n        ## extract and smooth centroids and headings\n        centroids = {k: v[\"centroid\"] for k, v in results.items()}\n        headings = {k: v[\"heading\"] for k, v in results.items()}\n\n        filter_size = 9  # default value\n        centroids, headings = filter_centroids_headings(\n            centroids, headings, filter_size=filter_size\n        )\n\n        # sample instances for each syllable\n        syllable_instances = get_syllable_instances(\n            syllables, min_duration=3, min_frequency=0.005\n        )\n\n        sampled_instances = sample_instances(\n            syllable_instances=syllable_instances,\n            num_samples=4 * 6,  # minimum rows * cols\n            coordinates=coordinates,\n            centroids=centroids,\n            headings=headings,\n        )\n\n        duration_seconds = None\n\n    self.insert1({**key, \"inference_duration\": duration_seconds})\n\n    for result_idx, result in results.items():\n        self.MotionSequence.insert1(\n            {\n                **key,\n                \"video_name\": result_idx,\n                \"syllable\": result[\"syllable\"],\n                \"latent_state\": result[\"latent_state\"],\n                \"centroid\": result[\"centroid\"],\n                \"heading\": result[\"heading\"],\n            }\n        )\n\n    for syllable, sampled_instance in sampled_instances.items():\n        self.GridMoviesSampledInstances.insert1(\n            {**key, \"syllable\": syllable, \"instances\": sampled_instance}\n        )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/", "title": "moseq_train.py", "text": ""}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.activate", "title": "<code>activate(train_schema_name, infer_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>train_schema_name</code> <code>str</code> <p>A string containing the name of the <code>moseq_train</code> schema.</p> required <code>infer_schema_name</code> <code>str</code> <p>A string containing the name of the <code>moseq_infer</code> schema.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True (default), schema  will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True (default), tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies: Functions:     get_kpms_root_data_dir(): Returns absolute path for root data director(y/ies)                              with all behavioral recordings, as (list of) string(s).     get_kpms_processed_data_dir(): Optional. Returns absolute path for processed                                   data.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>def activate(\n    train_schema_name: str,\n    infer_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n    \"\"\"Activate this schema.\n\n    Args:\n        train_schema_name (str): A string containing the name of the `moseq_train` schema.\n        infer_schema_name (str): A string containing the name of the `moseq_infer` schema.\n        create_schema (bool): If True (default), schema  will be created in the database.\n        create_tables (bool): If True (default), tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n    Dependencies:\n    Functions:\n        get_kpms_root_data_dir(): Returns absolute path for root data director(y/ies)\n                                 with all behavioral recordings, as (list of) string(s).\n        get_kpms_processed_data_dir(): Optional. Returns absolute path for processed\n                                      data.\n\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    assert hasattr(\n        linking_module, \"get_kpms_root_data_dir\"\n    ), \"The linking module must specify a lookup function for a root data directory\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    moseq_infer.activate(\n        infer_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n\n    schema.activate(\n        train_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.KeypointSet", "title": "<code>KeypointSet</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Store the keypoint data and the video set directory for model training.</p> <p>Attributes:</p> Name Type Description <code>kpset_id</code> <code>int)                          </code> <p>Unique ID for each keypoint set.</p> <code>PoseEstimationMethod</code> <code>foreign key)      </code> <p>Unique format method used to obtain the keypoints data.</p> <code>kpset_dir</code> <code>str)                         </code> <p>Path where the keypoint files are located together with the pose estimation <code>config</code> file, relative to root data directory.</p> <code>kpset_desc</code> <code>str)                            </code> <p>Optional. User-entered description.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass KeypointSet(dj.Manual):\n    \"\"\"Store the keypoint data and the video set directory for model training.\n\n    Attributes:\n        kpset_id (int)                          : Unique ID for each keypoint set.\n        PoseEstimationMethod (foreign key)      : Unique format method used to obtain the keypoints data.\n        kpset_dir (str)                         : Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory.\n        kpset_desc (str)                            : Optional. User-entered description.\n    \"\"\"\n\n    definition = \"\"\"\n    kpset_id                        : int           # Unique ID for each keypoint set   \n    ---\n    -&gt; moseq_infer.PoseEstimationMethod             # Unique format method used to obtain the keypoints data\n    kpset_dir                       : varchar(255)  # Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory \n    kpset_desc=''                   : varchar(1000) # Optional. User-entered description\n    \"\"\"\n\n    class VideoFile(dj.Part):\n        \"\"\"Store the IDs and file paths of each video file that will be used for model training.\n\n        Attributes:\n            KeypointSet (foreign key) : Unique ID for each keypoint set.\n            video_id (int)            : Unique ID for each video corresponding to each keypoint data file, relative to root data directory.\n            video_path (str)          : Filepath of each video from which the keypoints are derived, relative to root data directory.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        video_id                    : int           # Unique ID for each video corresponding to each keypoint data file, relative to root data directory\n        ---\n        video_path                  : varchar(1000) # Filepath of each video from which the keypoints are derived, relative to root data directory\n        \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.KeypointSet.VideoFile", "title": "<code>VideoFile</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Store the IDs and file paths of each video file that will be used for model training.</p> <p>Attributes:</p> Name Type Description <code>KeypointSet</code> <code>foreign key) </code> <p>Unique ID for each keypoint set.</p> <code>video_id</code> <code>int)            </code> <p>Unique ID for each video corresponding to each keypoint data file, relative to root data directory.</p> <code>video_path</code> <code>str)          </code> <p>Filepath of each video from which the keypoints are derived, relative to root data directory.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>class VideoFile(dj.Part):\n    \"\"\"Store the IDs and file paths of each video file that will be used for model training.\n\n    Attributes:\n        KeypointSet (foreign key) : Unique ID for each keypoint set.\n        video_id (int)            : Unique ID for each video corresponding to each keypoint data file, relative to root data directory.\n        video_path (str)          : Filepath of each video from which the keypoints are derived, relative to root data directory.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    video_id                    : int           # Unique ID for each video corresponding to each keypoint data file, relative to root data directory\n    ---\n    video_path                  : varchar(1000) # Filepath of each video from which the keypoints are derived, relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.Bodyparts", "title": "<code>Bodyparts</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Store the body parts to use in the analysis.</p> <p>Attributes:</p> Name Type Description <code>KeypointSet</code> <code>foreign key)       </code> <p>Unique ID for each <code>KeypointSet</code> key.</p> <code>bodyparts_id</code> <code>int)              </code> <p>Unique ID for a set of bodyparts for a particular keypoint set.</p> <code>anterior_bodyparts</code> <code>blob)       </code> <p>List of strings of anterior bodyparts</p> <code>posterior_bodyparts</code> <code>blob)      </code> <p>List of strings of posterior bodyparts</p> <code>use_bodyparts</code> <code>blob)            </code> <p>List of strings of bodyparts to be used</p> <code>bodyparts_desc(varchar)</code> <code> </code> <p>Optional. User-entered description.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass Bodyparts(dj.Manual):\n    \"\"\"Store the body parts to use in the analysis.\n\n    Attributes:\n        KeypointSet (foreign key)       : Unique ID for each `KeypointSet` key.\n        bodyparts_id (int)              : Unique ID for a set of bodyparts for a particular keypoint set.\n        anterior_bodyparts (blob)       : List of strings of anterior bodyparts\n        posterior_bodyparts (blob)      : List of strings of posterior bodyparts\n        use_bodyparts (blob)            : List of strings of bodyparts to be used\n        bodyparts_desc(varchar)         : Optional. User-entered description.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; KeypointSet                              # Unique ID for each `KeypointSet` key\n    bodyparts_id                : int           # Unique ID for a set of bodyparts for a particular keypoint set\n    ---\n    anterior_bodyparts          : blob          # List of strings of anterior bodyparts\n    posterior_bodyparts         : blob          # List of strings of posterior bodyparts\n    use_bodyparts               : blob          # List of strings of bodyparts to be used\n    bodyparts_desc=''           : varchar(1000) # Optional. User-entered description\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PCATask", "title": "<code>PCATask</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Staging table to define the PCA task and its output directory.</p> <p>Attributes:</p> Name Type Description <code>Bodyparts</code> <code>foreign key)         </code> <p>Unique ID for each <code>Bodyparts</code> key</p> <code>kpms_project_output_dir</code> <code>str)   </code> <p>Keypoint-MoSeq project output directory, relative to root data directory</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass PCATask(dj.Manual):\n    \"\"\"\n    Staging table to define the PCA task and its output directory.\n\n    Attributes:\n        Bodyparts (foreign key)         : Unique ID for each `Bodyparts` key\n        kpms_project_output_dir (str)   : Keypoint-MoSeq project output directory, relative to root data directory\n    \"\"\"\n\n    definition = \"\"\" \n    -&gt; Bodyparts                                                # Unique ID for each `Bodyparts` key\n    ---\n    kpms_project_output_dir=''          : varchar(255)          # Keypoint-MoSeq project output directory, relative to root data directory\n    task_mode='load'                 :enum('load','trigger') # Trigger or load the task\n\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PCAPrep", "title": "<code>PCAPrep</code>", "text": "<p>               Bases: <code>Imported</code></p> <p>Table to set up the Keypoint-MoSeq project output directory (<code>kpms_project_output_dir</code>) , creating the default <code>config.yml</code> and updating it in a new <code>kpms_dj_config.yml</code>.</p> <p>Attributes:</p> Name Type Description <code>PCATask</code> <code>foreign key)           </code> <p>Unique ID for each <code>PCATask</code> key.</p> <code>coordinates</code> <code>longblob)          </code> <p>Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3]).</p> <code>confidences</code> <code>longblob)          </code> <p>Dictionary mapping filenames to <code>likelihood</code> scores as ndarrays of shape (n_frames, n_bodyparts).</p> <code>formatted_bodyparts</code> <code>longblob)  </code> <p>List of bodypart names. The order of the names matches the order of the bodyparts in <code>coordinates</code> and <code>confidences</code>.</p> <code>average_frame_rate</code> <code>float)      </code> <p>Average frame rate of the videos for model training.</p> <code>frame_rates</code> <code>longblob)          </code> <p>List of the frame rates of the videos for model training.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass PCAPrep(dj.Imported):\n    \"\"\"\n    Table to set up the Keypoint-MoSeq project output directory (`kpms_project_output_dir`) , creating the default `config.yml` and updating it in a new `kpms_dj_config.yml`.\n\n    Attributes:\n        PCATask (foreign key)           : Unique ID for each `PCATask` key.\n        coordinates (longblob)          : Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3]).\n        confidences (longblob)          : Dictionary mapping filenames to `likelihood` scores as ndarrays of shape (n_frames, n_bodyparts).\n        formatted_bodyparts (longblob)  : List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.\n        average_frame_rate (float)      : Average frame rate of the videos for model training.\n        frame_rates (longblob)          : List of the frame rates of the videos for model training.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PCATask                          # Unique ID for each `PCATask` key\n    ---\n    coordinates             : longblob  # Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3])\n    confidences             : longblob  # Dictionary mapping filenames to `likelihood` scores as ndarrays of shape (n_frames, n_bodyparts)           \n    formatted_bodyparts     : longblob  # List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.\n    average_frame_rate      : float     # Average frame rate of the videos for model training\n    frame_rates             : longblob  # List of the frame rates of the videos for model training\n    \"\"\"\n\n    def make(self, key):\n        \"\"\"\n        Make function to:\n        1. Generate and update the `kpms_dj_config.yml` with both the videoset directory and the bodyparts.\n        2. Create the keypoint coordinates and confidences scores to format the data for the PCA fitting.\n\n        Args:\n            key (dict): Primary key from the `PCATask` table.\n\n        Raises:\n            NotImplementedError: `pose_estimation_method` is only supported for `deeplabcut`.\n\n        High-Level Logic:\n        1. Fetches the bodyparts, format method, and the directories for the Keypoint-MoSeq project output, the keypoint set, and the video set.\n        2. Set variables for each of the full path of the mentioned directories.\n        3. Find the first existing pose estimation config file in the `kpset_dir` directory, if not found, raise an error.\n        4. Check that the pose_estimation_method is `deeplabcut` and set up the project output directory with the default `config.yml`.\n        5. Create the `kpms_project_output_dir` (if it does not exist), and generates the kpms default `config.yml` with the default values from the pose estimation config.\n        6. Create a copy of the kpms `config.yml` named `kpms_dj_config.yml` that will be updated with both the `video_dir` and bodyparts\n        7. Load keypoint data from the keypoint files found in the `kpset_dir` that will serve as the training set.\n        8. As a result of the keypoint loading, the coordinates and confidences scores are generated and will be used to format the data for modeling.\n        9. Calculate the average frame rate and the frame rate list of the videoset from which the keypoint set is derived. This two attributes can be used to calculate the kappa value.\n        10. Insert the results of this `make` function into the table.\n        \"\"\"\n        from keypoint_moseq import setup_project, load_config, load_keypoints\n\n        anterior_bodyparts, posterior_bodyparts, use_bodyparts = (\n            Bodyparts &amp; key\n        ).fetch1(\n            \"anterior_bodyparts\",\n            \"posterior_bodyparts\",\n            \"use_bodyparts\",\n        )\n\n        pose_estimation_method, kpset_dir = (KeypointSet &amp; key).fetch1(\n            \"pose_estimation_method\", \"kpset_dir\"\n        )\n        video_paths, video_ids = (KeypointSet.VideoFile &amp; key).fetch(\n            \"video_path\", \"video_id\"\n        )\n\n        kpms_root = moseq_infer.get_kpms_root_data_dir()\n        kpms_processed = moseq_infer.get_kpms_processed_data_dir()\n\n        kpms_project_output_dir, task_mode = (PCATask &amp; key).fetch1(\n            \"kpms_project_output_dir\", \"task_mode\"\n        )\n\n        if task_mode == \"trigger\":\n            try:\n                kpms_project_output_dir = find_full_path(\n                    kpms_processed, kpms_project_output_dir\n                )\n\n            except FileNotFoundError:\n                kpms_project_output_dir = kpms_processed / kpms_project_output_dir\n\n            kpset_dir = find_full_path(kpms_root, kpset_dir)\n            videos_dir = find_full_path(kpms_root, Path(video_paths[0]).parent)\n\n            if pose_estimation_method == \"deeplabcut\":\n                setup_project(\n                    project_dir=kpms_project_output_dir.as_posix(),\n                    deeplabcut_config=(kpset_dir / \"config.yaml\")\n                    or (kpset_dir / \"config.yml\"),\n                )\n            else:\n                raise NotImplementedError(\n                    \"Currently, `deeplabcut` is the only pose estimation method supported by this Element. Please reach out at `support@datajoint.com` if you use another method.\"\n                )\n\n            kpms_config = load_config(\n                kpms_project_output_dir.as_posix(),\n                check_if_valid=True,\n                build_indexes=False,\n            )\n\n            kpms_dj_config_kwargs_dict = dict(\n                video_dir=videos_dir.as_posix(),\n                anterior_bodyparts=anterior_bodyparts,\n                posterior_bodyparts=posterior_bodyparts,\n                use_bodyparts=use_bodyparts,\n            )\n            kpms_config.update(**kpms_dj_config_kwargs_dict)\n            kpms_reader.generate_kpms_dj_config(\n                kpms_project_output_dir.as_posix(), **kpms_config\n            )\n        else:\n            kpms_project_output_dir = find_full_path(\n                kpms_processed, kpms_project_output_dir\n            )\n            kpset_dir = find_full_path(kpms_root, kpset_dir)\n            videos_dir = find_full_path(kpms_root, Path(video_paths[0]).parent)\n\n        coordinates, confidences, formatted_bodyparts = load_keypoints(\n            filepath_pattern=kpset_dir, format=pose_estimation_method\n        )\n\n        frame_rate_list = []\n        for fp, _ in zip(video_paths, video_ids):\n            video_path = (find_full_path(kpms_root, fp)).as_posix()\n            cap = cv2.VideoCapture(video_path)\n            frame_rate_list.append(int(cap.get(cv2.CAP_PROP_FPS)))\n            cap.release()\n        average_frame_rate = int(np.mean(frame_rate_list))\n\n        self.insert1(\n            dict(\n                **key,\n                coordinates=coordinates,\n                confidences=confidences,\n                formatted_bodyparts=formatted_bodyparts,\n                average_frame_rate=average_frame_rate,\n                frame_rates=frame_rate_list,\n            )\n        )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PCAPrep.make", "title": "<code>make(key)</code>", "text": "<p>Make function to: 1. Generate and update the <code>kpms_dj_config.yml</code> with both the videoset directory and the bodyparts. 2. Create the keypoint coordinates and confidences scores to format the data for the PCA fitting.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the <code>PCATask</code> table.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p><code>pose_estimation_method</code> is only supported for <code>deeplabcut</code>.</p> <p>High-Level Logic: 1. Fetches the bodyparts, format method, and the directories for the Keypoint-MoSeq project output, the keypoint set, and the video set. 2. Set variables for each of the full path of the mentioned directories. 3. Find the first existing pose estimation config file in the <code>kpset_dir</code> directory, if not found, raise an error. 4. Check that the pose_estimation_method is <code>deeplabcut</code> and set up the project output directory with the default <code>config.yml</code>. 5. Create the <code>kpms_project_output_dir</code> (if it does not exist), and generates the kpms default <code>config.yml</code> with the default values from the pose estimation config. 6. Create a copy of the kpms <code>config.yml</code> named <code>kpms_dj_config.yml</code> that will be updated with both the <code>video_dir</code> and bodyparts 7. Load keypoint data from the keypoint files found in the <code>kpset_dir</code> that will serve as the training set. 8. As a result of the keypoint loading, the coordinates and confidences scores are generated and will be used to format the data for modeling. 9. Calculate the average frame rate and the frame rate list of the videoset from which the keypoint set is derived. This two attributes can be used to calculate the kappa value. 10. Insert the results of this <code>make</code> function into the table.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>def make(self, key):\n    \"\"\"\n    Make function to:\n    1. Generate and update the `kpms_dj_config.yml` with both the videoset directory and the bodyparts.\n    2. Create the keypoint coordinates and confidences scores to format the data for the PCA fitting.\n\n    Args:\n        key (dict): Primary key from the `PCATask` table.\n\n    Raises:\n        NotImplementedError: `pose_estimation_method` is only supported for `deeplabcut`.\n\n    High-Level Logic:\n    1. Fetches the bodyparts, format method, and the directories for the Keypoint-MoSeq project output, the keypoint set, and the video set.\n    2. Set variables for each of the full path of the mentioned directories.\n    3. Find the first existing pose estimation config file in the `kpset_dir` directory, if not found, raise an error.\n    4. Check that the pose_estimation_method is `deeplabcut` and set up the project output directory with the default `config.yml`.\n    5. Create the `kpms_project_output_dir` (if it does not exist), and generates the kpms default `config.yml` with the default values from the pose estimation config.\n    6. Create a copy of the kpms `config.yml` named `kpms_dj_config.yml` that will be updated with both the `video_dir` and bodyparts\n    7. Load keypoint data from the keypoint files found in the `kpset_dir` that will serve as the training set.\n    8. As a result of the keypoint loading, the coordinates and confidences scores are generated and will be used to format the data for modeling.\n    9. Calculate the average frame rate and the frame rate list of the videoset from which the keypoint set is derived. This two attributes can be used to calculate the kappa value.\n    10. Insert the results of this `make` function into the table.\n    \"\"\"\n    from keypoint_moseq import setup_project, load_config, load_keypoints\n\n    anterior_bodyparts, posterior_bodyparts, use_bodyparts = (\n        Bodyparts &amp; key\n    ).fetch1(\n        \"anterior_bodyparts\",\n        \"posterior_bodyparts\",\n        \"use_bodyparts\",\n    )\n\n    pose_estimation_method, kpset_dir = (KeypointSet &amp; key).fetch1(\n        \"pose_estimation_method\", \"kpset_dir\"\n    )\n    video_paths, video_ids = (KeypointSet.VideoFile &amp; key).fetch(\n        \"video_path\", \"video_id\"\n    )\n\n    kpms_root = moseq_infer.get_kpms_root_data_dir()\n    kpms_processed = moseq_infer.get_kpms_processed_data_dir()\n\n    kpms_project_output_dir, task_mode = (PCATask &amp; key).fetch1(\n        \"kpms_project_output_dir\", \"task_mode\"\n    )\n\n    if task_mode == \"trigger\":\n        try:\n            kpms_project_output_dir = find_full_path(\n                kpms_processed, kpms_project_output_dir\n            )\n\n        except FileNotFoundError:\n            kpms_project_output_dir = kpms_processed / kpms_project_output_dir\n\n        kpset_dir = find_full_path(kpms_root, kpset_dir)\n        videos_dir = find_full_path(kpms_root, Path(video_paths[0]).parent)\n\n        if pose_estimation_method == \"deeplabcut\":\n            setup_project(\n                project_dir=kpms_project_output_dir.as_posix(),\n                deeplabcut_config=(kpset_dir / \"config.yaml\")\n                or (kpset_dir / \"config.yml\"),\n            )\n        else:\n            raise NotImplementedError(\n                \"Currently, `deeplabcut` is the only pose estimation method supported by this Element. Please reach out at `support@datajoint.com` if you use another method.\"\n            )\n\n        kpms_config = load_config(\n            kpms_project_output_dir.as_posix(),\n            check_if_valid=True,\n            build_indexes=False,\n        )\n\n        kpms_dj_config_kwargs_dict = dict(\n            video_dir=videos_dir.as_posix(),\n            anterior_bodyparts=anterior_bodyparts,\n            posterior_bodyparts=posterior_bodyparts,\n            use_bodyparts=use_bodyparts,\n        )\n        kpms_config.update(**kpms_dj_config_kwargs_dict)\n        kpms_reader.generate_kpms_dj_config(\n            kpms_project_output_dir.as_posix(), **kpms_config\n        )\n    else:\n        kpms_project_output_dir = find_full_path(\n            kpms_processed, kpms_project_output_dir\n        )\n        kpset_dir = find_full_path(kpms_root, kpset_dir)\n        videos_dir = find_full_path(kpms_root, Path(video_paths[0]).parent)\n\n    coordinates, confidences, formatted_bodyparts = load_keypoints(\n        filepath_pattern=kpset_dir, format=pose_estimation_method\n    )\n\n    frame_rate_list = []\n    for fp, _ in zip(video_paths, video_ids):\n        video_path = (find_full_path(kpms_root, fp)).as_posix()\n        cap = cv2.VideoCapture(video_path)\n        frame_rate_list.append(int(cap.get(cv2.CAP_PROP_FPS)))\n        cap.release()\n    average_frame_rate = int(np.mean(frame_rate_list))\n\n    self.insert1(\n        dict(\n            **key,\n            coordinates=coordinates,\n            confidences=confidences,\n            formatted_bodyparts=formatted_bodyparts,\n            average_frame_rate=average_frame_rate,\n            frame_rates=frame_rate_list,\n        )\n    )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PCAFit", "title": "<code>PCAFit</code>", "text": "<p>               Bases: <code>Computed</code></p> <p>Fit PCA model.</p> <p>Attributes:</p> Name Type Description <code>PCAPrep</code> <code>foreign key)           </code> <p><code>PCAPrep</code> Key.</p> <code>pca_fit_time</code> <code>datetime)         </code> <p>datetime of the PCA fitting analysis.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass PCAFit(dj.Computed):\n    \"\"\"Fit PCA model.\n\n    Attributes:\n        PCAPrep (foreign key)           : `PCAPrep` Key.\n        pca_fit_time (datetime)         : datetime of the PCA fitting analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PCAPrep                           # `PCAPrep` Key\n    ---\n    pca_fit_time=NULL        : datetime  # datetime of the PCA fitting analysis\n    \"\"\"\n\n    def make(self, key):\n        \"\"\"\n        Make function to format the keypoint data, fit the PCA model, and store it as a `pca.p` file in the Keypoint-MoSeq project output directory.\n\n        Args:\n            key (dict): `PCAPrep` Key\n\n        Raises:\n\n        High-Level Logic:\n        1. Fetch the `kpms_project_output_dir` from the `PCATask` table and define its full path.\n        2. Load the `kpms_dj_config` file that contains the updated `video_dir` and bodyparts, \\\n           and format the keypoint data with the coordinates and confidences scores to be used in the PCA fitting.\n        3. Fit the PCA model and save it as `pca.p` file in the output directory.\n        4.Insert the creation datetime as the `pca_fit_time` into the table.\n        \"\"\"\n        from keypoint_moseq import format_data, fit_pca, save_pca\n\n        kpms_project_output_dir, task_mode = (PCATask &amp; key).fetch1(\n            \"kpms_project_output_dir\", \"task_mode\"\n        )\n        kpms_project_output_dir = (\n            moseq_infer.get_kpms_processed_data_dir() / kpms_project_output_dir\n        )\n\n        kpms_default_config = kpms_reader.load_kpms_dj_config(\n            kpms_project_output_dir.as_posix(), check_if_valid=True, build_indexes=True\n        )\n        coordinates, confidences = (PCAPrep &amp; key).fetch1(\"coordinates\", \"confidences\")\n        data, _ = format_data(\n            **kpms_default_config, coordinates=coordinates, confidences=confidences\n        )\n\n        if task_mode == \"trigger\":\n            pca = fit_pca(**data, **kpms_default_config)\n            save_pca(pca, kpms_project_output_dir.as_posix())\n            creation_datetime = datetime.now(timezone.utc)\n        else:\n            creation_datetime = None\n\n        self.insert1(dict(**key, pca_fit_time=creation_datetime))\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PCAFit.make", "title": "<code>make(key)</code>", "text": "<p>Make function to format the keypoint data, fit the PCA model, and store it as a <code>pca.p</code> file in the Keypoint-MoSeq project output directory.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p><code>PCAPrep</code> Key</p> required <p>Raises:</p> <p>High-Level Logic: 1. Fetch the <code>kpms_project_output_dir</code> from the <code>PCATask</code> table and define its full path. 2. Load the <code>kpms_dj_config</code> file that contains the updated <code>video_dir</code> and bodyparts,            and format the keypoint data with the coordinates and confidences scores to be used in the PCA fitting. 3. Fit the PCA model and save it as <code>pca.p</code> file in the output directory. 4.Insert the creation datetime as the <code>pca_fit_time</code> into the table.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>def make(self, key):\n    \"\"\"\n    Make function to format the keypoint data, fit the PCA model, and store it as a `pca.p` file in the Keypoint-MoSeq project output directory.\n\n    Args:\n        key (dict): `PCAPrep` Key\n\n    Raises:\n\n    High-Level Logic:\n    1. Fetch the `kpms_project_output_dir` from the `PCATask` table and define its full path.\n    2. Load the `kpms_dj_config` file that contains the updated `video_dir` and bodyparts, \\\n       and format the keypoint data with the coordinates and confidences scores to be used in the PCA fitting.\n    3. Fit the PCA model and save it as `pca.p` file in the output directory.\n    4.Insert the creation datetime as the `pca_fit_time` into the table.\n    \"\"\"\n    from keypoint_moseq import format_data, fit_pca, save_pca\n\n    kpms_project_output_dir, task_mode = (PCATask &amp; key).fetch1(\n        \"kpms_project_output_dir\", \"task_mode\"\n    )\n    kpms_project_output_dir = (\n        moseq_infer.get_kpms_processed_data_dir() / kpms_project_output_dir\n    )\n\n    kpms_default_config = kpms_reader.load_kpms_dj_config(\n        kpms_project_output_dir.as_posix(), check_if_valid=True, build_indexes=True\n    )\n    coordinates, confidences = (PCAPrep &amp; key).fetch1(\"coordinates\", \"confidences\")\n    data, _ = format_data(\n        **kpms_default_config, coordinates=coordinates, confidences=confidences\n    )\n\n    if task_mode == \"trigger\":\n        pca = fit_pca(**data, **kpms_default_config)\n        save_pca(pca, kpms_project_output_dir.as_posix())\n        creation_datetime = datetime.now(timezone.utc)\n    else:\n        creation_datetime = None\n\n    self.insert1(dict(**key, pca_fit_time=creation_datetime))\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.LatentDimension", "title": "<code>LatentDimension</code>", "text": "<p>               Bases: <code>Imported</code></p> <p>Determine the latent dimension as part of the autoregressive hyperparameters (<code>ar_hypparams</code>) for the model fitting. The objective of the analysis is to inform the user about the number of principal components needed to explain a 90% variance threshold. Subsequently, the decision on how many components to utilize for the model fitting is left to the user.</p> <p>Attributes:</p> Name Type Description <code>PCAFit</code> <code>foreign key)               </code> <p><code>PCAFit</code> Key.</p> <code>variance_percentage</code> <code>float)        </code> <p>Variance threshold. Fixed value to 90%.</p> <code>latent_dimension</code> <code>int)             </code> <p>Number of principal components required to explain the specified variance.</p> <code>latent_dim_desc</code> <code>varchar)          </code> <p>Automated description of the computation result.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass LatentDimension(dj.Imported):\n    \"\"\"\n    Determine the latent dimension as part of the autoregressive hyperparameters (`ar_hypparams`) for the model fitting.\n    The objective of the analysis is to inform the user about the number of principal components needed to explain a\n    90% variance threshold. Subsequently, the decision on how many components to utilize for the model fitting is left\n    to the user.\n\n    Attributes:\n        PCAFit (foreign key)               : `PCAFit` Key.\n        variance_percentage (float)        : Variance threshold. Fixed value to 90%.\n        latent_dimension (int)             : Number of principal components required to explain the specified variance.\n        latent_dim_desc (varchar)          : Automated description of the computation result.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PCAFit                                   # `PCAFit` Key\n    ---\n    variance_percentage      : float            # Variance threshold. Fixed value to 90 percent.\n    latent_dimension         : int              # Number of principal components required to explain the specified variance.\n    latent_dim_desc          : varchar(1000)    # Automated description of the computation result.\n    \"\"\"\n\n    def make(self, key):\n        \"\"\"\n        Make function to compute and store the latent dimension that explains a 90% variance threshold.\n\n        Args:\n            key (dict): `PCAFit` Key.\n\n        Raises:\n\n        High-Level Logic:\n        1. Fetches the Keypoint-MoSeq project output directory from the PCATask table and define the full path.\n        2. Load the PCA model from file in this directory.\n        2. Set a specified variance threshold to 90% and compute the cumulative sum of the explained variance ratio.\n        3. Determine the number of components required to explain the specified variance.\n            3.1 If the cumulative sum of the explained variance ratio is less than the specified variance threshold, \\\n                it sets the `latent_dimension` to the total number of components and `variance_percentage` to the cumulative sum of the explained variance ratio.\n            3.2 If the cumulative sum of the explained variance ratio is greater than the specified variance threshold, \\\n                it sets the `latent_dimension` to the number of components that explain the specified variance and `variance_percentage` to the specified variance threshold.\n        4. Insert the results of this `make` function into the table.\n        \"\"\"\n        from keypoint_moseq import load_pca\n\n        kpms_project_output_dir = (PCATask &amp; key).fetch1(\"kpms_project_output_dir\")\n        kpms_project_output_dir = (\n            moseq_infer.get_kpms_processed_data_dir() / kpms_project_output_dir\n        )\n\n        pca_path = kpms_project_output_dir / \"pca.p\"\n        if pca_path:\n            pca = load_pca(kpms_project_output_dir.as_posix())\n        else:\n            raise FileNotFoundError(\n                f\"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}\"\n            )\n\n        variance_threshold = 0.90\n\n        cs = np.cumsum(\n            pca.explained_variance_ratio_\n        )  # explained_variance_ratio_ndarray of shape (n_components,)\n\n        if cs[-1] &lt; variance_threshold:\n            latent_dimension = len(cs)\n            variance_percentage = cs[-1] * 100\n            latent_dim_desc = (\n                f\"All components together only explain {cs[-1]*100}% of variance.\"\n            )\n        else:\n            latent_dimension = (cs &gt; variance_threshold).nonzero()[0].min() + 1\n            variance_percentage = variance_threshold * 100\n            latent_dim_desc = f\"&gt;={variance_threshold*100}% of variance explained by {(cs&gt;variance_threshold).nonzero()[0].min()+1} components.\"\n\n        self.insert1(\n            dict(\n                **key,\n                variance_percentage=variance_percentage,\n                latent_dimension=latent_dimension,\n                latent_dim_desc=latent_dim_desc,\n            )\n        )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.LatentDimension.make", "title": "<code>make(key)</code>", "text": "<p>Make function to compute and store the latent dimension that explains a 90% variance threshold.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p><code>PCAFit</code> Key.</p> required <p>Raises:</p> <p>High-Level Logic: 1. Fetches the Keypoint-MoSeq project output directory from the PCATask table and define the full path. 2. Load the PCA model from file in this directory. 2. Set a specified variance threshold to 90% and compute the cumulative sum of the explained variance ratio. 3. Determine the number of components required to explain the specified variance.     3.1 If the cumulative sum of the explained variance ratio is less than the specified variance threshold,                 it sets the <code>latent_dimension</code> to the total number of components and <code>variance_percentage</code> to the cumulative sum of the explained variance ratio.     3.2 If the cumulative sum of the explained variance ratio is greater than the specified variance threshold,                 it sets the <code>latent_dimension</code> to the number of components that explain the specified variance and <code>variance_percentage</code> to the specified variance threshold. 4. Insert the results of this <code>make</code> function into the table.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>def make(self, key):\n    \"\"\"\n    Make function to compute and store the latent dimension that explains a 90% variance threshold.\n\n    Args:\n        key (dict): `PCAFit` Key.\n\n    Raises:\n\n    High-Level Logic:\n    1. Fetches the Keypoint-MoSeq project output directory from the PCATask table and define the full path.\n    2. Load the PCA model from file in this directory.\n    2. Set a specified variance threshold to 90% and compute the cumulative sum of the explained variance ratio.\n    3. Determine the number of components required to explain the specified variance.\n        3.1 If the cumulative sum of the explained variance ratio is less than the specified variance threshold, \\\n            it sets the `latent_dimension` to the total number of components and `variance_percentage` to the cumulative sum of the explained variance ratio.\n        3.2 If the cumulative sum of the explained variance ratio is greater than the specified variance threshold, \\\n            it sets the `latent_dimension` to the number of components that explain the specified variance and `variance_percentage` to the specified variance threshold.\n    4. Insert the results of this `make` function into the table.\n    \"\"\"\n    from keypoint_moseq import load_pca\n\n    kpms_project_output_dir = (PCATask &amp; key).fetch1(\"kpms_project_output_dir\")\n    kpms_project_output_dir = (\n        moseq_infer.get_kpms_processed_data_dir() / kpms_project_output_dir\n    )\n\n    pca_path = kpms_project_output_dir / \"pca.p\"\n    if pca_path:\n        pca = load_pca(kpms_project_output_dir.as_posix())\n    else:\n        raise FileNotFoundError(\n            f\"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}\"\n        )\n\n    variance_threshold = 0.90\n\n    cs = np.cumsum(\n        pca.explained_variance_ratio_\n    )  # explained_variance_ratio_ndarray of shape (n_components,)\n\n    if cs[-1] &lt; variance_threshold:\n        latent_dimension = len(cs)\n        variance_percentage = cs[-1] * 100\n        latent_dim_desc = (\n            f\"All components together only explain {cs[-1]*100}% of variance.\"\n        )\n    else:\n        latent_dimension = (cs &gt; variance_threshold).nonzero()[0].min() + 1\n        variance_percentage = variance_threshold * 100\n        latent_dim_desc = f\"&gt;={variance_threshold*100}% of variance explained by {(cs&gt;variance_threshold).nonzero()[0].min()+1} components.\"\n\n    self.insert1(\n        dict(\n            **key,\n            variance_percentage=variance_percentage,\n            latent_dimension=latent_dimension,\n            latent_dim_desc=latent_dim_desc,\n        )\n    )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PreFitTask", "title": "<code>PreFitTask</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Insert the parameters for the model (AR-HMM) pre-fitting.</p> <p>Attributes:</p> Name Type Description <code>PCAFit</code> <code>foreign key)                </code> <p><code>PCAFit</code> task.</p> <code>pre_latent_dim</code> <code>int)                </code> <p>Latent dimension to use for the model pre-fitting.</p> <code>pre_kappa</code> <code>int)                     </code> <p>Kappa value to use for the model pre-fitting.</p> <code>pre_num_iterations</code> <code>int)            </code> <p>Number of Gibbs sampling iterations to run in the model pre-fitting.</p> <code>pre_fit_desc(varchar)</code> <code> </code> <p>User-defined description of the pre-fitting task.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass PreFitTask(dj.Manual):\n    \"\"\"Insert the parameters for the model (AR-HMM) pre-fitting.\n\n    Attributes:\n        PCAFit (foreign key)                : `PCAFit` task.\n        pre_latent_dim (int)                : Latent dimension to use for the model pre-fitting.\n        pre_kappa (int)                     : Kappa value to use for the model pre-fitting.\n        pre_num_iterations (int)            : Number of Gibbs sampling iterations to run in the model pre-fitting.\n        pre_fit_desc(varchar)               : User-defined description of the pre-fitting task.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PCAFit                                           # `PCAFit` Key\n    pre_latent_dim               : int                  # Latent dimension to use for the model pre-fitting\n    pre_kappa                    : int                  # Kappa value to use for the model pre-fitting\n    pre_num_iterations           : int                  # Number of Gibbs sampling iterations to run in the model pre-fitting\n    ---\n    model_name                   : varchar(100)         # Name of the model to be loaded if `task_mode='load'`\n    task_mode='load'             :enum('trigger','load')# 'load': load computed analysis results, 'trigger': trigger computation\n    pre_fit_desc=''              : varchar(1000)        # User-defined description of the pre-fitting task\n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PreFit", "title": "<code>PreFit</code>", "text": "<p>               Bases: <code>Computed</code></p> <p>Fit AR-HMM model.</p> <p>Attributes:</p> Name Type Description <code>PreFitTask</code> <code>foreign key)                </code> <p><code>PreFitTask</code> Key.</p> <code>model_name</code> <code>varchar)                    </code> <p>Name of the model as \"kpms_project_output_dir/model_name\".</p> <code>pre_fit_duration</code> <code>float)                </code> <p>Time duration (seconds) of the model fitting computation.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass PreFit(dj.Computed):\n    \"\"\"Fit AR-HMM model.\n\n    Attributes:\n        PreFitTask (foreign key)                : `PreFitTask` Key.\n        model_name (varchar)                    : Name of the model as \"kpms_project_output_dir/model_name\".\n        pre_fit_duration (float)                : Time duration (seconds) of the model fitting computation.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PreFitTask                               # `PreFitTask` Key\n    ---\n    model_name=''                : varchar(100) # Name of the model as \"kpms_project_output_dir/model_name\"\n    pre_fit_duration=NULL        : float        # Time duration (seconds) of the model fitting computation\n    \"\"\"\n\n    def make(self, key):\n        \"\"\"\n        Make function to fit the AR-HMM model using the latent trajectory defined by `model['states']['x'].\n\n        Args:\n            key (dict) : dictionary with the `PreFitTask` Key.\n\n        Raises:\n\n        High-level Logic:\n        1. Fetch the `kpms_project_output_dir` and define the full path.\n        2. Fetch the model parameters from the `PreFitTask` table.\n        3. Update the `dj_config.yml` with the latent dimension and kappa for the AR-HMM fitting.\n        4. Load the pca model from file in the `kpms_project_output_dir`.\n        5. Fetch `coordinates` and `confidences` scores to format the data for the model initialization. \\\n            # Data - contains the data for model fitting. \\\n            # Metadata - contains the recordings and start/end frames for the data.\n        6. Initialize the model that create a `model` dict containing states, parameters, hyperparameters, noise prior, and random seed.\n        7. Update the model dict with the selected kappa for the AR-HMM fitting.\n        8. Fit the AR-HMM model using the `pre_num_iterations` and create a subdirectory in `kpms_project_output_dir` with the model's latest checkpoint file.\n        9. Calculate the duration of the model fitting computation and insert it in the `PreFit` table.\n        \"\"\"\n        from keypoint_moseq import (\n            load_pca,\n            format_data,\n            init_model,\n            update_hypparams,\n            fit_model,\n        )\n\n        kpms_processed = moseq_infer.get_kpms_processed_data_dir()\n\n        kpms_project_output_dir = find_full_path(\n            kpms_processed, (PCATask &amp; key).fetch1(\"kpms_project_output_dir\")\n        )\n\n        pre_latent_dim, pre_kappa, pre_num_iterations, task_mode, model_name = (\n            PreFitTask &amp; key\n        ).fetch1(\n            \"pre_latent_dim\",\n            \"pre_kappa\",\n            \"pre_num_iterations\",\n            \"task_mode\",\n            \"model_name\",\n        )\n        if task_mode == \"trigger\":\n            kpms_dj_config = kpms_reader.load_kpms_dj_config(\n                kpms_project_output_dir.as_posix(),\n                check_if_valid=True,\n                build_indexes=True,\n            )\n\n            kpms_dj_config.update(\n                dict(latent_dim=int(pre_latent_dim), kappa=float(pre_kappa))\n            )\n            kpms_reader.generate_kpms_dj_config(\n                kpms_project_output_dir.as_posix(), **kpms_dj_config\n            )\n\n            pca_path = kpms_project_output_dir / \"pca.p\"\n            if pca_path:\n                pca = load_pca(kpms_project_output_dir.as_posix())\n            else:\n                raise FileNotFoundError(\n                    f\"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}\"\n                )\n\n            coordinates, confidences = (PCAPrep &amp; key).fetch1(\n                \"coordinates\", \"confidences\"\n            )\n            data, metadata = format_data(coordinates, confidences, **kpms_dj_config)\n\n            model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)\n\n            model = update_hypparams(\n                model, kappa=float(pre_kappa), latent_dim=int(pre_latent_dim)\n            )\n\n            start_time = datetime.now()\n            model, model_name = fit_model(\n                model=model,\n                data=data,\n                metadata=metadata,\n                project_dir=kpms_project_output_dir.as_posix(),\n                ar_only=True,\n                num_iters=pre_num_iterations,\n            )\n            end_time = datetime.now()\n\n            duration_seconds = (end_time - start_time).total_seconds()\n        else:\n            duration_seconds = None\n\n        self.insert1(\n            {\n                **key,\n                \"model_name\": (\n                    kpms_project_output_dir.relative_to(kpms_processed) / model_name\n                ).as_posix(),\n                \"pre_fit_duration\": duration_seconds,\n            }\n        )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.PreFit.make", "title": "<code>make(key)</code>", "text": "<p>Make function to fit the AR-HMM model using the latent trajectory defined by `model'states'.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict) </code> <p>dictionary with the <code>PreFitTask</code> Key.</p> required <p>Raises:</p> <p>High-level Logic: 1. Fetch the <code>kpms_project_output_dir</code> and define the full path. 2. Fetch the model parameters from the <code>PreFitTask</code> table. 3. Update the <code>dj_config.yml</code> with the latent dimension and kappa for the AR-HMM fitting. 4. Load the pca model from file in the <code>kpms_project_output_dir</code>. 5. Fetch <code>coordinates</code> and <code>confidences</code> scores to format the data for the model initialization.             # Data - contains the data for model fitting.             # Metadata - contains the recordings and start/end frames for the data. 6. Initialize the model that create a <code>model</code> dict containing states, parameters, hyperparameters, noise prior, and random seed. 7. Update the model dict with the selected kappa for the AR-HMM fitting. 8. Fit the AR-HMM model using the <code>pre_num_iterations</code> and create a subdirectory in <code>kpms_project_output_dir</code> with the model's latest checkpoint file. 9. Calculate the duration of the model fitting computation and insert it in the <code>PreFit</code> table.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>def make(self, key):\n    \"\"\"\n    Make function to fit the AR-HMM model using the latent trajectory defined by `model['states']['x'].\n\n    Args:\n        key (dict) : dictionary with the `PreFitTask` Key.\n\n    Raises:\n\n    High-level Logic:\n    1. Fetch the `kpms_project_output_dir` and define the full path.\n    2. Fetch the model parameters from the `PreFitTask` table.\n    3. Update the `dj_config.yml` with the latent dimension and kappa for the AR-HMM fitting.\n    4. Load the pca model from file in the `kpms_project_output_dir`.\n    5. Fetch `coordinates` and `confidences` scores to format the data for the model initialization. \\\n        # Data - contains the data for model fitting. \\\n        # Metadata - contains the recordings and start/end frames for the data.\n    6. Initialize the model that create a `model` dict containing states, parameters, hyperparameters, noise prior, and random seed.\n    7. Update the model dict with the selected kappa for the AR-HMM fitting.\n    8. Fit the AR-HMM model using the `pre_num_iterations` and create a subdirectory in `kpms_project_output_dir` with the model's latest checkpoint file.\n    9. Calculate the duration of the model fitting computation and insert it in the `PreFit` table.\n    \"\"\"\n    from keypoint_moseq import (\n        load_pca,\n        format_data,\n        init_model,\n        update_hypparams,\n        fit_model,\n    )\n\n    kpms_processed = moseq_infer.get_kpms_processed_data_dir()\n\n    kpms_project_output_dir = find_full_path(\n        kpms_processed, (PCATask &amp; key).fetch1(\"kpms_project_output_dir\")\n    )\n\n    pre_latent_dim, pre_kappa, pre_num_iterations, task_mode, model_name = (\n        PreFitTask &amp; key\n    ).fetch1(\n        \"pre_latent_dim\",\n        \"pre_kappa\",\n        \"pre_num_iterations\",\n        \"task_mode\",\n        \"model_name\",\n    )\n    if task_mode == \"trigger\":\n        kpms_dj_config = kpms_reader.load_kpms_dj_config(\n            kpms_project_output_dir.as_posix(),\n            check_if_valid=True,\n            build_indexes=True,\n        )\n\n        kpms_dj_config.update(\n            dict(latent_dim=int(pre_latent_dim), kappa=float(pre_kappa))\n        )\n        kpms_reader.generate_kpms_dj_config(\n            kpms_project_output_dir.as_posix(), **kpms_dj_config\n        )\n\n        pca_path = kpms_project_output_dir / \"pca.p\"\n        if pca_path:\n            pca = load_pca(kpms_project_output_dir.as_posix())\n        else:\n            raise FileNotFoundError(\n                f\"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}\"\n            )\n\n        coordinates, confidences = (PCAPrep &amp; key).fetch1(\n            \"coordinates\", \"confidences\"\n        )\n        data, metadata = format_data(coordinates, confidences, **kpms_dj_config)\n\n        model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)\n\n        model = update_hypparams(\n            model, kappa=float(pre_kappa), latent_dim=int(pre_latent_dim)\n        )\n\n        start_time = datetime.now()\n        model, model_name = fit_model(\n            model=model,\n            data=data,\n            metadata=metadata,\n            project_dir=kpms_project_output_dir.as_posix(),\n            ar_only=True,\n            num_iters=pre_num_iterations,\n        )\n        end_time = datetime.now()\n\n        duration_seconds = (end_time - start_time).total_seconds()\n    else:\n        duration_seconds = None\n\n    self.insert1(\n        {\n            **key,\n            \"model_name\": (\n                kpms_project_output_dir.relative_to(kpms_processed) / model_name\n            ).as_posix(),\n            \"pre_fit_duration\": duration_seconds,\n        }\n    )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.FullFitTask", "title": "<code>FullFitTask</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Insert the parameters for the full (Keypoint-SLDS model) fitting.    The full model will generally require a lower value of kappa to yield the same target syllable durations.</p> <p>Attributes:</p> Name Type Description <code>PCAFit</code> <code>foreign key)                 </code> <p><code>PCAFit</code> Key.</p> <code>full_latent_dim</code> <code>int)                </code> <p>Latent dimension to use for the model full fitting.</p> <code>full_kappa</code> <code>int)                     </code> <p>Kappa value to use for the model full fitting.</p> <code>full_num_iterations</code> <code>int)            </code> <p>Number of Gibbs sampling iterations to run in the model full fitting.</p> <code>full_fit_desc(varchar)</code> <code> </code> <p>User-defined description of the model full fitting task.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass FullFitTask(dj.Manual):\n    \"\"\"Insert the parameters for the full (Keypoint-SLDS model) fitting.\n       The full model will generally require a lower value of kappa to yield the same target syllable durations.\n\n    Attributes:\n        PCAFit (foreign key)                 : `PCAFit` Key.\n        full_latent_dim (int)                : Latent dimension to use for the model full fitting.\n        full_kappa (int)                     : Kappa value to use for the model full fitting.\n        full_num_iterations (int)            : Number of Gibbs sampling iterations to run in the model full fitting.\n        full_fit_desc(varchar)               : User-defined description of the model full fitting task.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PCAFit                                           # `PCAFit` Key\n    full_latent_dim              : int                  # Latent dimension to use for the model full fitting\n    full_kappa                   : int                  # Kappa value to use for the model full fitting\n    full_num_iterations          : int                  # Number of Gibbs sampling iterations to run in the model full fitting\n    ---\n    model_name                   : varchar(100)         # Name of the model to be loaded if `task_mode='load'`\n    task_mode='load'             :enum('load','trigger')# Trigger or load the task\n    full_fit_desc=''             : varchar(1000)        # User-defined description of the model full fitting task   \n    \"\"\"\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.FullFit", "title": "<code>FullFit</code>", "text": "<p>               Bases: <code>Computed</code></p> <p>Fit the full (Keypoint-SLDS) model.</p> <p>Attributes:</p> Name Type Description <code>FullFitTask</code> <code>foreign key)            </code> <p><code>FullFitTask</code> Key.</p> <code>model_name</code> <code> </code> <p>varchar(100) # Name of the model as \"kpms_project_output_dir/model_name\"</p> <code>full_fit_duration</code> <code>float)            </code> <p>Time duration (seconds) of the full fitting computation</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>@schema\nclass FullFit(dj.Computed):\n    \"\"\"Fit the full (Keypoint-SLDS) model.\n\n    Attributes:\n        FullFitTask (foreign key)            : `FullFitTask` Key.\n        model_name                           : varchar(100) # Name of the model as \"kpms_project_output_dir/model_name\"\n        full_fit_duration (float)            : Time duration (seconds) of the full fitting computation\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; FullFitTask                               # `FullFitTask` Key\n    ---\n    model_name                    : varchar(100) # Name of the model as \"kpms_project_output_dir/model_name\"\n    full_fit_duration=NULL        : float        # Time duration (seconds) of the full fitting computation \n    \"\"\"\n\n    def make(self, key):\n        \"\"\"\n            Make function to fit the full (keypoint-SLDS) model\n\n            Args:\n                key (dict): dictionary with the `FullFitTask` Key.\n\n            Raises:\n\n            High-level Logic:\n            1. Fetch the `kpms_project_output_dir` and define the full path.\n            2. Fetch the model parameters from the `FullFitTask` table.\n            2. Update the `dj_config.yml` with the selected latent dimension and kappa for the full-fitting.\n            3. Initialize and fit the full model in a new `model_name` directory.\n            4. Load the pca model from file in the `kpms_project_output_dir`.\n            5. Fetch the `coordinates` and `confidences` scores to format the data for the model initialization.\n            6. Initialize the model that create a `model` dict containing states, parameters, hyperparameters, noise prior, and random seed.\n            7. Update the model dict with the selected kappa for the Keypoint-SLDS fitting.\n            8. Fit the Keypoint-SLDS model using the `full_num_iterations` and create a subdirectory in `kpms_project_output_dir` with the model's latest checkpoint file.\n            8. Reindex syllable labels by their frequency in the most recent model snapshot in the checkpoint file. \\\n                This function permutes the states and parameters of a saved checkpoint so that syllables are labeled \\\n                in order of frequency (i.e. so that 0 is the most frequent, 1 is the second most, and so on).\n            8. Calculate the duration of the model fitting computation and insert it in the `PreFit` table.\n        \"\"\"\n        from keypoint_moseq import (\n            load_pca,\n            format_data,\n            init_model,\n            update_hypparams,\n            fit_model,\n            reindex_syllables_in_checkpoint,\n        )\n\n        kpms_processed = moseq_infer.get_kpms_processed_data_dir()\n\n        kpms_project_output_dir = find_full_path(\n            kpms_processed, (PCATask &amp; key).fetch1(\"kpms_project_output_dir\")\n        )\n\n        full_latent_dim, full_kappa, full_num_iterations, task_mode, model_name = (\n            FullFitTask &amp; key\n        ).fetch1(\n            \"full_latent_dim\",\n            \"full_kappa\",\n            \"full_num_iterations\",\n            \"task_mode\",\n            \"model_name\",\n        )\n        if task_mode == \"trigger\":\n            kpms_dj_config = kpms_reader.load_kpms_dj_config(\n                kpms_project_output_dir.as_posix(),\n                check_if_valid=True,\n                build_indexes=True,\n            )\n            kpms_dj_config.update(\n                dict(latent_dim=int(full_latent_dim), kappa=float(full_kappa))\n            )\n            kpms_reader.generate_kpms_dj_config(\n                kpms_project_output_dir.as_posix(), **kpms_dj_config\n            )\n\n            pca_path = kpms_project_output_dir / \"pca.p\"\n            if pca_path:\n                pca = load_pca(kpms_project_output_dir.as_posix())\n            else:\n                raise FileNotFoundError(\n                    f\"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}\"\n                )\n\n            coordinates, confidences = (PCAPrep &amp; key).fetch1(\n                \"coordinates\", \"confidences\"\n            )\n            data, metadata = format_data(coordinates, confidences, **kpms_dj_config)\n            model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)\n            model = update_hypparams(\n                model, kappa=float(full_kappa), latent_dim=int(full_latent_dim)\n            )\n\n            start_time = datetime.utcnow()\n            model, model_name = fit_model(\n                model=model,\n                data=data,\n                metadata=metadata,\n                project_dir=kpms_project_output_dir.as_posix(),\n                ar_only=False,\n                num_iters=full_num_iterations,\n            )\n            end_time = datetime.utcnow()\n            duration_seconds = (end_time - start_time).total_seconds()\n\n            reindex_syllables_in_checkpoint(\n                kpms_project_output_dir.as_posix(), Path(model_name).parts[-1]\n            )\n        else:\n            duration_seconds = None\n\n        self.insert1(\n            {\n                **key,\n                \"model_name\": (\n                    kpms_project_output_dir.relative_to(kpms_processed) / model_name\n                ).as_posix(),\n                \"full_fit_duration\": duration_seconds,\n            }\n        )\n</code></pre>"}, {"location": "api/element_moseq/moseq_train/#element_moseq.moseq_train.FullFit.make", "title": "<code>make(key)</code>", "text": "<p>Make function to fit the full (keypoint-SLDS) model</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary with the <code>FullFitTask</code> Key.</p> required <p>Raises:</p> <p>High-level Logic: 1. Fetch the <code>kpms_project_output_dir</code> and define the full path. 2. Fetch the model parameters from the <code>FullFitTask</code> table. 2. Update the <code>dj_config.yml</code> with the selected latent dimension and kappa for the full-fitting. 3. Initialize and fit the full model in a new <code>model_name</code> directory. 4. Load the pca model from file in the <code>kpms_project_output_dir</code>. 5. Fetch the <code>coordinates</code> and <code>confidences</code> scores to format the data for the model initialization. 6. Initialize the model that create a <code>model</code> dict containing states, parameters, hyperparameters, noise prior, and random seed. 7. Update the model dict with the selected kappa for the Keypoint-SLDS fitting. 8. Fit the Keypoint-SLDS model using the <code>full_num_iterations</code> and create a subdirectory in <code>kpms_project_output_dir</code> with the model's latest checkpoint file. 8. Reindex syllable labels by their frequency in the most recent model snapshot in the checkpoint file.                 This function permutes the states and parameters of a saved checkpoint so that syllables are labeled                 in order of frequency (i.e. so that 0 is the most frequent, 1 is the second most, and so on). 8. Calculate the duration of the model fitting computation and insert it in the <code>PreFit</code> table.</p> Source code in <code>element_moseq/moseq_train.py</code> <pre><code>def make(self, key):\n    \"\"\"\n        Make function to fit the full (keypoint-SLDS) model\n\n        Args:\n            key (dict): dictionary with the `FullFitTask` Key.\n\n        Raises:\n\n        High-level Logic:\n        1. Fetch the `kpms_project_output_dir` and define the full path.\n        2. Fetch the model parameters from the `FullFitTask` table.\n        2. Update the `dj_config.yml` with the selected latent dimension and kappa for the full-fitting.\n        3. Initialize and fit the full model in a new `model_name` directory.\n        4. Load the pca model from file in the `kpms_project_output_dir`.\n        5. Fetch the `coordinates` and `confidences` scores to format the data for the model initialization.\n        6. Initialize the model that create a `model` dict containing states, parameters, hyperparameters, noise prior, and random seed.\n        7. Update the model dict with the selected kappa for the Keypoint-SLDS fitting.\n        8. Fit the Keypoint-SLDS model using the `full_num_iterations` and create a subdirectory in `kpms_project_output_dir` with the model's latest checkpoint file.\n        8. Reindex syllable labels by their frequency in the most recent model snapshot in the checkpoint file. \\\n            This function permutes the states and parameters of a saved checkpoint so that syllables are labeled \\\n            in order of frequency (i.e. so that 0 is the most frequent, 1 is the second most, and so on).\n        8. Calculate the duration of the model fitting computation and insert it in the `PreFit` table.\n    \"\"\"\n    from keypoint_moseq import (\n        load_pca,\n        format_data,\n        init_model,\n        update_hypparams,\n        fit_model,\n        reindex_syllables_in_checkpoint,\n    )\n\n    kpms_processed = moseq_infer.get_kpms_processed_data_dir()\n\n    kpms_project_output_dir = find_full_path(\n        kpms_processed, (PCATask &amp; key).fetch1(\"kpms_project_output_dir\")\n    )\n\n    full_latent_dim, full_kappa, full_num_iterations, task_mode, model_name = (\n        FullFitTask &amp; key\n    ).fetch1(\n        \"full_latent_dim\",\n        \"full_kappa\",\n        \"full_num_iterations\",\n        \"task_mode\",\n        \"model_name\",\n    )\n    if task_mode == \"trigger\":\n        kpms_dj_config = kpms_reader.load_kpms_dj_config(\n            kpms_project_output_dir.as_posix(),\n            check_if_valid=True,\n            build_indexes=True,\n        )\n        kpms_dj_config.update(\n            dict(latent_dim=int(full_latent_dim), kappa=float(full_kappa))\n        )\n        kpms_reader.generate_kpms_dj_config(\n            kpms_project_output_dir.as_posix(), **kpms_dj_config\n        )\n\n        pca_path = kpms_project_output_dir / \"pca.p\"\n        if pca_path:\n            pca = load_pca(kpms_project_output_dir.as_posix())\n        else:\n            raise FileNotFoundError(\n                f\"No pca model (`pca.p`) found in the project directory {kpms_project_output_dir}\"\n            )\n\n        coordinates, confidences = (PCAPrep &amp; key).fetch1(\n            \"coordinates\", \"confidences\"\n        )\n        data, metadata = format_data(coordinates, confidences, **kpms_dj_config)\n        model = init_model(data=data, metadata=metadata, pca=pca, **kpms_dj_config)\n        model = update_hypparams(\n            model, kappa=float(full_kappa), latent_dim=int(full_latent_dim)\n        )\n\n        start_time = datetime.utcnow()\n        model, model_name = fit_model(\n            model=model,\n            data=data,\n            metadata=metadata,\n            project_dir=kpms_project_output_dir.as_posix(),\n            ar_only=False,\n            num_iters=full_num_iterations,\n        )\n        end_time = datetime.utcnow()\n        duration_seconds = (end_time - start_time).total_seconds()\n\n        reindex_syllables_in_checkpoint(\n            kpms_project_output_dir.as_posix(), Path(model_name).parts[-1]\n        )\n    else:\n        duration_seconds = None\n\n    self.insert1(\n        {\n            **key,\n            \"model_name\": (\n                kpms_project_output_dir.relative_to(kpms_processed) / model_name\n            ).as_posix(),\n            \"full_fit_duration\": duration_seconds,\n        }\n    )\n</code></pre>"}, {"location": "api/element_moseq/readers/kpms_reader/", "title": "kpms_reader.py", "text": ""}, {"location": "api/element_moseq/readers/kpms_reader/#element_moseq.readers.kpms_reader.generate_kpms_dj_config", "title": "<code>generate_kpms_dj_config(output_dir, **kwargs)</code>", "text": "<p>This function mirrors the behavior of the <code>generate_config</code> function from the <code>keypoint_moseq</code> package. Nonetheless, it produces a duplicate of the initial configuration file, titled <code>kpms_dj_config.yml</code>, in the output directory to maintain the integrity of the original file. This replicated file accommodates any customized project settings, with default configurations utilized unless specified differently via keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory containing the <code>kpms_dj_config.yml</code> that will be generated.</p> required <code>kwargs</code> <code>dict</code> <p>Custom project settings.</p> <code>{}</code> Source code in <code>element_moseq/readers/kpms_reader.py</code> <pre><code>def generate_kpms_dj_config(output_dir, **kwargs):\n    \"\"\"This function mirrors the behavior of the `generate_config` function from the `keypoint_moseq`\n    package. Nonetheless, it produces a duplicate of the initial configuration file, titled\n    `kpms_dj_config.yml`, in the output directory to maintain the integrity of the original file.\n    This replicated file accommodates any customized project settings, with default configurations\n    utilized unless specified differently via keyword arguments.\n\n    Args:\n        output_dir (str): Directory containing the `kpms_dj_config.yml` that will be generated.\n        kwargs (dict): Custom project settings.\n    \"\"\"\n\n    def _build_yaml(sections, comments):\n        text_blocks = []\n        for title, data in sections:\n            centered_title = f\" {title} \".center(50, \"=\")\n            text_blocks.append(f\"\\n\\n{'#'}{centered_title}{'#'}\")\n            for key, value in data.items():\n                text = yaml.dump({key: value}).strip(\"\\n\")\n                if key in comments:\n                    text = f\"\\n{'#'} {comments[key]}\\n{text}\"\n                text_blocks.append(text)\n        return \"\\n\".join(text_blocks)\n\n    def _update_dict(new, original):\n        return {k: new[k] if k in new else v for k, v in original.items()}\n\n    hypperams = _update_dict(\n        kwargs,\n        {\n            \"error_estimator\": {\"slope\": -0.5, \"intercept\": 0.25},\n            \"obs_hypparams\": {\n                \"sigmasq_0\": 0.1,\n                \"sigmasq_C\": 0.1,\n                \"nu_sigma\": 1e5,\n                \"nu_s\": 5,\n            },\n            \"ar_hypparams\": {\n                \"latent_dim\": 10,\n                \"nlags\": 3,\n                \"S_0_scale\": 0.01,\n                \"K_0_scale\": 10.0,\n            },\n            \"trans_hypparams\": {\n                \"num_states\": 100,\n                \"gamma\": 1e3,\n                \"alpha\": 5.7,\n                \"kappa\": 1e6,\n            },\n            \"cen_hypparams\": {\"sigmasq_loc\": 0.5},\n        },\n    )\n\n    hypperams = {k: _update_dict(kwargs, v) for k, v in hypperams.items()}\n\n    anatomy = _update_dict(\n        kwargs,\n        {\n            \"bodyparts\": [\"BODYPART1\", \"BODYPART2\", \"BODYPART3\"],\n            \"use_bodyparts\": [\"BODYPART1\", \"BODYPART2\", \"BODYPART3\"],\n            \"skeleton\": [\n                [\"BODYPART1\", \"BODYPART2\"],\n                [\"BODYPART2\", \"BODYPART3\"],\n            ],\n            \"anterior_bodyparts\": [\"BODYPART1\"],\n            \"posterior_bodyparts\": [\"BODYPART3\"],\n        },\n    )\n\n    other = _update_dict(\n        kwargs,\n        {\n            \"recording_name_suffix\": \"\",\n            \"verbose\": False,\n            \"conf_pseudocount\": 1e-3,\n            \"video_dir\": \"\",\n            \"keypoint_colormap\": \"autumn\",\n            \"whiten\": True,\n            \"fix_heading\": False,\n            \"seg_length\": 10000,\n        },\n    )\n\n    fitting = _update_dict(\n        kwargs,\n        {\n            \"added_noise_level\": 0.1,\n            \"PCA_fitting_num_frames\": 1000000,\n            \"conf_threshold\": 0.5,\n            #         'kappa_scan_target_duration': 12,\n            #         'kappa_scan_min': 1e2,\n            #         'kappa_scan_max': 1e12,\n            #         'num_arhmm_scan_iters': 50,\n            #         'num_arhmm_final_iters': 200,\n            #         'num_kpslds_scan_iters': 50,\n            #         'num_kpslds_final_iters': 500\n        },\n    )\n\n    comments = {\n        \"verbose\": \"whether to print progress messages during fitting\",\n        \"keypoint_colormap\": \"colormap used for visualization; see `matplotlib.cm.get_cmap` for options\",\n        \"added_noise_level\": \"upper bound of uniform noise added to the data during initial AR-HMM fitting; this is used to regularize the model\",\n        \"PCA_fitting_num_frames\": \"number of frames used to fit the PCA model during initialization\",\n        \"video_dir\": \"directory with videos from which keypoints were derived (used for crowd movies)\",\n        \"recording_name_suffix\": \"suffix used to match videos to recording names; this can usually be left empty (see `util.find_matching_videos` for details)\",\n        \"bodyparts\": \"used to access columns in the keypoint data\",\n        \"skeleton\": \"used for visualization only\",\n        \"use_bodyparts\": \"determines the subset of bodyparts to use for modeling and the order in which they are represented\",\n        \"anterior_bodyparts\": \"used to initialize heading\",\n        \"posterior_bodyparts\": \"used to initialize heading\",\n        \"seg_length\": \"data are broken up into segments to parallelize fitting\",\n        \"trans_hypparams\": \"transition hyperparameters\",\n        \"ar_hypparams\": \"autoregressive hyperparameters\",\n        \"obs_hypparams\": \"keypoint observation hyperparameters\",\n        \"cen_hypparams\": \"centroid movement hyperparameters\",\n        \"error_estimator\": \"parameters to convert neural net likelihoods to error size priors\",\n        \"save_every_n_iters\": \"frequency for saving model snapshots during fitting; if 0 only final state is saved\",\n        \"kappa_scan_target_duration\": \"target median syllable duration (in frames) for choosing kappa\",\n        \"whiten\": \"whether to whiten principal components; used to initialize the latent pose trajectory `x`\",\n        \"conf_threshold\": \"used to define outliers for interpolation when the model is initialized\",\n        \"conf_pseudocount\": \"pseudocount used regularize neural network confidences\",\n        \"fix_heading\": \"whether to keep the heading angle fixed; this should only be True if the pose is constrained to a narrow range of angles, e.g. a headfixed mouse.\",\n    }\n\n    sections = [\n        (\"ANATOMY\", anatomy),\n        (\"FITTING\", fitting),\n        (\"HYPER PARAMS\", hypperams),\n        (\"OTHER\", other),\n    ]\n\n    with open(os.path.join(output_dir, \"kpms_dj_config.yml\"), \"w\") as f:\n        f.write(_build_yaml(sections, comments))\n</code></pre>"}, {"location": "api/element_moseq/readers/kpms_reader/#element_moseq.readers.kpms_reader.load_kpms_dj_config", "title": "<code>load_kpms_dj_config(output_dir, check_if_valid=True, build_indexes=True)</code>", "text": "<p>This function mirrors the functionality of the <code>load_config</code> function from the <code>keypoint_moseq</code> package. Similarly, this function loads the <code>kpms_dj_config.yml</code> from the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory containing the <code>kpms_dj_config.yml</code> that will be loaded.</p> required <code>check_if_valid</code> <code>bool</code> <p>default=True. Check if the config is valid using func:<code>keypoint_moseq.io.check_config_validity</code></p> <code>True</code> <code>build_indexes</code> <code>bool</code> <p>default=True. Add keys <code>\"anterior_idxs\"</code> and <code>\"posterior_idxs\"</code> to the config. Each maps to a jax array indexing the elements of <code>config[\"anterior_bodyparts\"]</code> and <code>config[\"posterior_bodyparts\"]</code> by their order in <code>config[\"use_bodyparts\"]</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>kpms_dj_config</code> <code>dict</code> <p>configuration settings</p> Source code in <code>element_moseq/readers/kpms_reader.py</code> <pre><code>def load_kpms_dj_config(output_dir, check_if_valid=True, build_indexes=True):\n    \"\"\"\n    This function mirrors the functionality of the `load_config` function from the `keypoint_moseq`\n    package. Similarly, this function loads the `kpms_dj_config.yml` from the output directory.\n\n    Args:\n        output_dir (str): Directory containing the `kpms_dj_config.yml` that will be loaded.\n        check_if_valid (bool): default=True. Check if the config is valid using :py:func:`keypoint_moseq.io.check_config_validity`\n        build_indexes (bool): default=True. Add keys `\"anterior_idxs\"` and `\"posterior_idxs\"` to the config. Each maps to a jax array indexing the elements of `config[\"anterior_bodyparts\"]` and `config[\"posterior_bodyparts\"]` by their order in `config[\"use_bodyparts\"]`\n\n    Returns:\n        kpms_dj_config (dict): configuration settings\n    \"\"\"\n\n    from keypoint_moseq import check_config_validity\n\n    config_path = os.path.join(output_dir, \"kpms_dj_config.yml\")\n\n    with open(config_path, \"r\") as f:\n        kpms_dj_config = yaml.safe_load(f)\n\n    if check_if_valid:\n        check_config_validity(kpms_dj_config)\n\n    if build_indexes:\n        kpms_dj_config[\"anterior_idxs\"] = jnp.array(\n            [\n                kpms_dj_config[\"use_bodyparts\"].index(bp)\n                for bp in kpms_dj_config[\"anterior_bodyparts\"]\n            ]\n        )\n        kpms_dj_config[\"posterior_idxs\"] = jnp.array(\n            [\n                kpms_dj_config[\"use_bodyparts\"].index(bp)\n                for bp in kpms_dj_config[\"posterior_bodyparts\"]\n            ]\n        )\n\n    if not \"skeleton\" in kpms_dj_config or kpms_dj_config[\"skeleton\"] is None:\n        kpms_dj_config[\"skeleton\"] = []\n\n    return kpms_dj_config\n</code></pre>"}, {"location": "tutorials/", "title": "Tutorials", "text": "<ul> <li>Element MoSeq includes an interactive tutorial on GitHub Codespaces, which is configured for users to run the pipeline.</li> </ul> <ul> <li>DataJoint Elements are modular and can be connected into a complete pipeline.  In the interactive tutorial is a example Jupyter notebook that combine five DataJoint Elements - Lab, Animal, Session, Event, and MoSeq.  The notebook describes the pipeline and provides instructions for running the pipeline.  For convenience, this notebook is also rendered on this website:<ul> <li>Tutorial notebook</li> </ul> </li> </ul>"}, {"location": "tutorials/#installation-instructions-for-active-projects", "title": "Installation Instructions for Active Projects", "text": "<ul> <li>The Element MoSeq described above can be modified for a user's specific experimental requirements and thereby used in active projects.  </li> </ul> <ul> <li>The GitHub Codespace and Dev Container is configured for tutorials and prototyping. We recommend users to configure a database specifically for production pipelines.  Instructions for a local installation of the integrated development environment with a database can be found on the User Guide page.</li> </ul>"}, {"location": "tutorials/#pose-estimation-method", "title": "Pose Estimation Method", "text": "<ul> <li>At present, behavioral segmentation analysis is compatible with keypoint data extracted with DeepLabCut with single-animal datasets.</li> </ul>"}, {"location": "tutorials/tutorial/", "title": "Tutorial Notebook", "text": "<p>Welcome to the tutorial for the DataJoint Element for motion sequencing analysis. This tutorial aims to provide a comprehensive understanding of the open-source data pipeline by <code>element-moseq</code>.</p> <p></p> <p>The package is designed to seamlessly integrate the PCA fitting, model fitting through initialization, fitting an AR-HMM, and fitting the full keypoint-SLDS model into a data pipeline and streamline model and video management using DataJoint.</p> <p></p> <p>By the end of this tutorial, you will have a clear grasp of how to set up and integrate the <code>Element MoSeq</code> into your specific research projects and your lab.</p> <ul> <li>Setup</li> <li>Activate the DataJoint pipeline</li> <li>Insert example data into subject and session tables</li> <li>Insert the keypoint data from the pose estimation and the body parts in the DataJoint pipeline</li> <li>Fit a PCA model to aligned and centered keypoint coordinates and select the latent dimension</li> <li>Train the AR-HMM and Keypoint-SLDS Models</li> <li>Run the inference task and visualize the results</li> </ul> <p>This tutorial loads the keypoint data extracted by DeepLabCut of a single freely moving mouse in an open-field environment. The open-source data is used as an example in the Keypoint-MoSeq collab tutorial.</p> <p>The goal is to link this point tracking to pose dynamics by identifying its behavioral modules (\"syllables\") without human supervision. The modeling results are stored as a <code>.h5</code> file and a subdirectory of <code>.csv</code> files that contain the following information:</p> <ul> <li>Behavior modules as \"syllables\": the syllable label assigned to each frame (i.e. the state indexes assigned by the model)</li> <li>Centroid and heading in each frame, as estimated by the model, that capture the animal's overall position in allocentric coordinates</li> <li>Latent state: low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, and are modified to reflect the pose dynamics and noise estimates inferred by the model.</li> </ul> <p>The results of this Element example can be combined with other modalities to create a complete customizable data pipeline for your specific lab or study. For instance, you can combine <code>element-moseq</code> with <code>element-deeplabcut</code> and <code>element-calcium-imaging</code> to characterize the neural activity along with natural sub-second rhythmicity in mouse movement.</p> <p>Let's start this tutorial by importing the packages necessary to run the data pipeline.</p> In\u00a0[1]: Copied! <pre>import os\n\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\n</pre> import os  if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\") In\u00a0[2]: Copied! <pre>import datajoint as dj\nfrom pathlib import Path\nimport numpy as np\nfrom datetime import datetime\n\nfrom element_moseq.moseq_infer import get_kpms_processed_data_dir\n</pre> import datajoint as dj from pathlib import Path import numpy as np from datetime import datetime  from element_moseq.moseq_infer import get_kpms_processed_data_dir <p>If the tutorial is run in Codespaces, a private, local database server is created and made available for you. This is where we will insert and store our processed results.</p> <p>Let's connect to the database server.</p> In\u00a0[5]: Copied! <pre>dj.conn()\n</pre> dj.conn() <pre>[2024-08-17 00:40:37,105][INFO]: Connecting root@fakeservices.datajoint.io:3306\nINFO:datajoint:Connecting root@fakeservices.datajoint.io:3306\n[2024-08-17 00:40:37,112][INFO]: Connected root@fakeservices.datajoint.io:3306\nINFO:datajoint:Connected root@fakeservices.datajoint.io:3306\n</pre> Out[5]: <pre>DataJoint connection (connected) root@fakeservices.datajoint.io:3306</pre> <p>This tutorial presumes that the <code>element-moseq</code> has been pre-configured and instantiated, with the database linked downstream to pre-existing <code>subject</code> and <code>session</code> tables. Please refer to the <code>tutorial_pipeline.py</code> for the source code.</p> <p>Now, we will proceed to import the essential schemas required to construct this data pipeline, with particular attention to the primary components: <code>moseq_train</code> and <code>moseq_infer</code>.</p> In\u00a0[6]: Copied! <pre>from tutorial_pipeline import lab, subject, session, moseq_train, moseq_infer\n</pre> from tutorial_pipeline import lab, subject, session, moseq_train, moseq_infer <pre>[2024-08-17 00:40:41,878][WARNING]: lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\nWARNING:datajoint:lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\n</pre> <p>We can represent the tables in the <code>moseq_train</code> and <code>moseq_infer</code> schemas as well as some of the upstream dependencies to <code>session</code> and <code>subject</code> schemas as a diagram.</p> In\u00a0[7]: Copied! <pre>(\n    dj.Diagram(subject.Subject)\n    + dj.Diagram(session.Session)\n    + dj.Diagram(moseq_train)\n    + dj.Diagram(moseq_infer)\n)\n</pre> (     dj.Diagram(subject.Subject)     + dj.Diagram(session.Session)     + dj.Diagram(moseq_train)     + dj.Diagram(moseq_infer) ) Out[7]: <p>As evident from the diagram, this data pipeline encompasses several tables associated with different keypoint-MoSeq components like pca, pre-fitting of AR-HMM, and full fitting of the model. A few tables, such as <code>subject.Subject</code> or <code>session.Session</code>, while important for a complete pipeline, fall outside the scope of the <code>element-moseq</code> tutorial, and will therefore, not be explored extensively here. The primary focus of this tutorial will be on the <code>moseq_train</code> and <code>moseq_infer</code> schemas.</p> In\u00a0[8]: Copied! <pre>dj.Diagram(moseq_train) + dj.Diagram(moseq_infer)\n</pre> dj.Diagram(moseq_train) + dj.Diagram(moseq_infer) Out[8]: <p>Let's delve into the <code>subject.Subject</code> and <code>session.Session</code> tables and include some example data.</p> In\u00a0[9]: Copied! <pre>subject.Subject()\n</pre> subject.Subject() Out[9]: <p>subject</p> <p>subject_nickname</p> <p>sex</p> <p>subject_birth_date</p> <p>subject_description</p> <p>Total: 0</p> <p>Add a new entry for a subject in the <code>Subject</code> table:</p> In\u00a0[10]: Copied! <pre>subject.Subject.insert1(\n    dict(\n        subject=\"subject1\",\n        sex=\"F\",\n        subject_birth_date=\"2024-01-01\",\n        subject_description=\"test subject\",\n    ),\n    skip_duplicates=True,\n)\n</pre> subject.Subject.insert1(     dict(         subject=\"subject1\",         sex=\"F\",         subject_birth_date=\"2024-01-01\",         subject_description=\"test subject\",     ),     skip_duplicates=True, ) <p>Create session keys and input them into the <code>Session</code> table:</p> In\u00a0[11]: Copied! <pre># Definition of the dictionary named \"session_keys\"\nsession_keys = [\n    dict(subject=\"subject1\", session_datetime=\"2024-03-15 14:04:22\"),\n    dict(subject=\"subject1\", session_datetime=\"2024-03-16 14:43:10\"),\n]\n\n# Insert this dictionary in the Session table\nsession.Session.insert(session_keys, skip_duplicates=True)\n</pre> # Definition of the dictionary named \"session_keys\" session_keys = [     dict(subject=\"subject1\", session_datetime=\"2024-03-15 14:04:22\"),     dict(subject=\"subject1\", session_datetime=\"2024-03-16 14:43:10\"), ]  # Insert this dictionary in the Session table session.Session.insert(session_keys, skip_duplicates=True) <p>Confirm the inserted data:</p> In\u00a0[12]: Copied! <pre>session.Session()\n</pre> session.Session() Out[12]: <p>subject</p> <p>session_datetime</p> <p>session_id</p> subject1 2024-03-15 14:04:22 Nonesubject1 2024-03-16 14:43:10 None <p>Total: 2</p> <p>Let's define a <code>key</code> to use throughout the notebook:</p> In\u00a0[13]: Copied! <pre>session_key = dict(subject=\"subject1\", session_datetime=\"2024-03-15 14:04:22\")\nsession_key\n</pre> session_key = dict(subject=\"subject1\", session_datetime=\"2024-03-15 14:04:22\") session_key Out[13]: <pre>{'subject': 'subject1', 'session_datetime': '2024-03-15 14:04:22'}</pre> <p>The <code>PoseEstimationMethod</code> table contains the pose estimation methods and file formats supported by the keypoint loader of <code>keypoint-moseq</code> package. In this tutorial, the keypoint input data are <code>.h5</code> files that have been obtained using <code>DeepLabCut</code>.</p> In\u00a0[14]: Copied! <pre>moseq_infer.PoseEstimationMethod()\n</pre> moseq_infer.PoseEstimationMethod() Out[14]: Pose estimation methods supported by the keypoint loader of `keypoint-moseq` package. <p>pose_estimation_method</p> Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap) <p>pose_estimation_desc</p> Optional. Pose estimation method description with the supported formats. anipose `.csv` files generated by anipose analysisdeeplabcut `.csv` and `.h5/.hdf5` files generated by DeepLabcut analysisfacemap `.h5` files generated by Facemap analysisnwb `.nwb` files with Neurodata Without Borders (NWB) formatsleap `.slp` and `.h5/.hdf5` files generated by SLEAP analysissleap-anipose `.h5/.hdf5` files generated by sleap-anipose analysis <p>Total: 6</p> <p>Insert keypoint input metadata into the <code>KeypointSet</code> table:</p> In\u00a0[15]: Copied! <pre>moseq_train.KeypointSet.insert1(\n    {\n        \"kpset_id\": 1,\n        \"pose_estimation_method\": \"deeplabcut\",\n        \"kpset_dir\": \"dlc_project\",\n        \"kpset_desc\": \"Example keypoint set\",\n    }\n)\n</pre> moseq_train.KeypointSet.insert1(     {         \"kpset_id\": 1,         \"pose_estimation_method\": \"deeplabcut\",         \"kpset_dir\": \"dlc_project\",         \"kpset_desc\": \"Example keypoint set\",     } ) In\u00a0[16]: Copied! <pre>moseq_train.KeypointSet()\n</pre> moseq_train.KeypointSet() Out[16]: <p>kpset_id</p> Unique ID for each keypoint set <p>pose_estimation_method</p> Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap) <p>kpset_dir</p> Path where the keypoint files are located together with the pose estimation `config` file, relative to root data directory <p>kpset_desc</p> Optional. User-entered description 1 deeplabcut dlc_project Example keypoint set <p>Total: 1</p> <p>Add the video files in <code>KeypointSet.VideoFile</code> that will be used to fit the model:</p> In\u00a0[17]: Copied! <pre>videos_path = [\n    \"dlc_project/videos/21_12_10_def6a_3.top.ir.mp4\",\n    \"dlc_project/videos/22_04_26_cage4_1_1.top.ir.mp4\",\n    \"dlc_project/videos/21_12_10_def6a_1_1.top.ir.mp4\",\n    \"dlc_project/videos/22_27_04_cage4_mouse2_0.top.ir.mp4\",\n    \"dlc_project/videos/22_04_26_cage4_0.top.ir.mp4\",\n    \"dlc_project/videos/21_11_8_one_mouse.top.ir.Mp4\",\n    \"dlc_project/videos/21_12_2_def6b_2.top.ir.mp4\",\n    \"dlc_project/videos/21_12_10_def6b_3.top.ir.Mp4\",\n    \"dlc_project/videos/22_04_26_cage4_0_2.top.ir.mp4\",\n    \"dlc_project/videos/21_12_2_def6a_1.top.ir.mp4\",\n]\n\n# Insert the video files in the `VideoFile` table\nmoseq_train.KeypointSet.VideoFile.insert(\n    (\n        {\"kpset_id\": 1, \"video_id\": v_idx, \"video_path\": f}\n        for v_idx, f in enumerate(videos_path)\n    ),\n    skip_duplicates=True,\n)\nmoseq_train.KeypointSet.VideoFile()\n</pre> videos_path = [     \"dlc_project/videos/21_12_10_def6a_3.top.ir.mp4\",     \"dlc_project/videos/22_04_26_cage4_1_1.top.ir.mp4\",     \"dlc_project/videos/21_12_10_def6a_1_1.top.ir.mp4\",     \"dlc_project/videos/22_27_04_cage4_mouse2_0.top.ir.mp4\",     \"dlc_project/videos/22_04_26_cage4_0.top.ir.mp4\",     \"dlc_project/videos/21_11_8_one_mouse.top.ir.Mp4\",     \"dlc_project/videos/21_12_2_def6b_2.top.ir.mp4\",     \"dlc_project/videos/21_12_10_def6b_3.top.ir.Mp4\",     \"dlc_project/videos/22_04_26_cage4_0_2.top.ir.mp4\",     \"dlc_project/videos/21_12_2_def6a_1.top.ir.mp4\", ]  # Insert the video files in the `VideoFile` table moseq_train.KeypointSet.VideoFile.insert(     (         {\"kpset_id\": 1, \"video_id\": v_idx, \"video_path\": f}         for v_idx, f in enumerate(videos_path)     ),     skip_duplicates=True, ) moseq_train.KeypointSet.VideoFile() Out[17]: <p>kpset_id</p> Unique ID for each keypoint set <p>video_id</p> Unique ID for each video corresponding to each keypoint data file, relative to root data directory <p>video_path</p> Filepath of each video from which the keypoints are derived, relative to root data directory 1 0 dlc_project/videos/21_12_10_def6a_3.top.ir.mp41 1 dlc_project/videos/22_04_26_cage4_1_1.top.ir.mp41 2 dlc_project/videos/21_12_10_def6a_1_1.top.ir.mp41 3 dlc_project/videos/22_27_04_cage4_mouse2_0.top.ir.mp41 4 dlc_project/videos/22_04_26_cage4_0.top.ir.mp41 5 dlc_project/videos/21_11_8_one_mouse.top.ir.Mp41 6 dlc_project/videos/21_12_2_def6b_2.top.ir.mp41 7 dlc_project/videos/21_12_10_def6b_3.top.ir.Mp41 8 dlc_project/videos/22_04_26_cage4_0_2.top.ir.mp41 9 dlc_project/videos/21_12_2_def6a_1.top.ir.mp4 <p>Total: 10</p> <p>Now, let's insert the body parts to use in the analysis:</p> In\u00a0[18]: Copied! <pre>pca_task_key = {\"kpset_id\": 1, \"bodyparts_id\": 1}\nmoseq_train.Bodyparts.insert1(\n    {\n        **pca_task_key,\n        \"anterior_bodyparts\": [\"nose\"],\n        \"posterior_bodyparts\": [\"spine4\"],\n        \"use_bodyparts\": [\n            \"spine4\",\n            \"spine3\",\n            \"spine2\",\n            \"spine1\",\n            \"head\",\n            \"nose\",\n            \"right ear\",\n            \"left ear\",\n        ],\n        \"bodyparts_desc\": \"Example of KPMS bodyparts extracted with DLC 2.3.9\",\n    }\n)\n</pre> pca_task_key = {\"kpset_id\": 1, \"bodyparts_id\": 1} moseq_train.Bodyparts.insert1(     {         **pca_task_key,         \"anterior_bodyparts\": [\"nose\"],         \"posterior_bodyparts\": [\"spine4\"],         \"use_bodyparts\": [             \"spine4\",             \"spine3\",             \"spine2\",             \"spine1\",             \"head\",             \"nose\",             \"right ear\",             \"left ear\",         ],         \"bodyparts_desc\": \"Example of KPMS bodyparts extracted with DLC 2.3.9\",     } ) In\u00a0[19]: Copied! <pre>moseq_train.Bodyparts()\n</pre> moseq_train.Bodyparts() Out[19]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>anterior_bodyparts</p> List of strings of anterior bodyparts <p>posterior_bodyparts</p> List of strings of posterior bodyparts <p>use_bodyparts</p> List of strings of bodyparts to be used <p>bodyparts_desc</p> Optional. User-entered description 1 1 =BLOB= =BLOB= =BLOB= Example of KPMS bodyparts extracted with DLC 2.3.9 <p>Total: 1</p> <p>To conduct model fitting for keypoint-MoSeq, both a PCA model and the latent dimension of the pose trajectory are necessary.</p> In\u00a0[20]: Copied! <pre>dj.Diagram(moseq_train)\n</pre> dj.Diagram(moseq_train) Out[20]: <p>The <code>PCATask</code> table serves the purpose of specifying the PCA task.</p> In\u00a0[21]: Copied! <pre>moseq_train.PCATask()\n</pre> moseq_train.PCATask() Out[21]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>kpms_project_output_dir</p> Keypoint-MoSeq project output directory, relative to root data directory <p>task_mode</p> Trigger or load the task <p>Total: 0</p> <p>Defining and inserting a PCA task requires:</p> <ol> <li>Select a keypoint set</li> <li>Select the body parts to use</li> <li>Specify the output directory for the KPMS project</li> </ol> In\u00a0[22]: Copied! <pre>moseq_train.PCATask.insert1(\n    {\n        **pca_task_key,\n        \"kpms_project_output_dir\": \"kpms_project_tutorial\",\n        \"task_mode\": \"load\",\n    }\n)\n</pre> moseq_train.PCATask.insert1(     {         **pca_task_key,         \"kpms_project_output_dir\": \"kpms_project_tutorial\",         \"task_mode\": \"load\",     } ) In\u00a0[23]: Copied! <pre>moseq_train.PCATask()\n</pre> moseq_train.PCATask() Out[23]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>kpms_project_output_dir</p> Keypoint-MoSeq project output directory, relative to root data directory <p>task_mode</p> Trigger or load the task 1 1 kpms_project_tutorial load <p>Total: 1</p> <p>Before running the PCA fitting, the keypoint detections and body parts need to be formatted. The resulting coordinates and confidences scores will be used to format the data for modeling.</p> In\u00a0[24]: Copied! <pre>moseq_train.PCAPrep()\n</pre> moseq_train.PCAPrep() Out[24]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>coordinates</p> Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3]) <p>confidences</p> Dictionary mapping filenames to `likelihood` scores as ndarrays of shape (n_frames, n_bodyparts) <p>formatted_bodyparts</p> List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`. <p>average_frame_rate</p> Average frame rate of the videos for model training <p>frame_rates</p> List of the frame rates of the videos for model training <p>Total: 0</p> <p>Populate the <code>PCAPrep</code> table will:</p> <ol> <li>Create the output directory, if it does not exist, with the kpms default <code>config.yml</code> file that contains the default values from the pose estimation</li> <li>Generate a copy as <code>dj_config.yml</code> and update it with both the video directory and the bodyparts</li> <li>Create and store the keypoint coordinates and confidences scores to format the data for the PCA fitting</li> <li>Calculate the average frame rate of the videoset chosen to train the model. This will be useful to calculate the kappa value in the next step.</li> </ol> In\u00a0[25]: Copied! <pre>moseq_train.PCAPrep.populate(pca_task_key)\n</pre> moseq_train.PCAPrep.populate(pca_task_key) <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n/home/vscode/.local/lib/python3.9/site-packages/keypoint_moseq/analysis.py:20: UserWarning:\n\nUsing Panel interactively in VSCode notebooks requires the jupyter_bokeh package to be installed. You can install it with:\n\n   pip install jupyter_bokeh\n\nor:\n    conda install jupyter_bokeh\n\nand try again.\n\n</pre> <pre>Loading keypoints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&lt;00:00,  1.12it/s]\n</pre> In\u00a0[26]: Copied! <pre>moseq_train.PCAPrep()\n</pre> moseq_train.PCAPrep() Out[26]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>coordinates</p> Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3]) <p>confidences</p> Dictionary mapping filenames to `likelihood` scores as ndarrays of shape (n_frames, n_bodyparts) <p>formatted_bodyparts</p> List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`. <p>average_frame_rate</p> Average frame rate of the videos for model training <p>frame_rates</p> List of the frame rates of the videos for model training 1 1 =BLOB= =BLOB= =BLOB= 30.0 =BLOB= <p>Total: 1</p> <p>The <code>PCAFit</code> computation will format the aligned and centered keypoint coordinates, fit a PCA model, and save it as <code>pca.p</code> file in the output directory.</p> In\u00a0[27]: Copied! <pre>moseq_train.PCAFit.populate(pca_task_key)\n</pre> moseq_train.PCAFit.populate(pca_task_key) In\u00a0[28]: Copied! <pre>moseq_train.PCAFit()\n</pre> moseq_train.PCAFit() Out[28]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>pca_fit_time</p> datetime of the PCA fitting analysis 1 1 None <p>Total: 1</p> <p>However, we still need to determine the specific dimension of the pose trajectory to utilize for fitting the keypoint-MoSeq model. A helpful guideline is to consider the number of dimensions required to explain 90% of the variance, or a maximum of 10 dimensions, whichever is lower.</p> <p>The computation of <code>LatentDimension</code> will automatically identify the components that explain 90% of the variance, aiding the user in making the final decision regarding an appropriate latent dimension for model fitting.</p> In\u00a0[29]: Copied! <pre>moseq_train.LatentDimension.populate(pca_task_key)\n</pre> moseq_train.LatentDimension.populate(pca_task_key) <pre>/usr/local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning:\n\nTrying to unpickle estimator PCA from version 1.3.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n\n</pre> In\u00a0[30]: Copied! <pre>moseq_train.LatentDimension()\n</pre> moseq_train.LatentDimension() Out[30]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>variance_percentage</p> Variance threshold. Fixed value to 90 percent. <p>latent_dimension</p> Number of principal components required to explain the specified variance. <p>latent_dim_desc</p> Automated description of the computation result. 1 1 90.0 4 &gt;=90.0% of variance explained by 4 components. <p>Total: 1</p> <p>To aid the user in selecting the latent dimensions for model fitting, two plots are created below: a cumulative scree plot and a visualization of each Principal Component (PC). In this visualization, translucent nodes/edges represent the mean pose, while opaque nodes/edges represent a perturbation in the direction of the PC. The plots are stored in the output directory.</p> In\u00a0[31]: Copied! <pre># Generate and store plots for the user to choose the latent dimensions in the next step\nfrom keypoint_moseq import load_pca, plot_scree, plot_pcs\nfrom element_moseq.readers.kpms_reader import load_kpms_dj_config\nfrom element_moseq.moseq_infer import get_kpms_processed_data_dir\n\nkpms_project_output_dir = (moseq_train.PCATask &amp; pca_task_key).fetch1(\n    \"kpms_project_output_dir\"\n)\nkpms_project_output_dir = get_kpms_processed_data_dir() / kpms_project_output_dir\n\nkpms_dj_config = load_kpms_dj_config(\n    kpms_project_output_dir.as_posix(), check_if_valid=False, build_indexes=False\n)\npca = load_pca(kpms_project_output_dir.as_posix())\n\n# plot_scree(pca, project_dir=kpms_project_output_dir.as_posix())\n# plot_pcs(pca, project_dir=kpms_project_output_dir.as_posix(), **kpms_dj_config)\nplot_scree(pca, savefig=False)\nplot_pcs(pca, savefig=False, **kpms_dj_config)\n</pre> # Generate and store plots for the user to choose the latent dimensions in the next step from keypoint_moseq import load_pca, plot_scree, plot_pcs from element_moseq.readers.kpms_reader import load_kpms_dj_config from element_moseq.moseq_infer import get_kpms_processed_data_dir  kpms_project_output_dir = (moseq_train.PCATask &amp; pca_task_key).fetch1(     \"kpms_project_output_dir\" ) kpms_project_output_dir = get_kpms_processed_data_dir() / kpms_project_output_dir  kpms_dj_config = load_kpms_dj_config(     kpms_project_output_dir.as_posix(), check_if_valid=False, build_indexes=False ) pca = load_pca(kpms_project_output_dir.as_posix())  # plot_scree(pca, project_dir=kpms_project_output_dir.as_posix()) # plot_pcs(pca, project_dir=kpms_project_output_dir.as_posix(), **kpms_dj_config) plot_scree(pca, savefig=False) plot_pcs(pca, savefig=False, **kpms_dj_config) <pre>/usr/local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning:\n\nTrying to unpickle estimator PCA from version 1.3.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n\n</pre> <p>The chosen dimension for the next steps in the analysis will be <code>latent dimension = 4</code>.</p> <p>The pre-fitting and full-fitting processes for the KPMS Model involve the following steps:</p> <ol> <li>Initialization: Auto-regressive (AR) parameters and syllable sequences are randomly initialized using pose trajectories from PCA</li> <li>Fitting an AR-HMM: AR parameters, transition probabilities and syllable sequences are iteratively updated through Gibbs sampling</li> <li>Fitting the full model: All parameters, including both AR-HMM and centroid, heading, noise-estimates, and continuous latent states (i.e., pose trajectories) are iteratively updated through Gibbs sampling. This step is particularly useful for noisy data.</li> </ol> In\u00a0[32]: Copied! <pre>dj.Diagram(moseq_train)\n</pre> dj.Diagram(moseq_train) Out[32]: <p>For the pre-fitting step (fitting an AR-HMM), a pre-fitting task needs to be defined and inserted:</p> In\u00a0[33]: Copied! <pre>moseq_train.PreFitTask()\n</pre> moseq_train.PreFitTask() Out[33]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>pre_latent_dim</p> Latent dimension to use for the model pre-fitting <p>pre_kappa</p> Kappa value to use for the model pre-fitting <p>pre_num_iterations</p> Number of Gibbs sampling iterations to run in the model pre-fitting <p>model_name</p> Name of the model to be loaded if `task_mode='load'` <p>task_mode</p> 'load': load computed analysis results, 'trigger': trigger computation <p>pre_fit_desc</p> User-defined description of the pre-fitting task <p>Total: 0</p> <p>This task requires the following inputs:</p> <ol> <li>The keypoint set, body parts, and latent dimension (extracted in the section above).</li> <li>A kappa value for the model pre-fitting.</li> <li>The number of iterations for the model pre-fitting.</li> </ol> <p>Kappa hyperparameter: An important decision for the user is to adjust the kappa hyperparameter to achieve the desired distribution of syllable durations. Higher values of kappa result in longer syllables.</p> <p>As a reference, let's choose a kappa value that yields a median syllable duration of 12 frames (400 ms), a duration recommended for rodents.</p> <p>During the model pre-fitting, it's advisable to explore different values of kappa (<code>kappa_range</code>) until the syllable durations stabilize.</p> In\u00a0[34]: Copied! <pre>fps = (moseq_train.PCAPrep &amp; pca_task_key).fetch1(\"average_frame_rate\")\nkappa_min = (12 / fps) * 1000  # ms\nkappa_max = 1e4  # ms\nkappa_range = np.logspace(np.log10(kappa_min), np.log10(kappa_max), num=3)\nkappa_range = np.round(kappa_range).astype(int)\nprint([\"kappa = {:.2f} ms\".format(x) for x in kappa_range])\n</pre> fps = (moseq_train.PCAPrep &amp; pca_task_key).fetch1(\"average_frame_rate\") kappa_min = (12 / fps) * 1000  # ms kappa_max = 1e4  # ms kappa_range = np.logspace(np.log10(kappa_min), np.log10(kappa_max), num=3) kappa_range = np.round(kappa_range).astype(int) print([\"kappa = {:.2f} ms\".format(x) for x in kappa_range]) <pre>['kappa = 400.00 ms', 'kappa = 2000.00 ms', 'kappa = 10000.00 ms']\n</pre> <p>Number of Iterations: Typically, stabilizing the syllable duration requires 10-50 iterations during the model pre-fitting stage, while stabilizing the syllable sequence after setting kappa may take 200-500 iterations during the model full-fitting stage.</p> <p>We have already prepared one model with a <code>prefit_key</code> with <code>pre_latent_dim =4</code>, <code>pre_kappa=1e6</code> with <code>task_mode=trigger</code>.</p> <p>For tutorial purposes, we will use the <code>task_mode = load</code>, which will load the pre-fitted model located in the <code>outbox/kpms_project_tutorial</code>, as follows:</p> In\u00a0[35]: Copied! <pre>prefit_key = {\n    **pca_task_key,\n    \"pre_latent_dim\": 4,\n    \"pre_kappa\": 1000000.0,\n    \"pre_num_iterations\": 5,\n    \"pre_fit_desc\": \"Tutorial PreFit task\",\n    \"task_mode\": \"load\",\n    \"model_name\": \"2024_03_28-18_14_26\",\n}\nprefit_key\n</pre> prefit_key = {     **pca_task_key,     \"pre_latent_dim\": 4,     \"pre_kappa\": 1000000.0,     \"pre_num_iterations\": 5,     \"pre_fit_desc\": \"Tutorial PreFit task\",     \"task_mode\": \"load\",     \"model_name\": \"2024_03_28-18_14_26\", } prefit_key Out[35]: <pre>{'kpset_id': 1,\n 'bodyparts_id': 1,\n 'pre_latent_dim': 4,\n 'pre_kappa': 1000000.0,\n 'pre_num_iterations': 5,\n 'pre_fit_desc': 'Tutorial PreFit task',\n 'task_mode': 'load',\n 'model_name': '2024_03_28-18_14_26'}</pre> <p>Thus, we will insert different entries (<code>prefit_keys</code>) in the <code>PreFitTask</code> with various kappa values until the target syllable time-scale is achieved.</p> In\u00a0[36]: Copied! <pre>moseq_train.PreFitTask.heading\n</pre> moseq_train.PreFitTask.heading Out[36]: <pre># \nkpset_id             : int                          # Unique ID for each keypoint set\nbodyparts_id         : int                          # Unique ID for a set of bodyparts for a particular keypoint set\npre_latent_dim       : int                          # Latent dimension to use for the model pre-fitting\npre_kappa            : int                          # Kappa value to use for the model pre-fitting\npre_num_iterations   : int                          # Number of Gibbs sampling iterations to run in the model pre-fitting\n---\nmodel_name           : varchar(100)                 # Name of the model to be loaded if `task_mode='load'`\ntask_mode=\"load\"     : enum('trigger','load')       # 'load': load computed analysis results, 'trigger': trigger computation\npre_fit_desc=\"\"      : varchar(1000)                # User-defined description of the pre-fitting task</pre> In\u00a0[37]: Copied! <pre>moseq_train.PreFitTask.insert1(prefit_key, skip_duplicates=True)\n</pre> moseq_train.PreFitTask.insert1(prefit_key, skip_duplicates=True) <p>Show the contents of the <code>PreFittingTask</code> table.</p> In\u00a0[38]: Copied! <pre>moseq_train.PreFitTask()\n</pre> moseq_train.PreFitTask() Out[38]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>pre_latent_dim</p> Latent dimension to use for the model pre-fitting <p>pre_kappa</p> Kappa value to use for the model pre-fitting <p>pre_num_iterations</p> Number of Gibbs sampling iterations to run in the model pre-fitting <p>model_name</p> Name of the model to be loaded if `task_mode='load'` <p>task_mode</p> 'load': load computed analysis results, 'trigger': trigger computation <p>pre_fit_desc</p> User-defined description of the pre-fitting task 1 1 4 1000000 5 2024_03_28-18_14_26 load Tutorial PreFit task <p>Total: 1</p> <p>When populating the <code>PreFit</code> table, the fitting of different AR-HMM models for each kappa defined in the <code>PreFitTask</code> will be automatically computed. This step will take a few minutes.</p> In\u00a0[39]: Copied! <pre>moseq_train.PreFit.populate(prefit_key)\n</pre> moseq_train.PreFit.populate(prefit_key) In\u00a0[40]: Copied! <pre>moseq_train.PreFit()\n</pre> moseq_train.PreFit() Out[40]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>pre_latent_dim</p> Latent dimension to use for the model pre-fitting <p>pre_kappa</p> Kappa value to use for the model pre-fitting <p>pre_num_iterations</p> Number of Gibbs sampling iterations to run in the model pre-fitting <p>model_name</p> Name of the model as \"kpms_project_output_dir/model_name\" <p>pre_fit_duration</p> Time duration (seconds) of the model fitting computation 1 1 4 1000000 5 kpms_project_tutorial/2024_03_28-18_14_26 nan <p>Total: 1</p> <p>Now we can define a <code>FullFitTask</code> based on the selected <code>latent_dimension = 4</code>, the chosen <code>kappa = 10000</code> based on the previous exploration.</p> <p>Again and for tutorial purposes, we will <code>load</code> a model already generated to ensure a smooth run of this notebook.</p> In\u00a0[41]: Copied! <pre>moseq_train.FullFitTask.heading\n</pre> moseq_train.FullFitTask.heading Out[41]: <pre># \nkpset_id             : int                          # Unique ID for each keypoint set\nbodyparts_id         : int                          # Unique ID for a set of bodyparts for a particular keypoint set\nfull_latent_dim      : int                          # Latent dimension to use for the model full fitting\nfull_kappa           : int                          # Kappa value to use for the model full fitting\nfull_num_iterations  : int                          # Number of Gibbs sampling iterations to run in the model full fitting\n---\nmodel_name           : varchar(100)                 # Name of the model to be loaded if `task_mode='load'`\ntask_mode=\"load\"     : enum('load','trigger')       # Trigger or load the task\nfull_fit_desc=\"\"     : varchar(1000)                # User-defined description of the model full fitting task</pre> In\u00a0[42]: Copied! <pre># modify kappa to maintain the desired syllable time-scale\nfull_fit_key_1 = {\n    **pca_task_key,\n    \"full_latent_dim\": 4,\n    \"full_kappa\": 10000.0,\n    \"full_num_iterations\": 25,\n    \"full_fit_desc\": \"Fitting task with kappa = 10000 ms\",\n    \"task_mode\": \"load\",\n    \"model_name\": \"2024_03_28-18_54_08\",\n}\n\nmoseq_train.FullFitTask.insert1(full_fit_key_1, skip_duplicates=True)\n</pre> # modify kappa to maintain the desired syllable time-scale full_fit_key_1 = {     **pca_task_key,     \"full_latent_dim\": 4,     \"full_kappa\": 10000.0,     \"full_num_iterations\": 25,     \"full_fit_desc\": \"Fitting task with kappa = 10000 ms\",     \"task_mode\": \"load\",     \"model_name\": \"2024_03_28-18_54_08\", }  moseq_train.FullFitTask.insert1(full_fit_key_1, skip_duplicates=True) <p>Let's add a second FullFitting task:</p> In\u00a0[43]: Copied! <pre>full_fit_key_2 = {\n    **pca_task_key,\n    \"full_latent_dim\": 4,\n    \"full_kappa\": 5000.0,\n    \"full_num_iterations\": 25,\n    \"full_fit_desc\": \"Fitting task with kappa = 5000 ms\",\n    \"task_mode\": \"load\",\n    \"model_name\": \"2024_03_28-18_15_54\",\n}\n\nmoseq_train.FullFitTask.insert1(full_fit_key_2, skip_duplicates=True)\n</pre> full_fit_key_2 = {     **pca_task_key,     \"full_latent_dim\": 4,     \"full_kappa\": 5000.0,     \"full_num_iterations\": 25,     \"full_fit_desc\": \"Fitting task with kappa = 5000 ms\",     \"task_mode\": \"load\",     \"model_name\": \"2024_03_28-18_15_54\", }  moseq_train.FullFitTask.insert1(full_fit_key_2, skip_duplicates=True) In\u00a0[44]: Copied! <pre>moseq_train.FullFitTask()\n</pre> moseq_train.FullFitTask() Out[44]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>full_latent_dim</p> Latent dimension to use for the model full fitting <p>full_kappa</p> Kappa value to use for the model full fitting <p>full_num_iterations</p> Number of Gibbs sampling iterations to run in the model full fitting <p>model_name</p> Name of the model to be loaded if `task_mode='load'` <p>task_mode</p> Trigger or load the task <p>full_fit_desc</p> User-defined description of the model full fitting task 1 1 4 5000 25 2024_03_28-18_15_54 load Fitting task with kappa = 5000 ms1 1 4 10000 25 2024_03_28-18_54_08 load Fitting task with kappa = 10000 ms <p>Total: 2</p> In\u00a0[45]: Copied! <pre>moseq_train.FullFit.populate([full_fit_key_1, full_fit_key_2])\n</pre> moseq_train.FullFit.populate([full_fit_key_1, full_fit_key_2]) In\u00a0[46]: Copied! <pre>moseq_train.FullFit()\n</pre> moseq_train.FullFit() Out[46]: <p>kpset_id</p> Unique ID for each keypoint set <p>bodyparts_id</p> Unique ID for a set of bodyparts for a particular keypoint set <p>full_latent_dim</p> Latent dimension to use for the model full fitting <p>full_kappa</p> Kappa value to use for the model full fitting <p>full_num_iterations</p> Number of Gibbs sampling iterations to run in the model full fitting <p>model_name</p> Name of the model as \"kpms_project_output_dir/model_name\" <p>full_fit_duration</p> Time duration (seconds) of the full fitting computation 1 1 4 5000 25 kpms_project_tutorial/2024_03_28-18_15_54 nan1 1 4 10000 25 kpms_project_tutorial/2024_03_28-18_54_08 nan <p>Total: 2</p> <p>The models, along with their relevant information, will be registered in the DataJoint pipeline as follows:</p> In\u00a0[47]: Copied! <pre>model_name, latent_dim, kappa = (moseq_train.FullFit &amp; \"full_kappa = 10000.\").fetch1(\n    \"model_name\", \"full_latent_dim\", \"full_kappa\"\n)\nmoseq_infer.Model.insert1(\n    {\n        \"model_id\": 1,\n        \"model_name\": \"model 1\",\n        \"model_dir\": model_name,\n        \"latent_dim\": latent_dim,\n        \"kappa\": kappa,\n    },\n    skip_duplicates=True,\n)\n</pre> model_name, latent_dim, kappa = (moseq_train.FullFit &amp; \"full_kappa = 10000.\").fetch1(     \"model_name\", \"full_latent_dim\", \"full_kappa\" ) moseq_infer.Model.insert1(     {         \"model_id\": 1,         \"model_name\": \"model 1\",         \"model_dir\": model_name,         \"latent_dim\": latent_dim,         \"kappa\": kappa,     },     skip_duplicates=True, ) In\u00a0[48]: Copied! <pre>model_name, latent_dim, kappa = (moseq_train.FullFit &amp; \"full_kappa = 5000.\").fetch1(\n    \"model_name\", \"full_latent_dim\", \"full_kappa\"\n)\nmoseq_infer.Model.insert1(\n    {\n        \"model_id\": 2,\n        \"model_name\": \"model 2\",\n        \"model_dir\": model_name,\n        \"latent_dim\": latent_dim,\n        \"kappa\": kappa,\n    },\n    skip_duplicates=True,\n)\n</pre> model_name, latent_dim, kappa = (moseq_train.FullFit &amp; \"full_kappa = 5000.\").fetch1(     \"model_name\", \"full_latent_dim\", \"full_kappa\" ) moseq_infer.Model.insert1(     {         \"model_id\": 2,         \"model_name\": \"model 2\",         \"model_dir\": model_name,         \"latent_dim\": latent_dim,         \"kappa\": kappa,     },     skip_duplicates=True, ) <p>We can check the <code>Model</code> table to confirm that the two models have been registered:</p> In\u00a0[49]: Copied! <pre>moseq_infer.Model()\n</pre> moseq_infer.Model() Out[49]: <p>model_id</p> Unique ID for each model <p>model_name</p> User-friendly model name <p>model_dir</p> Model directory relative to root data directory <p>latent_dim</p> Latent dimension of the model <p>kappa</p> Kappa value of the model <p>model_desc</p> Optional. User-defined description of the model 1 model 1 kpms_project_tutorial/2024_03_28-18_54_08 4 10000.0 2 model 2 kpms_project_tutorial/2024_03_28-18_15_54 4 5000.0 <p>Total: 2</p> In\u00a0[50]: Copied! <pre>model_names = (moseq_train.FullFit).fetch(\"model_name\")\n\ncheckpoint_paths = []\nfor model_name in model_names:\n    checkpoint_paths.append(\n        get_kpms_processed_data_dir() / Path(model_name) / \"checkpoint.h5\"\n    )\ncheckpoint_paths\n\nfrom keypoint_moseq import expected_marginal_likelihoods, plot_eml_scores\n\neml_scores, eml_std_errs = expected_marginal_likelihoods(\n    checkpoint_paths=checkpoint_paths\n)\nbest_model = model_names[np.argmax(eml_scores)]\nprint(f\"Best model: {best_model}\")\n\nplot_eml_scores(eml_scores, eml_std_errs, model_names)\n</pre> model_names = (moseq_train.FullFit).fetch(\"model_name\")  checkpoint_paths = [] for model_name in model_names:     checkpoint_paths.append(         get_kpms_processed_data_dir() / Path(model_name) / \"checkpoint.h5\"     ) checkpoint_paths  from keypoint_moseq import expected_marginal_likelihoods, plot_eml_scores  eml_scores, eml_std_errs = expected_marginal_likelihoods(     checkpoint_paths=checkpoint_paths ) best_model = model_names[np.argmax(eml_scores)] print(f\"Best model: {best_model}\")  plot_eml_scores(eml_scores, eml_std_errs, model_names) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07&lt;00:00,  3.64s/it]</pre> <pre>Best model: kpms_project_tutorial/2024_03_28-18_15_54\n</pre> <pre>\n/home/vscode/.local/lib/python3.9/site-packages/keypoint_moseq/viz.py:2914: UserWarning:\n\nTight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n\n</pre> Out[50]: <pre>(&lt;Figure size 400x350 with 1 Axes&gt;, &lt;Axes: ylabel='EML score'&gt;)</pre> <p>Thus, we choose the best ranked model for the inference task:</p> In\u00a0[51]: Copied! <pre>best_model_id = (moseq_infer.Model &amp; \"model_dir = '{}'\".format(best_model)).fetch1(\n    \"model_id\"\n)\nprint(f\"Best model id: {best_model_id}\")\n</pre> best_model_id = (moseq_infer.Model &amp; \"model_dir = '{}'\".format(best_model)).fetch1(     \"model_id\" ) print(f\"Best model id: {best_model_id}\") <pre>Best model id: 2\n</pre> <p>For tutorial purposes, we'll utilize the same video set (<code>videos_path</code>) employed for modeling training as the video set for inference. This will be incorporated into the <code>VideoRecording</code> table as well.</p> In\u00a0[52]: Copied! <pre>recording_key = {\n    **session_key,\n    \"recording_id\": 1,\n}\nmoseq_infer.VideoRecording.insert1(\n    {**recording_key, \"device\": \"Camera1\"}, skip_duplicates=True\n)\n</pre> recording_key = {     **session_key,     \"recording_id\": 1, } moseq_infer.VideoRecording.insert1(     {**recording_key, \"device\": \"Camera1\"}, skip_duplicates=True ) In\u00a0[53]: Copied! <pre>for idx, video_name in enumerate(videos_path):\n    moseq_infer.VideoRecording.File.insert1(\n        dict(**recording_key, file_id=idx, file_path=video_name), skip_duplicates=True\n    )\n</pre> for idx, video_name in enumerate(videos_path):     moseq_infer.VideoRecording.File.insert1(         dict(**recording_key, file_id=idx, file_path=video_name), skip_duplicates=True     ) In\u00a0[54]: Copied! <pre>moseq_infer.VideoRecording * moseq_infer.VideoRecording.File\n</pre> moseq_infer.VideoRecording * moseq_infer.VideoRecording.File Out[54]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> Unique ID for each recording <p>file_id</p> Unique ID for each file <p>device</p> <p>file_path</p> Filepath of each video, relative to root data directory. subject1 2024-03-15 14:04:22 1 0 Camera1 dlc_project/videos/21_12_10_def6a_3.top.ir.mp4subject1 2024-03-15 14:04:22 1 1 Camera1 dlc_project/videos/22_04_26_cage4_1_1.top.ir.mp4subject1 2024-03-15 14:04:22 1 2 Camera1 dlc_project/videos/21_12_10_def6a_1_1.top.ir.mp4subject1 2024-03-15 14:04:22 1 3 Camera1 dlc_project/videos/22_27_04_cage4_mouse2_0.top.ir.mp4subject1 2024-03-15 14:04:22 1 4 Camera1 dlc_project/videos/22_04_26_cage4_0.top.ir.mp4subject1 2024-03-15 14:04:22 1 5 Camera1 dlc_project/videos/21_11_8_one_mouse.top.ir.Mp4subject1 2024-03-15 14:04:22 1 6 Camera1 dlc_project/videos/21_12_2_def6b_2.top.ir.mp4subject1 2024-03-15 14:04:22 1 7 Camera1 dlc_project/videos/21_12_10_def6b_3.top.ir.Mp4subject1 2024-03-15 14:04:22 1 8 Camera1 dlc_project/videos/22_04_26_cage4_0_2.top.ir.mp4subject1 2024-03-15 14:04:22 1 9 Camera1 dlc_project/videos/21_12_2_def6a_1.top.ir.mp4 <p>Total: 10</p> <p>The <code>InferenceTask</code> table serves the purpose of specifying an inference task:</p> In\u00a0[55]: Copied! <pre>moseq_infer.InferenceTask.heading\n</pre> moseq_infer.InferenceTask.heading Out[55]: <pre># \nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \nrecording_id         : int                          # Unique ID for each recording\nmodel_id             : int                          # Unique ID for each model\n---\npose_estimation_method : char(15)                     # Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap)\nkeypointset_dir      : varchar(1000)                # Keypointset directory for the specified VideoRecording\ninference_output_dir=\"\" : varchar(1000)                # Optional. Sub-directory where the results will be stored\ninference_desc=\"\"    : varchar(1000)                # Optional. User-defined description of the inference task\nnum_iterations=null  : int                          # Optional. Number of iterations to use for the model inference. If null, the default number internally is 50.\ntask_mode=\"load\"     : enum('load','trigger')       # Task mode for the inference task</pre> <p>Defining and inserting a inference task requires:</p> <ol> <li>Define the subject and session datetime</li> <li>Define the video recording</li> <li>Define the pose estimation method used for the video recording</li> <li>Choose a model</li> <li>Specify the output directory and any optional parameters</li> </ol> In\u00a0[56]: Copied! <pre>inference_task = {**recording_key, \"model_id\": best_model_id}\n</pre> inference_task = {**recording_key, \"model_id\": best_model_id} In\u00a0[57]: Copied! <pre>moseq_infer.InferenceTask.insert1(\n    {\n        **inference_task,\n        \"pose_estimation_method\": \"deeplabcut\",\n        \"keypointset_dir\": \"dlc_project/videos\",\n        \"inference_output_dir\": \"inference_output\",\n        \"inference_desc\": \"Inference task for the tutorial\",\n        \"num_iterations\": 5,  # Limited iterations for tutorial purposes.\n    },\n    skip_duplicates=True,\n)\n</pre> moseq_infer.InferenceTask.insert1(     {         **inference_task,         \"pose_estimation_method\": \"deeplabcut\",         \"keypointset_dir\": \"dlc_project/videos\",         \"inference_output_dir\": \"inference_output\",         \"inference_desc\": \"Inference task for the tutorial\",         \"num_iterations\": 5,  # Limited iterations for tutorial purposes.     },     skip_duplicates=True, ) In\u00a0[58]: Copied! <pre>moseq_infer.InferenceTask()\n</pre> moseq_infer.InferenceTask() Out[58]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> Unique ID for each recording <p>model_id</p> Unique ID for each model <p>pose_estimation_method</p> Supported pose estimation method (deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap) <p>keypointset_dir</p> Keypointset directory for the specified VideoRecording <p>inference_output_dir</p> Optional. Sub-directory where the results will be stored <p>inference_desc</p> Optional. User-defined description of the inference task <p>num_iterations</p> Optional. Number of iterations to use for the model inference. If null, the default number internally is 50. <p>task_mode</p> Task mode for the inference task subject1 2024-03-15 14:04:22 1 2 deeplabcut dlc_project/videos inference_output Inference task for the tutorial 5 load <p>Total: 1</p> <p>Populating the <code>Inference</code> table will automatically extract learned states of the model (syllables, latent_state, centroid, and heading) and stored in the inference output directory together with visualizations and grid movies.</p> In\u00a0[59]: Copied! <pre>moseq_infer.Inference.populate(inference_task)\n</pre> moseq_infer.Inference.populate(inference_task) <pre>/usr/local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning:\n\nTrying to unpickle estimator PCA from version 1.3.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n\nLoading keypoints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&lt;00:00,  1.27it/s]\n</pre> In\u00a0[60]: Copied! <pre>moseq_infer.Inference()\n</pre> moseq_infer.Inference() Out[60]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> Unique ID for each recording <p>model_id</p> Unique ID for each model <p>inference_duration</p> Time duration (seconds) of the inference computation subject1 2024-03-15 14:04:22 1 2 nan <p>Total: 1</p> <p>The <code>MotionSequence</code> table contains the results for the inference (syllables, latent_state, centroid, and heading):</p> In\u00a0[61]: Copied! <pre>moseq_infer.Inference.MotionSequence()\n</pre> moseq_infer.Inference.MotionSequence() Out[61]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> Unique ID for each recording <p>model_id</p> Unique ID for each model <p>video_name</p> Name of the video <p>syllable</p> Syllable labels (z). The syllable label assigned to each frame (i.e. the state indexes assigned by the model) <p>latent_state</p> Inferred low-dim pose state (x). Low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, are modified to reflect the pose dynamics and noise estimates inferred by the model <p>centroid</p> Inferred centroid (v). The centroid of the animal in each frame, as estimated by the model <p>heading</p> Inferred heading (h). The heading of the animal in each frame, as estimated by the model subject1 2024-03-15 14:04:22 1 2 21_11_8_one_mouse.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 21_12_10_def6a_1_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 21_12_10_def6a_3.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 21_12_10_def6b_3.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 21_12_2_def6a_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 22_04_26_cage4_0_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 22_04_26_cage4_0.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 22_04_26_cage4_1_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB=subject1 2024-03-15 14:04:22 1 2 22_27_04_cage4_mouse2_0.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000 =BLOB= =BLOB= =BLOB= =BLOB= <p>Total: 10</p> <p>The <code>GridMoviesSampledInstances</code> table contains the sampled instances for the grid movies. The sampled instances is a dictionary mapping syllables to lists of instances shown in each grid movie (in row-major order).</p> In\u00a0[62]: Copied! <pre>moseq_infer.Inference.GridMoviesSampledInstances()\n</pre> moseq_infer.Inference.GridMoviesSampledInstances() Out[62]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> Unique ID for each recording <p>model_id</p> Unique ID for each model <p>syllable</p> Syllable label <p>instances</p> List of instances shown in each in grid movie (in row-major order), where each instance is specified as a tuple with the video name, start frame and end frame subject1 2024-03-15 14:04:22 1 2 0 =BLOB=subject1 2024-03-15 14:04:22 1 2 1 =BLOB=subject1 2024-03-15 14:04:22 1 2 2 =BLOB=subject1 2024-03-15 14:04:22 1 2 3 =BLOB=subject1 2024-03-15 14:04:22 1 2 4 =BLOB=subject1 2024-03-15 14:04:22 1 2 5 =BLOB=subject1 2024-03-15 14:04:22 1 2 6 =BLOB=subject1 2024-03-15 14:04:22 1 2 7 =BLOB=subject1 2024-03-15 14:04:22 1 2 8 =BLOB=subject1 2024-03-15 14:04:22 1 2 9 =BLOB=subject1 2024-03-15 14:04:22 1 2 10 =BLOB=subject1 2024-03-15 14:04:22 1 2 11 =BLOB= <p>...</p> <p>Total: 36</p> In\u00a0[63]: Copied! <pre>instance_syllable_0 = (\n    moseq_infer.Inference.GridMoviesSampledInstances &amp; \"syllable = 0\"\n).fetch1(\"instances\")\ninstance_syllable_0\n</pre> instance_syllable_0 = (     moseq_infer.Inference.GridMoviesSampledInstances &amp; \"syllable = 0\" ).fetch1(\"instances\") instance_syllable_0 Out[63]: <pre>[('22_04_26_cage4_0.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  23065,\n  23074),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  104500,\n  104518),\n ('21_11_8_one_mouse.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  7956,\n  7974),\n ('21_12_2_def6a_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  70500,\n  70540),\n ('22_04_26_cage4_0_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  39581,\n  39611),\n ('21_12_2_def6a_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  74072,\n  74103),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  100604,\n  100645),\n ('21_12_10_def6a_1_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  45835,\n  45853),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  100008,\n  100022),\n ('22_04_26_cage4_0.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  21223,\n  21232),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  7026,\n  7053),\n ('21_12_10_def6b_3.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  35468,\n  35504),\n ('21_12_2_def6a_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  63734,\n  63749),\n ('22_27_04_cage4_mouse2_0.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  16209,\n  16214),\n ('22_27_04_cage4_mouse2_0.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  39123,\n  39140),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  19257,\n  19274),\n ('21_12_2_def6a_1.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  62477,\n  62533),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  78366,\n  78382),\n ('22_04_26_cage4_0_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  44743,\n  44770),\n ('22_04_26_cage4_0_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  23396,\n  23478),\n ('21_11_8_one_mouse.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  37546,\n  37557),\n ('21_11_8_one_mouse.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  10772,\n  10777),\n ('21_12_2_def6b_2.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  41181,\n  41195),\n ('21_12_10_def6b_3.top.irDLC_resnet50_moseq_exampleAug21shuffle1_500000',\n  24808,\n  24845)]</pre> <p>The instance for syllable 0 is represented as a tuple containing the video name, start frame, and end frame. This format facilitates downstream analysis.</p> In\u00a0[64]: Copied! <pre>import ipywidgets as widgets\nfrom IPython.display import Image, display\n\nfrom element_interface.utils import find_full_path\n\nsyllable_id = 0\n\nmodel_dir = (moseq_infer.Model &amp; inference_task).fetch1(\"model_dir\")\ninference_output_dir = (\n    moseq_infer.InferenceTask * moseq_infer.Inference.MotionSequence &amp; inference_task\n).fetch(\"inference_output_dir\", limit=1)[0]\nmodel_path = find_full_path(get_kpms_processed_data_dir(), model_dir)\nvideo_path = (\n    model_path\n    / inference_output_dir\n    / \"grid_movies\"\n    / (\"syllable\" + str(syllable_id) + \".mp4\")\n).as_posix()\nprint(video_path)\ngif_path = (\n    model_path\n    / inference_output_dir\n    / \"trajectory_plots\"\n    / (\"Syllable\" + str(syllable_id) + \".gif\")\n).as_posix()\ngif_path1 = (\n    model_path\n    / inference_output_dir\n    / \"trajectory_plots\"\n    / (\"Syllable\" + str(syllable_id + 1) + \".gif\")\n).as_posix()\ngif_path2 = (\n    model_path\n    / inference_output_dir\n    / \"trajectory_plots\"\n    / (\"Syllable\" + str(syllable_id + 2) + \".gif\")\n).as_posix()\nvideo_widget = widgets.Video.from_file(video_path, format=\"mp4\", width=640, height=480)\ndisplay(video_widget)\ndisplay(Image(filename=gif_path))\ndisplay(Image(filename=gif_path1))\ndisplay(Image(filename=gif_path2))\n</pre> import ipywidgets as widgets from IPython.display import Image, display  from element_interface.utils import find_full_path  syllable_id = 0  model_dir = (moseq_infer.Model &amp; inference_task).fetch1(\"model_dir\") inference_output_dir = (     moseq_infer.InferenceTask * moseq_infer.Inference.MotionSequence &amp; inference_task ).fetch(\"inference_output_dir\", limit=1)[0] model_path = find_full_path(get_kpms_processed_data_dir(), model_dir) video_path = (     model_path     / inference_output_dir     / \"grid_movies\"     / (\"syllable\" + str(syllable_id) + \".mp4\") ).as_posix() print(video_path) gif_path = (     model_path     / inference_output_dir     / \"trajectory_plots\"     / (\"Syllable\" + str(syllable_id) + \".gif\") ).as_posix() gif_path1 = (     model_path     / inference_output_dir     / \"trajectory_plots\"     / (\"Syllable\" + str(syllable_id + 1) + \".gif\") ).as_posix() gif_path2 = (     model_path     / inference_output_dir     / \"trajectory_plots\"     / (\"Syllable\" + str(syllable_id + 2) + \".gif\") ).as_posix() video_widget = widgets.Video.from_file(video_path, format=\"mp4\", width=640, height=480) display(video_widget) display(Image(filename=gif_path)) display(Image(filename=gif_path1)) display(Image(filename=gif_path2)) <pre>/workspaces/element-moseq/example_data/outbox/kpms_project_tutorial/2024_03_28-18_15_54/inference_output/grid_movies/syllable0.mp4\n</pre> <pre>Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...', height='480', wid\u2026</pre> <pre>&lt;IPython.core.display.Image object&gt;</pre> <pre>&lt;IPython.core.display.Image object&gt;</pre> <pre>&lt;IPython.core.display.Image object&gt;</pre> <p>Following this tutorial, we have:</p> <ul> <li>Covered the essential functionality of <code>element-moseq</code></li> <li>Acquired the skills to load the keypoint data and insert metadata into the pipeline</li> <li>Learned how to fit a PCA, run the AR-HMM fitting and the Keypoint-SLDS fitting</li> <li>Executed and ingested results of the motion sequencing analysis with Keypoint-MoSeq</li> <li>Visualized and stored the results</li> </ul> <ul> <li>Detailed documentation on <code>element-moseq</code></li> <li>General <code>DataJoint-Python</code> interactive tutorials, covering fundamentals, such as table tiers, query operations, fetch operations, automated computations with the make function, and more.</li> <li>Documentation for <code>DataJoint-Python</code></li> </ul>"}, {"location": "tutorials/tutorial/#datajoint-element-for-motion-sequencing-with-keypoint-moseq", "title": "DataJoint Element for Motion Sequencing with Keypoint-MoSeq\u00b6", "text": ""}, {"location": "tutorials/tutorial/#open-source-data-pipeline-for-motion-sequencing-in-neurophysiology", "title": "Open-source Data Pipeline for Motion Sequencing in Neurophysiology\u00b6", "text": ""}, {"location": "tutorials/tutorial/#prerequisites", "title": "Prerequisites\u00b6", "text": "<p>Please see the datajoint tutorials GitHub repository proceeding. A basic understanding of the following DataJoint concepts will be beneficial to your understanding of this tutorial:</p> <ol> <li>The <code>Imported</code> and <code>Computed</code> tables types in <code>datajoint-python</code>.</li> <li>The functionality of the <code>.populate()</code> method.</li> </ol>"}, {"location": "tutorials/tutorial/#tutorial-overview", "title": "Tutorial Overview\u00b6", "text": ""}, {"location": "tutorials/tutorial/#setup", "title": "Setup\u00b6", "text": ""}, {"location": "tutorials/tutorial/#steps-to-run-the-element-moseq", "title": "Steps to Run the Element-MoSeq\u00b6", "text": "<p>The input data for this data pipeline is as follows:</p> <ul> <li>A DeepLabCut (DLC) project folder with its configuration file as <code>.yaml</code> file, video set as <code>.mp4</code>, and keypoint tracking as <code>.h5</code> files.</li> <li>Selection of the anterior, posterior, and use bodyparts for the model fitting.</li> </ul> <p>This tutorial includes the keypoints example data in <code>example_data/inbox/dlc_project</code>.</p>"}, {"location": "tutorials/tutorial/#activate-the-datajoint-pipeline", "title": "Activate the DataJoint pipeline\u00b6", "text": ""}, {"location": "tutorials/tutorial/#insert-example-data-into-subject-and-session-tables", "title": "Insert example data into subject and session tables\u00b6", "text": ""}, {"location": "tutorials/tutorial/#insert-the-keypoint-data-from-the-pose-estimation-and-the-body-parts-in-the-datajoint-pipeline", "title": "Insert the keypoint data from the pose estimation and the body parts in the DataJoint pipeline\u00b6", "text": ""}, {"location": "tutorials/tutorial/#fit-a-pca-model-to-aligned-and-centered-keypoint-coordinates-and-select-the-latent-dimension", "title": "Fit a PCA model to aligned and centered keypoint coordinates and select the latent dimension\u00b6", "text": ""}, {"location": "tutorials/tutorial/#train-the-ar-hmm-and-keypoint-slds-models", "title": "Train the AR-HMM and keypoint-SLDS Models\u00b6", "text": ""}, {"location": "tutorials/tutorial/#run-the-inference-task-and-visualize-the-results", "title": "Run the inference task and visualize the results\u00b6", "text": ""}, {"location": "tutorials/tutorial/#optional-model-comparison-to-select-a-model", "title": "Optional: Model comparison to select a model\u00b6", "text": "<p>The expected marginal likelihood (EML) score can be used to rank models. The model with the highest EML score can then be selected for further analysis.</p>"}, {"location": "tutorials/tutorial/#summary", "title": "Summary\u00b6", "text": ""}, {"location": "tutorials/tutorial/#documentation-and-datajoint-tutorials", "title": "Documentation and DataJoint tutorials\u00b6", "text": ""}]}